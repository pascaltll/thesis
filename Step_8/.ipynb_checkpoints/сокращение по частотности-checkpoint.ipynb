{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e66a70c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m../dataset\u001b[0m\r\n",
      "├── \u001b[01;34men_espanol\u001b[0m\r\n",
      "│   ├── \u001b[00mdocx2txt.py\u001b[0m\r\n",
      "│   ├── \u001b[00mВторой_жанр_исходная.txt\u001b[0m\r\n",
      "│   └── \u001b[00mПервый_жанр_исходная.txt\u001b[0m\r\n",
      "├── \u001b[00mВторой_жанр_исходная.txt\u001b[0m\r\n",
      "├── \u001b[00mПервый_жанр_исходная.txt\u001b[0m\r\n",
      "├── \u001b[01;34mСокращение по частям речи\u001b[0m\r\n",
      "│   ├── \u001b[00m1.Первый жанр исходная выборка.txt\u001b[0m\r\n",
      "│   ├── \u001b[00m2.Первый жанр без клауз, включающих наречия.txt\u001b[0m\r\n",
      "│   ├── \u001b[00m3.Первый жанр без клауз, включающих глаголы.txt\u001b[0m\r\n",
      "│   ├── \u001b[00m4.Первый жанр без клауз, включающих глаголы и наречия.txt\u001b[0m\r\n",
      "│   ├── \u001b[00mБез прилагательных второй жанр.txt\u001b[0m\r\n",
      "│   ├── \u001b[00mБез прилагательных первый жанр.txt\u001b[0m\r\n",
      "│   └── \u001b[00mСлучайные выборки.txt\u001b[0m\r\n",
      "└── \u001b[01;34mсокращение по частотности\u001b[0m\r\n",
      "    ├── \u001b[00m1а_ без сокращений.txt\u001b[0m\r\n",
      "    ├── \u001b[00m1б_Изъяты лексемы с частотой выше 100.txt\u001b[0m\r\n",
      "    ├── \u001b[00m1в_Изъяты лексемы с частотой выше 49.txt\u001b[0m\r\n",
      "    ├── \u001b[00m1г_Изъяты лексемы с частотой выше 29.txt\u001b[0m\r\n",
      "    ├── \u001b[00m1д_Изъяты лексемы с частотой выше 9.txt\u001b[0m\r\n",
      "    ├── \u001b[00m1е_Изъяты лексемы с частотой выше 5.txt\u001b[0m\r\n",
      "    ├── \u001b[00m1ё_Изъяты лексемы с частотой выше 3.txt\u001b[0m\r\n",
      "    ├── \u001b[00m2а_ без сокращений.txt\u001b[0m\r\n",
      "    ├── \u001b[00m2б_Изъяты лексемы с частотой выше 100.txt\u001b[0m\r\n",
      "    ├── \u001b[00m2в_Изъяты лексемы с частотой выше 49.txt\u001b[0m\r\n",
      "    ├── \u001b[00m2г_Изъяты лексемы с частотой выше 29.txt\u001b[0m\r\n",
      "    ├── \u001b[00m2д_Изъяты лексемы с частотой выше 9.txt\u001b[0m\r\n",
      "    ├── \u001b[00m2е_Изъяты лексемы с частотой выше 5.txt\u001b[0m\r\n",
      "    └── \u001b[00m2ё_Изъяты лексемы с частотой выше 3.txt\u001b[0m\r\n",
      "\r\n",
      "3 directories, 26 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree ../dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accb463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Untitled.ipynb\r\n",
      " \u001b[0m\u001b[01;34m__pycache__\u001b[0m/\r\n",
      " accuracy_by_dataset_type.png\r\n",
      " comprehensive_comparison_freq.png\r\n",
      " confronto_prestazioni.png\r\n",
      "'confusion_matrices_gpt2_Изъяты лексемы с частотой выше 100.png'\r\n",
      " confusion_matrix_DeepPavlov_rubert-base-cased_Изъяты_лексемы_с_частотой_выше_100.png\r\n",
      " confusion_matrix_bert-base-multilingual-cased_Изъяты_лексемы_с_частотой_выше_100.png\r\n",
      " confusion_matrix_distilbert-base-multilingual-cased_Изъяты_лексемы_с_частотой_выше_100.png\r\n",
      " confusion_matrix_facebook_opt-125m_Изъяты_лексемы_с_частотой_выше_100.png\r\n",
      " confusion_matrix_gpt2_Изъяты_лексемы_с_частотой_выше_100.png\r\n",
      " confusion_matrix_roberta-base_Изъяты_лексемы_с_частотой_выше_100.png\r\n",
      " f1_macro_by_dataset_type.png\r\n",
      " frecuencia.png\r\n",
      " frecuencia_lineas.png\r\n",
      " frecuencia_profesional.png\r\n",
      " frecuencia_profesional_ajustada.png\r\n",
      " frecuencia_step_1.png\r\n",
      " frecuencia_test_1.png\r\n",
      " freq_analysis.png\r\n",
      " freq_comparison.png\r\n",
      " funciones.py\r\n",
      " generador.ipynb\r\n",
      " herramientas.ipynb\r\n",
      " loss_comparison.png\r\n",
      " model_comparison_all_datasets.png\r\n",
      " model_results.json\r\n",
      " model_results_GPT.json\r\n",
      " model_results_GPT_bert.json\r\n",
      " model_results_GPT_bert_all.json\r\n",
      " parte_2.png\r\n",
      " parte_2_Step1.png\r\n",
      " partes_discurso.png\r\n",
      " performance_comparison_other.png\r\n",
      " performance_table_freq.csv\r\n",
      " \u001b[01;34mplots\u001b[0m/\r\n",
      " plto.ipynb\r\n",
      " pos_analysis.png\r\n",
      " utils.py\r\n",
      " \u001b[01;34mwandb\u001b[0m/\r\n",
      "\u001b[01;34m'сокращение по частотности'\u001b[0m/\r\n",
      "'сокращение по частотности.ipynb'\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b299fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_results(results, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c11038bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Изъяты лексемы с частотой выше 100.csv\n",
      "test_Изъяты лексемы с частотой выше 100.csv\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([876])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([205])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacity of 10.58 GiB of which 5.06 MiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 10.04 GiB is allocated by PyTorch, and 363.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 281\u001b[0m\n\u001b[1;32m    277\u001b[0m     save_results(results, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_results_GPT_bert_all.json\u001b[39m\u001b[38;5;124m'\u001b[39m)    \n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 281\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 262\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# Entrenar y limpiar memoria\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m--> 262\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunciones\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_evaluate_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(result) \n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "File \u001b[0;32m/workspace/notebooks/thesis/Step_8/funciones.py:579\u001b[0m, in \u001b[0;36mtrain_and_evaluate_dataset\u001b[0;34m(path1, path2, config, dataset_name, dataset_type)\u001b[0m\n\u001b[1;32m    574\u001b[0m     loss_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m    576\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(\u001b[38;5;28mlist\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters()) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mlist\u001b[39m(classifier\u001b[38;5;241m.\u001b[39mparameters()) \u001b[38;5;28;01mif\u001b[39;00m classifier \u001b[38;5;28;01melse\u001b[39;00m []), \n\u001b[1;32m    577\u001b[0m                  lr\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlearning_rate)\n\u001b[0;32m--> 579\u001b[0m final_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(final_loss)\n\u001b[1;32m    584\u001b[0m results \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_loader, config\u001b[38;5;241m.\u001b[39mdevice, config\u001b[38;5;241m.\u001b[39mtokenizer, \n\u001b[1;32m    585\u001b[0m                        config\u001b[38;5;241m.\u001b[39mmodel_type, classifier\u001b[38;5;241m=\u001b[39mclassifier)\n",
      "File \u001b[0;32m/workspace/notebooks/thesis/Step_8/funciones.py:409\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, optimizer, loss_fn, device, epochs, model_type, dataset_name, model_name, classifier)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassifier must be provided for GPT model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Obtener hidden states para GPT\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    413\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# Tomar el último layer de hidden states [batch_size, seq_len, hidden_size]\u001b[39;00m\n\u001b[1;32m    416\u001b[0m last_hidden \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1517\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:1315\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1309\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1315\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1332\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1517\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:1129\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1117\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1118\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1119\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1126\u001b[0m         output_attentions,\n\u001b[1;32m   1127\u001b[0m     )\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1129\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1140\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1517\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:614\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    612\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    613\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 614\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    623\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1517\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:537\u001b[0m, in \u001b[0;36mGPT2SdpaAttention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_contiguous_qkv \u001b[38;5;129;01mand\u001b[39;00m query\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    536\u001b[0m     query \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m--> 537\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m     value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    540\u001b[0m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacity of 10.58 GiB of which 5.06 MiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 10.04 GiB is allocated by PyTorch, and 363.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import funciones\n",
    "from utils import train_wrapper\n",
    "import warnings\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "from contextlib import redirect_stderr\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "#import wandb\n",
    "import nbformat\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import gc\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from razdel import sentenize\n",
    "import numpy as np\n",
    "\n",
    "# Suprimir warnings específicos\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def convert_numpy(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    if isinstance(obj, (np.float32, np.float64, np.int32, np.int64)):\n",
    "        return obj.item()\n",
    "    return obj\n",
    "\n",
    "def save_results(results, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=4, ensure_ascii=False, default=convert_numpy)\n",
    "\n",
    "# import funciones\n",
    "# from utils import train_wrapper\n",
    "# import warnings\n",
    "# import os\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Suprimir warnings específicos\n",
    "# warnings.filterwarnings('ignore', category=UserWarning)  # Para sklearn y otros\n",
    "# warnings.filterwarnings('ignore', category=FutureWarning)  # Para huggingface y transformers\n",
    "# from concurrent.futures import ProcessPoolExecutor\n",
    "# from tqdm import tqdm\n",
    "# from contextlib import redirect_stderr\n",
    "# from concurrent.futures import ProcessPoolExecutor\n",
    "# import multiprocessing as mp\n",
    "# import numpy as np\n",
    "# mp.set_start_method('spawn', force=True)\n",
    "# #os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "\n",
    "###############################################################################\n",
    "# import wandb\n",
    "# import nbformat\n",
    "# # Configurar la clave de la API como variable de entorno\n",
    "# os.environ[\"WANDB_API_KEY\"] = \"ca85316ed713b2425615fb3a613d7eb414c9f57f\"  # Reemplaza con tu clave\n",
    "# # Iniciar wandb sin especificar entity (se detecta automáticamente)\n",
    "# wandb.init(project=\"model-clasification\")\n",
    "# wandb.init(\n",
    "#     settings=wandb.Settings(\n",
    "#         start_method=\"thread\",\n",
    "#         timeout=30,\n",
    "#         sync_dir=\"/tmp/wandb\",  # Usa un directorio temporal\n",
    "#         disable_code=True       # Mejora la estabilidad\n",
    "#     )\n",
    "# )\n",
    "#os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"сокращение по частотности.ipynb\"\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def create_custom_config(model_name, model_type, dataset):\n",
    "    \"\"\"Crea configuración de entrenamiento adaptativa\"\"\"\n",
    "    common_config = {\n",
    "#         'dataset_name':dataset.get('name', ''),\n",
    "        'model_name': model_name,\n",
    "        'model_type': model_type,\n",
    "        'num_repeats': 6,\n",
    "        'test_size': 0.2,\n",
    "        'threshold': 0.5\n",
    "    }\n",
    "\n",
    "    # Configuración específica para GPT\n",
    "#     if model_type == 'gpt':\n",
    "#         return funciones.TrainingConfig(\n",
    "#             **common_config,\n",
    "#             max_length=52,\n",
    "#             batch_size=64,\n",
    "#             epochs=6,\n",
    "#             learning_rate=1e-5\n",
    "#         )\n",
    "\n",
    "    # Configuraciones basadas en frecuencia\n",
    "    freq_threshold = dataset.get('freq_threshold')\n",
    "    if freq_threshold in [100, 49, 29, 9, 5, 3]:\n",
    "        configs = {\n",
    "            100: (52, 128, 3, 2e-5),\n",
    "            49: (60, 128, 4, 3e-5),\n",
    "            29: (51, 128, 4, 3e-5),\n",
    "            9: (45, 128, 5, 4e-5),\n",
    "            5: (150, 32, 6, 5e-5),\n",
    "            3: (150, 32, 6, 5e-5)\n",
    "        }\n",
    "        max_len, batch, epochs, lr = configs[freq_threshold]\n",
    "        return funciones.TrainingConfig(\n",
    "            **common_config,\n",
    "            max_length=max_len,\n",
    "            batch_size=batch,\n",
    "            epochs=epochs,\n",
    "            learning_rate=lr\n",
    "        )\n",
    "\n",
    "    # Configuración por nombre de dataset\n",
    "    dataset_name = dataset.get('name', '')\n",
    "    if any(x in dataset_name for x in ['1', '2', '3', '4']):\n",
    "        return funciones.TrainingConfig(\n",
    "            **common_config,\n",
    "            max_length=60,\n",
    "            batch_size=128,\n",
    "            epochs=6,\n",
    "            learning_rate=5e-5\n",
    "        )\n",
    "\n",
    "    # Configuración por defecto\n",
    "    return funciones.TrainingConfig(\n",
    "        **common_config,\n",
    "        max_length=60,\n",
    "        batch_size=32,\n",
    "        epochs=4,\n",
    "        learning_rate=3e-5\n",
    "    )\n",
    "\n",
    "models = [\n",
    "#         {'model':'DeepPavlov/rubert-base-cased',\n",
    "#          'name':'DeepPavlov-rubert-base',\n",
    "#          'type': 'bert'},# Modelo original\n",
    "#         {'model':'bert-base-multilingual-cased',\n",
    "#          'name':'BERT multilingual',\n",
    "#          'type': 'bert'},# BERT multilingüe\n",
    "#         {'model':'distilbert-base-multilingual-cased',\n",
    "#          'name':'distilbert-base-multilingual',\n",
    "#          'type': 'bert'},# Versión ligera de BERT\n",
    "#         {'model':'roberta-base',\n",
    "#          'name':'roberta-base', \n",
    "#         'type': 'bert'}, # RoBERTa \n",
    "        {'model': 'gpt2',\n",
    "         'name': 'gpt2',\n",
    "         'type': 'gpt'},  #  GPT model\n",
    "        {'model': 'facebook/opt-125m',\n",
    "         'name': 'facebook',\n",
    "         'type': 'gpt'},  #  GPT model\n",
    "        {'model': 'sberbank-ai/rugpt3small_based_on_gpt2',\n",
    "         'name': 'rugpt3',\n",
    "         'type': 'gpt'}\n",
    "    ]\n",
    "\n",
    "datasets = [\n",
    "        {\n",
    "            'path1': '../dataset/сокращение по частотности/1б_Изъяты лексемы с частотой выше 100.txt',\n",
    "            'path2': '../dataset/сокращение по частотности/2б_Изъяты лексемы с частотой выше 100.txt',\n",
    "            'name': 'Изъяты лексемы с частотой выше 100',\n",
    "            'type': 'freq',\n",
    "            'freq': 100\n",
    "        },\n",
    "#         {\n",
    "#             'path1': '../dataset/сокращение по частотности/1в_Изъяты лексемы с частотой выше 49.txt',\n",
    "#             'path2': '../dataset/сокращение по частотности/2в_Изъяты лексемы с частотой выше 49.txt',\n",
    "#             'name': 'Изъяты лексемы с частотой выше 49',\n",
    "#             'type': 'freq',\n",
    "#             'freq': 49\n",
    "#         },\n",
    "#         {\n",
    "#             'path1': '../dataset/сокращение по частотности/1г_Изъяты лексемы с частотой выше 29.txt',\n",
    "#             'path2': '../dataset/сокращение по частотности/2г_Изъяты лексемы с частотой выше 29.txt',\n",
    "#             'name': 'Изъяты лексемы с частотой выше 29',\n",
    "#             'type': 'freq',\n",
    "#             'freq': 29\n",
    "#         },\n",
    "#         {\n",
    "#             'path1': '../dataset/сокращение по частотности/1д_Изъяты лексемы с частотой выше 9.txt',\n",
    "#             'path2': '../dataset/сокращение по частотности/2д_Изъяты лексемы с частотой выше 9.txt',\n",
    "#             'name': 'Изъяты лексемы с частотой выше 9',\n",
    "#             'type': 'freq',\n",
    "#             'freq': 9\n",
    "#         },\n",
    "#         {\n",
    "#             'path1': '../dataset/сокращение по частотности/1е_Изъяты лексемы с частотой выше 5.txt',\n",
    "#             'path2': '../dataset/сокращение по частотности/2е_Изъяты лексемы с частотой выше 5.txt',\n",
    "#             'name': 'Изъяты лексемы с частотой выше 5',\n",
    "#             'type': 'freq',\n",
    "#             'freq': 5\n",
    "#         },\n",
    "#         {\n",
    "#             'path1': '../dataset/сокращение по частотности/1ё_Изъяты лексемы с частотой выше 3.txt',\n",
    "#             'path2': '../dataset/сокращение по частотности/2ё_Изъяты лексемы с частотой выше 3.txt',\n",
    "#             'name': 'Изъяты лексемы с частотой выше 3',\n",
    "#             'type': 'freq',\n",
    "#             'freq': 3\n",
    "#         },\n",
    "\n",
    "#             {\n",
    "#             'path1': '../dataset/Сокращение по частям речи/Без прилагательных первый жанр.txt',\n",
    "#             'path2': '../dataset/Сокращение по частям речи/Без прилагательных второй жанр.txt',\n",
    "#             'name': 'Без прилагательных второй жанр',\n",
    "#             'type': 'pos',\n",
    "#             'freq': None\n",
    "#             },\n",
    "#           {\n",
    "#             'path1': '../dataset/Сокращение по частям речи/1.Первый жанр исходная выборка.txt',\n",
    "#             'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "#             'name': '1.Первый жанр исходная выборка',\n",
    "#             'type': 'pos',\n",
    "#             'freq': None\n",
    "\n",
    "#         },\n",
    "#         {\n",
    "#             'path1': '../dataset/Сокращение по частям речи/2.Первый жанр без клауз, включающих наречия.txt',\n",
    "#             'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "#             'name': '2.Первый жанр без клауз, включающих наречия',\n",
    "#             'type': 'pos',\n",
    "#             'freq': None\n",
    "#         },\n",
    "#         {\n",
    "#             'path1': '../dataset/Сокращение по частям речи/3.Первый жанр без клауз, включающих глаголы.txt',\n",
    "#             'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "#             'name': '3.Первый жанр без клауз, включающих глаголы',\n",
    "#             'type': 'pos',\n",
    "#             'freq': None\n",
    "#         },\n",
    "#         {\n",
    "#             'path1': '../dataset/Сокращение по частям речи/4.Первый жанр без клауз, включающих глаголы и наречия.txt',\n",
    "#             'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "#             'name': '4.Первый жанр без клауз, включающих глаголы и наречия',\n",
    "#             'type': 'pos',\n",
    "#             'freq': None\n",
    "#         },\n",
    "    ]\n",
    "\n",
    "def main():\n",
    "\n",
    "    results = []\n",
    "    for model in models:\n",
    "        for dataset in datasets:\n",
    "            config = create_custom_config(model['model'], model['type'], dataset)\n",
    "            \n",
    "            \n",
    "            # Entrenar y limpiar memoria\n",
    "            torch.cuda.empty_cache()\n",
    "            result = funciones.train_and_evaluate_dataset(\n",
    "                dataset['path1'],\n",
    "                dataset['path2'],\n",
    "                config,\n",
    "                dataset['name'],\n",
    "                dataset['type']\n",
    "                \n",
    "            )\n",
    "            results.append(result) \n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    \n",
    "    save_results(results, 'model_results_GPT_bert_all.json')    \n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd31457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dfbb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1ac659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import IPython\n",
    "\n",
    "# Reiniciar el kernel\n",
    "IPython.display.display(IPython.display.Javascript(\"Jupyter.notebook.kernel.restart()\"))\n",
    "\n",
    "# Apagar el kernel después de reiniciar\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8fee8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22f6a60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
