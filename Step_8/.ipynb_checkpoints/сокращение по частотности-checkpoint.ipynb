{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e66a70c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m../dataset\u001b[00m\r\n",
      "├── \u001b[01;34men_espanol\u001b[00m\r\n",
      "│   ├── docx2txt.py\r\n",
      "│   ├── Второй_жанр_исходная.txt\r\n",
      "│   └── Первый_жанр_исходная.txt\r\n",
      "├── Второй_жанр_исходная.txt\r\n",
      "├── Первый_жанр_исходная.txt\r\n",
      "├── \u001b[01;34mСокращение по частям речи\u001b[00m\r\n",
      "│   ├── 1.Первый жанр исходная выборка.txt\r\n",
      "│   ├── 2.Первый жанр без клауз, включающих наречия.txt\r\n",
      "│   ├── 3.Первый жанр без клауз, включающих глаголы.txt\r\n",
      "│   ├── 4.Первый жанр без клауз, включающих глаголы и наречия.txt\r\n",
      "│   ├── Без прилагательных второй жанр.txt\r\n",
      "│   ├── Без прилагательных первый жанр.txt\r\n",
      "│   └── Случайные выборки.txt\r\n",
      "└── \u001b[01;34mсокращение по частотности\u001b[00m\r\n",
      "    ├── 1а_ без сокращений.txt\r\n",
      "    ├── 1б_Изъяты лексемы с частотой выше 100.txt\r\n",
      "    ├── 1в_Изъяты лексемы с частотой выше 49.txt\r\n",
      "    ├── 1г_Изъяты лексемы с частотой выше 29.txt\r\n",
      "    ├── 1д_Изъяты лексемы с частотой выше 9.txt\r\n",
      "    ├── 1е_Изъяты лексемы с частотой выше 5.txt\r\n",
      "    ├── 1ё_Изъяты лексемы с частотой выше 3.txt\r\n",
      "    ├── 2а_ без сокращений.txt\r\n",
      "    ├── 2б_Изъяты лексемы с частотой выше 100.txt\r\n",
      "    ├── 2в_Изъяты лексемы с частотой выше 49.txt\r\n",
      "    ├── 2г_Изъяты лексемы с частотой выше 29.txt\r\n",
      "    ├── 2д_Изъяты лексемы с частотой выше 9.txt\r\n",
      "    ├── 2е_Изъяты лексемы с частотой выше 5.txt\r\n",
      "    └── 2ё_Изъяты лексемы с частотой выше 3.txt\r\n",
      "\r\n",
      "3 directories, 26 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree ../dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accb463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \u001b[0m\u001b[01;34m__pycache__\u001b[0m/                          model_results.json\r\n",
      " frecuencia.png                        parte_2.png\r\n",
      " frecuencia_lineas.png                 parte_2_Step1.png\r\n",
      " frecuencia_profesional.png            partes_discurso.png\r\n",
      " frecuencia_profesional_ajustada.png   utils.py\r\n",
      " frecuencia_step_1.png                 \u001b[01;34mwandb\u001b[0m/\r\n",
      " frecuencia_test_1.png                \u001b[01;34m'Сокращение по частям речи'\u001b[0m\u001b[K/\r\n",
      " funciones.py                         \u001b[01;34m'сокращение по частотности'\u001b[0m\u001b[K/\r\n",
      " generador.ipynb                      'сокращение по частотности.ipynb'\r\n",
      " herramientas.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b299fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_results(results, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c11038bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 22:03:31.284835: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtuesta-lx\u001b[0m (\u001b[33mtuesta-lx-moscow-institute-of-physics-and-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/Carlos/fine_tunig_project/thesis/Step_8/wandb/run-20250414_220334-v2ah52d4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tuesta-lx-moscow-institute-of-physics-and-technology/model-comparison-frecuency/runs/v2ah52d4' target=\"_blank\">faithful-smoke-32</a></strong> to <a href='https://wandb.ai/tuesta-lx-moscow-institute-of-physics-and-technology/model-comparison-frecuency' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tuesta-lx-moscow-institute-of-physics-and-technology/model-comparison-frecuency' target=\"_blank\">https://wandb.ai/tuesta-lx-moscow-institute-of-physics-and-technology/model-comparison-frecuency</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tuesta-lx-moscow-institute-of-physics-and-technology/model-comparison-frecuency/runs/v2ah52d4' target=\"_blank\">https://wandb.ai/tuesta-lx-moscow-institute-of-physics-and-technology/model-comparison-frecuency/runs/v2ah52d4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">faithful-smoke-32</strong> at: <a href='https://wandb.ai/tuesta-lx-moscow-institute-of-physics-and-technology/model-comparison-frecuency/runs/v2ah52d4' target=\"_blank\">https://wandb.ai/tuesta-lx-moscow-institute-of-physics-and-technology/model-comparison-frecuency/runs/v2ah52d4</a><br> View project at: <a href='https://wandb.ai/tuesta-lx-moscow-institute-of-physics-and-technology/model-comparison-frecuency' target=\"_blank\">https://wandb.ai/tuesta-lx-moscow-institute-of-physics-and-technology/model-comparison-frecuency</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250414_220334-v2ah52d4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/Carlos/fine_tunig_project/thesis/Step_8/wandb/run-20250414_220335-48hwy8qf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tuesta-lx-moscow-institute-of-physics-and-technology/model-comparison/runs/48hwy8qf' target=\"_blank\">Изъяты лексемы с частотой выше 100 - roberta-base</a></strong> to <a href='https://wandb.ai/tuesta-lx-moscow-institute-of-physics-and-technology/model-comparison' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tuesta-lx-moscow-institute-of-physics-and-technology/model-comparison' target=\"_blank\">https://wandb.ai/tuesta-lx-moscow-institute-of-physics-and-technology/model-comparison</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tuesta-lx-moscow-institute-of-physics-and-technology/model-comparison/runs/48hwy8qf' target=\"_blank\">https://wandb.ai/tuesta-lx-moscow-institute-of-physics-and-technology/model-comparison/runs/48hwy8qf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Изъяты лексемы с частотой выше 100.csv\n",
      "test_Изъяты лексемы с частотой выше 100.csv\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 356\u001b[0m\n\u001b[1;32m    353\u001b[0m     save_results(results, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_results.json\u001b[39m\u001b[38;5;124m'\u001b[39m)    \n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 356\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 325\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# Entrenar y limpiar memoria\u001b[39;00m\n\u001b[1;32m    324\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m--> 325\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunciones\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_evaluate_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_run\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwandb_run\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(result) \n\u001b[1;32m    334\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_avg_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_std_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_avg_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    338\u001b[0m })\n",
      "File \u001b[0;32m/workspace/notebooks/Carlos/fine_tunig_project/thesis/Step_8/funciones.py:606\u001b[0m, in \u001b[0;36mtrain_and_evaluate_dataset\u001b[0;34m(path1, path2, config, dataset_name, wandb_run)\u001b[0m\n\u001b[1;32m    594\u001b[0m         optimizer \u001b[38;5;241m=\u001b[39m AdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlearning_rate)\n\u001b[1;32m    596\u001b[0m         \u001b[38;5;66;03m# Entrenar y evaluar\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;66;03m#         def train_model(model,\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m#                         train_loader,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;66;03m#                         model_type='bert'\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m#                         ,wandb_run=None):\u001b[39;00m\n\u001b[0;32m--> 606\u001b[0m         final_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mwandb_run\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwandb_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m                                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m         losses\u001b[38;5;241m.\u001b[39mappend(final_loss)\n\u001b[1;32m    620\u001b[0m         results \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_loader, config\u001b[38;5;241m.\u001b[39mdevice, \n\u001b[1;32m    621\u001b[0m                                config\u001b[38;5;241m.\u001b[39mtokenizer, config\u001b[38;5;241m.\u001b[39mmodel_type)\n",
      "File \u001b[0;32m/workspace/notebooks/Carlos/fine_tunig_project/thesis/Step_8/funciones.py:452\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, optimizer, loss_fn, device, epochs, model_type, wandb_run, dataset_name, model_name)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wandb_run:\n\u001b[1;32m    448\u001b[0m         wandb\u001b[38;5;241m.\u001b[39mlog({\n\u001b[1;32m    449\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/train/batch_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    450\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: epoch \u001b[38;5;241m+\u001b[39m (batch_idx\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader))  \u001b[38;5;66;03m# Log fractional epoch\u001b[39;00m\n\u001b[1;32m    451\u001b[0m         })\n\u001b[0;32m--> 452\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtotal_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtotal_batches\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# Log epoch metrics\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wandb_run:\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "import funciones\n",
    "from utils import train_wrapper\n",
    "import warnings\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "from contextlib import redirect_stderr\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import wandb\n",
    "import nbformat\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import gc\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from razdel import sentenize\n",
    "import numpy as np\n",
    "\n",
    "# Suprimir warnings específicos\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "def save_results(results, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "# import funciones\n",
    "# from utils import train_wrapper\n",
    "# import warnings\n",
    "# import os\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Suprimir warnings específicos\n",
    "# warnings.filterwarnings('ignore', category=UserWarning)  # Para sklearn y otros\n",
    "# warnings.filterwarnings('ignore', category=FutureWarning)  # Para huggingface y transformers\n",
    "# from concurrent.futures import ProcessPoolExecutor\n",
    "# from tqdm import tqdm\n",
    "# from contextlib import redirect_stderr\n",
    "# from concurrent.futures import ProcessPoolExecutor\n",
    "# import multiprocessing as mp\n",
    "# import numpy as np\n",
    "# mp.set_start_method('spawn', force=True)\n",
    "# #os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "\n",
    "###############################################################################\n",
    "import wandb\n",
    "import nbformat\n",
    "# Configurar la clave de la API como variable de entorno\n",
    "os.environ[\"WANDB_API_KEY\"] = \"ca85316ed713b2425615fb3a613d7eb414c9f57f\"  # Reemplaza con tu clave\n",
    "# Iniciar wandb sin especificar entity (se detecta automáticamente)\n",
    "wandb.init(project=\"model-clasification\")\n",
    "#os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"сокращение по частотности.ipynb\"\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "# def train_wrapper(args):\n",
    "#     dataset, config = args  # Desempaquetar los argumentos\n",
    "#     result = funciones.train_and_evaluate_dataset(\n",
    "#         dataset['path1'],\n",
    "#         dataset['path2'],\n",
    "#         config,\n",
    "#         dataset['name'],\n",
    "#     )\n",
    "#     result['type'] = dataset['type']\n",
    "#     return result\n",
    "#     \"\"\"\n",
    "#       return {\n",
    "#         'dataset_name': dataset_name,\n",
    "#         'avg_accuracy': avg_accuracy,\n",
    "#         'std_accuracy': std_accuracy,\n",
    "#         'accuracies': accuracies,\n",
    "#         'type': type\n",
    "#         }\n",
    "#     \"\"\"\n",
    "\n",
    "###########################################################################\n",
    "def create_custom_config(dataset, model_name, model_type):\n",
    "    base_config = {\n",
    "        'model_name': model_name,\n",
    "        'num_repeats': 6,\n",
    "        'test_size': 0.2,\n",
    "        'threshold': 0.5,\n",
    "        'model_type': model_type\n",
    "    }\n",
    "    if model_type == 'gpt':\n",
    "        return funciones.TrainingConfig(\n",
    "            **base_config,\n",
    "            max_length=512,  # GPT models typically handle longer sequences\n",
    "            batch_size=4,    # Smaller batch size for GPT due to memory constraints\n",
    "            epochs=2,        # Fewer epochs for few-shot learning\n",
    "            learning_rate=1e-5\n",
    "        )\n",
    "    \n",
    "    if dataset.get('freq_threshold') == 100:\n",
    "        return funciones.TrainingConfig(\n",
    "            **base_config,\n",
    "            max_length=52,\n",
    "            batch_size=128,\n",
    "            epochs=3,\n",
    "            learning_rate=2e-5\n",
    "        )\n",
    "    elif dataset.get('freq_threshold') == 49:\n",
    "        return funciones.TrainingConfig(\n",
    "            **base_config,\n",
    "            max_length=60,\n",
    "            batch_size=128,\n",
    "            epochs=4,\n",
    "            learning_rate=3e-5\n",
    "        )\n",
    "    elif dataset.get('freq_threshold') == 29:\n",
    "        return funciones.TrainingConfig(\n",
    "            **base_config,\n",
    "            max_length=51,\n",
    "            batch_size=128,\n",
    "            epochs=4,\n",
    "            learning_rate=3e-5\n",
    "        )\n",
    "    elif dataset.get('freq_threshold') == 9:\n",
    "        return funciones.TrainingConfig(\n",
    "            **base_config,\n",
    "            max_length=45,\n",
    "            batch_size=128,\n",
    "            epochs=5,\n",
    "            learning_rate=4e-5\n",
    "        )\n",
    "    elif dataset.get('freq_threshold') == 5:\n",
    "        return funciones.TrainingConfig(\n",
    "            **base_config,\n",
    "            max_length=150,\n",
    "            batch_size=32,\n",
    "            epochs=6,\n",
    "            learning_rate=5e-5\n",
    "        )\n",
    "    elif dataset.get('freq_threshold') == 3:\n",
    "        return funciones.TrainingConfig(\n",
    "            **base_config,\n",
    "            max_length=150,\n",
    "            batch_size=32,\n",
    "            epochs=6,\n",
    "            learning_rate=5e-5\n",
    "        )\n",
    "    # Second dataset naming pattern\n",
    "    elif \"1\" in dataset.get('name', ''):\n",
    "        return funciones.TrainingConfig(\n",
    "            **base_config,\n",
    "            max_length=60,\n",
    "            batch_size=128,\n",
    "            epochs=6,\n",
    "            learning_rate=5e-5\n",
    "        )\n",
    "    elif \"2\" in dataset.get('name', ''):\n",
    "        return funciones.TrainingConfig(\n",
    "            **base_config,\n",
    "            max_length=60,\n",
    "            batch_size=128,\n",
    "            epochs=6,\n",
    "            learning_rate=5e-5\n",
    "        )\n",
    "    elif \"3\" in dataset.get('name', ''):\n",
    "        return funciones.TrainingConfig(\n",
    "            **base_config,\n",
    "            max_length=60,\n",
    "            batch_size=128,\n",
    "            epochs=6,\n",
    "            learning_rate=5e-5\n",
    "        )\n",
    "    elif \"4\" in dataset.get('name', ''):\n",
    "        return funciones.TrainingConfig(\n",
    "            **base_config,\n",
    "            max_length=60,\n",
    "            batch_size=128,\n",
    "            epochs=6,\n",
    "            learning_rate=5e-5\n",
    "        )\n",
    "\n",
    "    # Default configuration if no conditions match\n",
    "    return funciones.TrainingConfig(\n",
    "        **base_config,\n",
    "        max_length=60,\n",
    "        batch_size=128,\n",
    "        epochs=4,\n",
    "        learning_rate=3e-5\n",
    "    )\n",
    "        \n",
    "\n",
    "models = [\n",
    "#         {'model':'DeepPavlov/rubert-base-cased',\n",
    "#          'name':'rubert-base',\n",
    "#          'type': 'bert'},# Modelo original\n",
    "#         {'model':'bert-base-multilingual-cased',\n",
    "#          'name':'BERT multilingual',\n",
    "#          'type': 'bert'},# BERT multilingüe\n",
    "#         {'model':'distilbert-base-multilingual-cased',\n",
    "#          'name':'distilbert-base-multilingual',\n",
    "#          'type': 'bert'},# Versión ligera de BERT\n",
    "        {'model':'roberta-base',\n",
    "         'name':'roberta-base', \n",
    "        'type': 'bert'}, # RoBERTa \n",
    "#         {'model': 'gpt2',\n",
    "#          'name': 'gpt2',\n",
    "#          'type': 'gpt'},  #  GPT model\n",
    "#         {'model': 'facebook/opt-125m',\n",
    "#          'name': 'opt-125m',\n",
    "#          'type': 'gpt'},  #  GPT model\n",
    "#         {'model': 'sberbank-ai/rugpt3small_based_on_gpt2',\n",
    "#          'name': 'rugpt3',\n",
    "#          'type': 'gpt'}\n",
    "    ]\n",
    "\n",
    "datasets = [\n",
    "        {\n",
    "            'path1': '../dataset/сокращение по частотности/1б_Изъяты лексемы с частотой выше 100.txt',\n",
    "            'path2': '../dataset/сокращение по частотности/2б_Изъяты лексемы с частотой выше 100.txt',\n",
    "            'name': 'Изъяты лексемы с частотой выше 100',\n",
    "            'type': 'freq',\n",
    "            'freq': 100\n",
    "        },\n",
    "        {\n",
    "            'path1': '../dataset/сокращение по частотности/1в_Изъяты лексемы с частотой выше 49.txt',\n",
    "            'path2': '../dataset/сокращение по частотности/2в_Изъяты лексемы с частотой выше 49.txt',\n",
    "            'name': 'Изъяты лексемы с частотой выше 49',\n",
    "            'type': 'freq',\n",
    "            'freq': 49\n",
    "        },\n",
    "#         {\n",
    "#             'path1': '../dataset/сокращение по частотности/1г_Изъяты лексемы с частотой выше 29.txt',\n",
    "#             'path2': '../dataset/сокращение по частотности/2г_Изъяты лексемы с частотой выше 29.txt',\n",
    "#             'name': 'Изъяты лексемы с частотой выше 29',\n",
    "#             'type': 'freq',\n",
    "#             'freq': 29\n",
    "#         },\n",
    "#         {\n",
    "#             'path1': '../dataset/сокращение по частотности/1д_Изъяты лексемы с частотой выше 9.txt',\n",
    "#             'path2': '../dataset/сокращение по частотности/2д_Изъяты лексемы с частотой выше 9.txt',\n",
    "#             'name': 'Изъяты лексемы с частотой выше 9',\n",
    "#             'type': 'freq',\n",
    "#             'freq': 9\n",
    "#         },\n",
    "#         {\n",
    "#             'path1': '../dataset/сокращение по частотности/1е_Изъяты лексемы с частотой выше 5.txt',\n",
    "#             'path2': '../dataset/сокращение по частотности/2е_Изъяты лексемы с частотой выше 5.txt',\n",
    "#             'name': 'Изъяты лексемы с частотой выше 5',\n",
    "#             'type': 'freq',\n",
    "#             'freq': 5\n",
    "#         },\n",
    "#         {\n",
    "#             'path1': '../dataset/сокращение по частотности/1ё_Изъяты лексемы с частотой выше 3.txt',\n",
    "#             'path2': '../dataset/сокращение по частотности/2ё_Изъяты лексемы с частотой выше 3.txt',\n",
    "#             'name': 'Изъяты лексемы с частотой выше 3',\n",
    "#             'type': 'freq',\n",
    "#             'freq': 3\n",
    "#         },\n",
    "#             {\n",
    "#             'path1': '../dataset/Сокращение по частям речи/1.Первый жанр исходная выборка.txt',\n",
    "#             'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "#             'name': '1.Первый жанр исходная выборка',\n",
    "#             'type': 'pos',\n",
    "#             'freq': None\n",
    "\n",
    "#         },\n",
    "#         {\n",
    "#             'path1': '../dataset/Сокращение по частям речи/2.Первый жанр без клауз, включающих наречия.txt',\n",
    "#             'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "#             'name': '2.Первый жанр без клауз, включающих наречия',\n",
    "#             'type': 'pos',\n",
    "#             'freq': None\n",
    "#         },\n",
    "        {\n",
    "            'path1': '../dataset/Сокращение по частям речи/3.Первый жанр без клауз, включающих глаголы.txt',\n",
    "            'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "            'name': '3.Первый жанр без клауз, включающих глаголы',\n",
    "            'type': 'pos',\n",
    "            'freq': None\n",
    "        },\n",
    "        {\n",
    "            'path1': '../dataset/Сокращение по частям речи/4.Первый жанр без клауз, включающих глаголы и наречия.txt',\n",
    "            'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "            'name': '4.Первый жанр без клауз, включающих глаголы и наречия',\n",
    "            'type': 'pos',\n",
    "            'freq': None\n",
    "        },\n",
    "    ]\n",
    "\n",
    "def main():\n",
    "\n",
    "    results = []\n",
    "    for model in models:\n",
    "        for dataset in datasets:\n",
    "            dataset_copy = dataset.copy()\n",
    "            dataset_copy['name'] = f\"{dataset['name']} - {model['name']}\"\n",
    "            config = create_custom_config(dataset, model['model'], model['type'])\n",
    "            args = (dataset_copy, config)\n",
    "            \n",
    "            # Configurar wandb para este experimento\n",
    "          \n",
    "            wandb_run = wandb.init(\n",
    "                project=\"model-comparison\",\n",
    "                name=f\"{dataset['name']} - {model['name']}\",\n",
    "                group=f\"{dataset['type']}_{dataset.get('freq', 'base')}\",\n",
    "                config={\n",
    "                    \"model\": model['model'],\n",
    "                    \"model_type\": model['type'],\n",
    "                    \"dataset\": dataset['name'],\n",
    "                    \"type\": dataset['type'],\n",
    "                    \"freq\": dataset.get('freq', None),\n",
    "                    **config.__dict__\n",
    "                },\n",
    "                reinit=True\n",
    "            )\n",
    "            \n",
    "            # Entrenar y limpiar memoria\n",
    "            torch.cuda.empty_cache()\n",
    "            result = funciones.train_and_evaluate_dataset(\n",
    "                dataset['path1'],\n",
    "                dataset['path2'],\n",
    "                config,\n",
    "                dataset['name'],\n",
    "                wandb_run=wandb_run\n",
    "            )\n",
    "            results.append(result) \n",
    "            \n",
    "            wandb.log({\n",
    "                \"final_avg_accuracy\": result['avg_accuracy'],\n",
    "                \"final_std_accuracy\": result['std_accuracy'],\n",
    "                \"final_avg_loss\": result['avg_loss']\n",
    "            })\n",
    "            \n",
    "            # Crear tablas de resumen\n",
    "            summary_table = wandb.Table(columns=[\"Metric\", \"Value\"])\n",
    "            summary_table.add_data(\"Avg Accuracy\", result['avg_accuracy'])\n",
    "            summary_table.add_data(\"Std Accuracy\", result['std_accuracy'])\n",
    "            summary_table.add_data(\"Avg Loss\", result['avg_loss'])\n",
    "            \n",
    "            wandb.log({\n",
    "                f\"summary/{dataset['name']}_{clean_model_name}\": summary_table,\n",
    "                \"epoch\": config.epochs  # Para alinear con otras métricas\n",
    "            })\n",
    "            \n",
    "            wandb_run.finish()\n",
    "        \n",
    "    save_results(results, 'model_results.json')    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927a716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd31457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size=128v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dfbb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1ac659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import IPython\n",
    "\n",
    "# Reiniciar el kernel\n",
    "IPython.display.display(IPython.display.Javascript(\"Jupyter.notebook.kernel.restart()\"))\n",
    "\n",
    "# Apagar el kernel después de reiniciar\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0cb97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def plot_model_performance(datasets, models, results, save_path='frecuencia_profesional.png'):\n",
    "    \"\"\"\n",
    "    Genera un gráfico de barras comparando el rendimiento de modelos por dataset.\n",
    "\n",
    "    Args:\n",
    "        datasets (list): Lista de diccionarios con nombres de datasets (cada uno con clave 'name').\n",
    "        models (list): Lista de diccionarios con nombres de modelos (cada uno con clave 'name').\n",
    "        results (list): Lista de resultados con 'dataset_name', 'model_name' y 'avg_accuracy'.\n",
    "        save_path (str): Ruta donde guardar el gráfico (por defecto 'frecuencia_profesional.png').\n",
    "    \"\"\"\n",
    "    # Extraer nombres\n",
    "    dataset_names = [d['name'] for d in datasets]\n",
    "    model_names = [m['name'] for m in models]\n",
    "\n",
    "    # Crear matriz de precisiones\n",
    "    accuracies = np.zeros((len(dataset_names), len(model_names)))\n",
    "    for i, dataset_name in enumerate(dataset_names):\n",
    "        for j, model_name in enumerate(model_names):\n",
    "            for result in results:\n",
    "                if dataset_name in result['dataset_name'] and model_name in result['model_name']:\n",
    "                    accuracies[i, j] = result['avg_accuracy']\n",
    "                    break  # Salir del bucle una vez encontrada la precisión\n",
    "\n",
    "    # Configuración de estilo profesional\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "\n",
    "    # Crear figura con mayor resolución\n",
    "    fig, ax = plt.subplots(figsize=(20, 8), dpi=100)\n",
    "\n",
    "    # Parámetros dinámicos según cantidad de modelos\n",
    "    n_models = len(model_names)\n",
    "    n_datasets = len(dataset_names)\n",
    "\n",
    "    # Ajustar el ancho de las barras en función del número de modelos y datasets\n",
    "    # Reducir el ancho máximo para evitar que los números se vean demasiado juntos\n",
    "    max_bar_width = 0.8 / (n_models * 1.2) if n_models > 0 else 0.15\n",
    "    bar_width = min(max_bar_width, 0.1)\n",
    "    index = np.arange(n_datasets)\n",
    "    colors = cm.Set2(np.linspace(0, 1, n_models))  # Paleta de colores profesional\n",
    "\n",
    "    # Espacio entre grupos de barras (datasets)\n",
    "    group_spacing = 0.5\n",
    "\n",
    "    # Calcular el ancho total ocupado por las barras de un dataset\n",
    "    total_bar_width_per_dataset = n_models * bar_width\n",
    "\n",
    "    # Calcular el desplazamiento para centrar el grupo de barras de cada dataset\n",
    "    group_offset = (total_bar_width_per_dataset + (n_models - 1) * 0.02) / 2 if n_models > 1 else bar_width / 2 # Añadir un pequeño espacio entre barras del mismo dataset\n",
    "\n",
    "    # Crear barras con mejoras visuales\n",
    "    for i, (model_name, color) in enumerate(zip(model_names, colors)):\n",
    "        positions = index + (i - (n_models - 1) / 2) * bar_width\n",
    "        bars = ax.bar(positions,\n",
    "                       accuracies[:, i],\n",
    "                       bar_width,\n",
    "                       label=model_name,\n",
    "                       color=color,\n",
    "                       edgecolor='black',\n",
    "                       alpha=0.8)\n",
    "\n",
    "        # Añadir valores encima de cada barra solo si no hay demasiados modelos\n",
    "        if n_models <= 10:  # Límite para evitar clutter\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                        f'{height:.3f}',\n",
    "                        ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    # Personalización de ejes\n",
    "    plt.xlabel('Dataset', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Precisión Promedio', fontsize=12, fontweight='bold')\n",
    "    plt.title('Rendimiento de Modelos por Dataset', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "    # Ajustar marcas del eje X\n",
    "    plt.xticks(index,\n",
    "               dataset_names,\n",
    "               rotation=45,\n",
    "               ha='right',\n",
    "               fontsize=10)\n",
    "\n",
    "    # Añadir grid\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Mover la leyenda fuera del gráfico\n",
    "    plt.legend(title='Modelos',\n",
    "               title_fontsize=12,\n",
    "               fontsize=10,\n",
    "               loc='center left',\n",
    "               bbox_to_anchor=(1.05, 0.5),\n",
    "               frameon=True,\n",
    "               borderaxespad=0.)\n",
    "\n",
    "    # Ajustar límites del eje Y\n",
    "    plt.ylim(0, max(accuracies.max() * 1.1, 1))\n",
    "\n",
    "    # Ajustar layout para incluir la leyenda externa\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Guardar con alta calidad\n",
    "    plt.savefig(save_path,\n",
    "                dpi=300,\n",
    "                bbox_inches='tight')\n",
    "\n",
    "    # Mostrar gráfico\n",
    "    plt.show()\n",
    "\n",
    "# Ejemplo de uso:\n",
    "datasets = [{'name': 'dataset1'}, {'name': 'dataset2'}, {'name': 'dataset3'}, {'name': 'dataset4'}, {'name': 'dataset5'}]\n",
    "models = [{'name': 'model1'}, {'name': 'model2'}, {'name': 'model3'}, {'name': 'model4'}]\n",
    "results = [\n",
    "    {'dataset_name': 'dataset1', 'model_name': 'model1', 'avg_accuracy': 0.85},\n",
    "    {'dataset_name': 'dataset1', 'model_name': 'model2', 'avg_accuracy': 0.88},\n",
    "    {'dataset_name': 'dataset1', 'model_name': 'model3', 'avg_accuracy': 0.82},\n",
    "    {'dataset_name': 'dataset1', 'model_name': 'model4', 'avg_accuracy': 0.90},\n",
    "    {'dataset_name': 'dataset2', 'model_name': 'model1', 'avg_accuracy': 0.78},\n",
    "    {'dataset_name': 'dataset2', 'model_name': 'model2', 'avg_accuracy': 0.81},\n",
    "    {'dataset_name': 'dataset2', 'model_name': 'model3', 'avg_accuracy': 0.75},\n",
    "    {'dataset_name': 'dataset2', 'model_name': 'model4', 'avg_accuracy': 0.83},\n",
    "    {'dataset_name': 'dataset3', 'model_name': 'model1', 'avg_accuracy': 0.92},\n",
    "    {'dataset_name': 'dataset3', 'model_name': 'model2', 'avg_accuracy': 0.95},\n",
    "    {'dataset_name': 'dataset3', 'model_name': 'model3', 'avg_accuracy': 0.90},\n",
    "    {'dataset_name': 'dataset3', 'model_name': 'model4', 'avg_accuracy': 0.96},\n",
    "    {'dataset_name': 'dataset4', 'model_name': 'model1', 'avg_accuracy': 0.65},\n",
    "    {'dataset_name': 'dataset4', 'model_name': 'model2', 'avg_accuracy': 0.68},\n",
    "    {'dataset_name': 'dataset4', 'model_name': 'model3', 'avg_accuracy': 0.62},\n",
    "    {'dataset_name': 'dataset4', 'model_name': 'model4', 'avg_accuracy': 0.70},\n",
    "    {'dataset_name': 'dataset5', 'model_name': 'model1', 'avg_accuracy': 0.70},\n",
    "    {'dataset_name': 'dataset5', 'model_name': 'model2', 'avg_accuracy': 0.73},\n",
    "    {'dataset_name': 'dataset5', 'model_name': 'model3', 'avg_accuracy': 0.68},\n",
    "    {'dataset_name': 'dataset5', 'model_name': 'model4', 'avg_accuracy': 0.75},\n",
    "]\n",
    "plot_model_performance(datasets, models, results, save_path='frecuencia_profesional_ajustada.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe52c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mi Entorno (Python 3.9)",
   "language": "python",
   "name": "mi_entorno"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
