{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e66a70c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m../dataset\u001b[0m\r\n",
      "├── \u001b[01;34men_espanol\u001b[0m\r\n",
      "│   ├── \u001b[00mdocx2txt.py\u001b[0m\r\n",
      "│   ├── \u001b[00mВторой_жанр_исходная.txt\u001b[0m\r\n",
      "│   └── \u001b[00mПервый_жанр_исходная.txt\u001b[0m\r\n",
      "├── \u001b[00mВторой_жанр_исходная.txt\u001b[0m\r\n",
      "├── \u001b[00mПервый_жанр_исходная.txt\u001b[0m\r\n",
      "├── \u001b[01;34mСокращение по частям речи\u001b[0m\r\n",
      "│   ├── \u001b[00m1.Первый жанр исходная выборка.txt\u001b[0m\r\n",
      "│   ├── \u001b[00m2.Первый жанр без клауз, включающих наречия.txt\u001b[0m\r\n",
      "│   ├── \u001b[00m3.Первый жанр без клауз, включающих глаголы.txt\u001b[0m\r\n",
      "│   ├── \u001b[00m4.Первый жанр без клауз, включающих глаголы и наречия.txt\u001b[0m\r\n",
      "│   ├── \u001b[00mБез прилагательных второй жанр.txt\u001b[0m\r\n",
      "│   ├── \u001b[00mБез прилагательных первый жанр.txt\u001b[0m\r\n",
      "│   └── \u001b[00mСлучайные выборки.txt\u001b[0m\r\n",
      "└── \u001b[01;34mсокращение по частотности\u001b[0m\r\n",
      "    ├── \u001b[00m1а_ без сокращений.txt\u001b[0m\r\n",
      "    ├── \u001b[00m1б_Изъяты лексемы с частотой выше 100.txt\u001b[0m\r\n",
      "    ├── \u001b[00m1в_Изъяты лексемы с частотой выше 49.txt\u001b[0m\r\n",
      "    ├── \u001b[00m1г_Изъяты лексемы с частотой выше 29.txt\u001b[0m\r\n",
      "    ├── \u001b[00m1д_Изъяты лексемы с частотой выше 9.txt\u001b[0m\r\n",
      "    ├── \u001b[00m1е_Изъяты лексемы с частотой выше 5.txt\u001b[0m\r\n",
      "    ├── \u001b[00m1ё_Изъяты лексемы с частотой выше 3.txt\u001b[0m\r\n",
      "    ├── \u001b[00m2а_ без сокращений.txt\u001b[0m\r\n",
      "    ├── \u001b[00m2б_Изъяты лексемы с частотой выше 100.txt\u001b[0m\r\n",
      "    ├── \u001b[00m2в_Изъяты лексемы с частотой выше 49.txt\u001b[0m\r\n",
      "    ├── \u001b[00m2г_Изъяты лексемы с частотой выше 29.txt\u001b[0m\r\n",
      "    ├── \u001b[00m2д_Изъяты лексемы с частотой выше 9.txt\u001b[0m\r\n",
      "    ├── \u001b[00m2е_Изъяты лексемы с частотой выше 5.txt\u001b[0m\r\n",
      "    └── \u001b[00m2ё_Изъяты лексемы с частотой выше 3.txt\u001b[0m\r\n",
      "\r\n",
      "3 directories, 26 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree ../dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accb463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \u001b[0m\u001b[01;34m__pycache__\u001b[0m/                          loss_comparison.png\r\n",
      " confronto_prestazioni.png             model_results.json\r\n",
      " frecuencia.png                        model_results_GPT.json\r\n",
      " frecuencia_lineas.png                 parte_2.png\r\n",
      " frecuencia_profesional.png            parte_2_Step1.png\r\n",
      " frecuencia_profesional_ajustada.png   partes_discurso.png\r\n",
      " frecuencia_step_1.png                 performance_comparison_other.png\r\n",
      " frecuencia_test_1.png                 pos_analysis.png\r\n",
      " freq_analysis.png                     utils.py\r\n",
      " freq_comparison.png                   \u001b[01;34mwandb\u001b[0m/\r\n",
      " funciones.py                         \u001b[01;34m'сокращение по частотности'\u001b[0m\u001b[K/\r\n",
      " generador.ipynb                      'сокращение по частотности.ipynb'\r\n",
      " herramientas.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b299fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_results(results, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c11038bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Изъяты лексемы с частотой выше 100.csv\n",
      "test_Изъяты лексемы с частотой выше 100.csv\n",
      "Train labels sample: tensor([1, 1, 1, 0, 0]), Shape: torch.Size([871])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([190])\n",
      "Epoch 1, Loss: 0.9272108503750393\n",
      "Epoch 2, Loss: 0.7021433455603463\n",
      "Epoch 3, Loss: 0.6637476852961949\n",
      "Epoch 4, Loss: 0.6615314568792071\n",
      "Epoch 5, Loss: 0.6553323439189366\n",
      "Epoch 6, Loss: 0.662190990788596\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 309\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m#     funciones.plot_model_performance(datasets,\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m#                                      models,\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m#                                      results,\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m#                                      save_path='frecuencia_profesional.png'\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m#                                     )\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 309\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 268\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[38;5;66;03m# Configurar wandb para este experimento\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m#             wandb_run = wandb.init(\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m#                 project=\"model-comparison\",\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \n\u001b[1;32m    266\u001b[0m             \u001b[38;5;66;03m# Entrenar y limpiar memoria\u001b[39;00m\n\u001b[1;32m    267\u001b[0m             torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m--> 268\u001b[0m             result \u001b[38;5;241m=\u001b[39m \u001b[43mfunciones\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_evaluate_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m                \u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;43;03m#                 wandb_run=wandb_run\u001b[39;49;00m\n\u001b[1;32m    276\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m             results\u001b[38;5;241m.\u001b[39mappend(result) \n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m#             wandb.log({\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m#                 \"final_avg_accuracy\": result['avg_accuracy'],\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m#                 \"final_std_accuracy\": result['std_accuracy'],\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m#             wandb_run.finish()\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/notebooks/thesis/Step_8/funciones.py:571\u001b[0m, in \u001b[0;36mtrain_and_evaluate_dataset\u001b[0;34m(path1, path2, config, dataset_name, dataset_type)\u001b[0m\n\u001b[1;32m    566\u001b[0m final_loss \u001b[38;5;241m=\u001b[39m train_model(model, train_loader, optimizer, loss_fn, config\u001b[38;5;241m.\u001b[39mdevice, \n\u001b[1;32m    567\u001b[0m                         epochs\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mepochs, model_type\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmodel_type, \n\u001b[1;32m    568\u001b[0m                         dataset_name\u001b[38;5;241m=\u001b[39mdataset_name, model_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmodel_name, classifier\u001b[38;5;241m=\u001b[39mclassifier)\n\u001b[1;32m    569\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(final_loss)\n\u001b[0;32m--> 571\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m accuracies\u001b[38;5;241m.\u001b[39mappend(results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    575\u001b[0m f1_weighteds\u001b[38;5;241m.\u001b[39mappend(results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_weighted\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/workspace/notebooks/thesis/Step_8/funciones.py:498\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, test_loader, device, tokenizer, model_type, classifier)\u001b[0m\n\u001b[1;32m    496\u001b[0m report \u001b[38;5;241m=\u001b[39m classification_report(all_labels, all_preds, target_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mжанр0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mжанр1\u001b[39m\u001b[38;5;124m\"\u001b[39m], output_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    497\u001b[0m conf_matrix \u001b[38;5;241m=\u001b[39m confusion_matrix(all_labels, all_preds)\n\u001b[0;32m--> 498\u001b[0m roc_auc \u001b[38;5;241m=\u001b[39m roc_auc_score(all_labels, \u001b[43mall_probs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)  \u001b[38;5;66;03m# Probabilidad de clase 1\u001b[39;00m\n\u001b[1;32m    499\u001b[0m pr_auc \u001b[38;5;241m=\u001b[39m average_precision_score(all_labels, all_probs[:, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    500\u001b[0m log_loss_val \u001b[38;5;241m=\u001b[39m log_loss(all_labels, all_probs)\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "import funciones\n",
    "from utils import train_wrapper\n",
    "import warnings\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "from contextlib import redirect_stderr\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "#import wandb\n",
    "import nbformat\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import gc\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from razdel import sentenize\n",
    "import numpy as np\n",
    "\n",
    "# Suprimir warnings específicos\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "def save_results(results, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "# import funciones\n",
    "# from utils import train_wrapper\n",
    "# import warnings\n",
    "# import os\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Suprimir warnings específicos\n",
    "# warnings.filterwarnings('ignore', category=UserWarning)  # Para sklearn y otros\n",
    "# warnings.filterwarnings('ignore', category=FutureWarning)  # Para huggingface y transformers\n",
    "# from concurrent.futures import ProcessPoolExecutor\n",
    "# from tqdm import tqdm\n",
    "# from contextlib import redirect_stderr\n",
    "# from concurrent.futures import ProcessPoolExecutor\n",
    "# import multiprocessing as mp\n",
    "# import numpy as np\n",
    "# mp.set_start_method('spawn', force=True)\n",
    "# #os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "\n",
    "###############################################################################\n",
    "# import wandb\n",
    "# import nbformat\n",
    "# # Configurar la clave de la API como variable de entorno\n",
    "# os.environ[\"WANDB_API_KEY\"] = \"ca85316ed713b2425615fb3a613d7eb414c9f57f\"  # Reemplaza con tu clave\n",
    "# # Iniciar wandb sin especificar entity (se detecta automáticamente)\n",
    "# wandb.init(project=\"model-clasification\")\n",
    "# wandb.init(\n",
    "#     settings=wandb.Settings(\n",
    "#         start_method=\"thread\",\n",
    "#         timeout=30,\n",
    "#         sync_dir=\"/tmp/wandb\",  # Usa un directorio temporal\n",
    "#         disable_code=True       # Mejora la estabilidad\n",
    "#     )\n",
    "# )\n",
    "#os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"сокращение по частотности.ipynb\"\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def create_custom_config(model_name, model_type, dataset):\n",
    "    \"\"\"Crea configuración de entrenamiento adaptativa\"\"\"\n",
    "    common_config = {\n",
    "#         'dataset_name':dataset.get('name', ''),\n",
    "        'model_name': model_name,\n",
    "        'model_type': model_type,\n",
    "        'num_repeats': 6,\n",
    "        'test_size': 0.2,\n",
    "        'threshold': 0.5\n",
    "    }\n",
    "\n",
    "    # Configuración específica para GPT\n",
    "    if model_type == 'gpt':\n",
    "        return funciones.TrainingConfig(\n",
    "            **common_config,\n",
    "            max_length=512,\n",
    "            batch_size=4,\n",
    "            epochs=2,\n",
    "            learning_rate=1e-5\n",
    "        )\n",
    "\n",
    "    # Configuraciones basadas en frecuencia\n",
    "    freq_threshold = dataset.get('freq_threshold')\n",
    "    if freq_threshold in [100, 49, 29, 9, 5, 3]:\n",
    "        configs = {\n",
    "            100: (52, 128, 3, 2e-5),\n",
    "            49: (60, 128, 4, 3e-5),\n",
    "            29: (51, 128, 4, 3e-5),\n",
    "            9: (45, 128, 5, 4e-5),\n",
    "            5: (150, 32, 6, 5e-5),\n",
    "            3: (150, 32, 6, 5e-5)\n",
    "        }\n",
    "        max_len, batch, epochs, lr = configs[freq_threshold]\n",
    "        return funciones.TrainingConfig(\n",
    "            **common_config,\n",
    "            max_length=max_len,\n",
    "            batch_size=batch,\n",
    "            epochs=epochs,\n",
    "            learning_rate=lr\n",
    "        )\n",
    "\n",
    "    # Configuración por nombre de dataset\n",
    "    dataset_name = dataset.get('name', '')\n",
    "    if any(x in dataset_name for x in ['1', '2', '3', '4']):\n",
    "        return funciones.TrainingConfig(\n",
    "            **common_config,\n",
    "            max_length=60,\n",
    "            batch_size=128,\n",
    "            epochs=6,\n",
    "            learning_rate=5e-5\n",
    "        )\n",
    "\n",
    "    # Configuración por defecto\n",
    "    return funciones.TrainingConfig(\n",
    "        **common_config,\n",
    "        max_length=60,\n",
    "        batch_size=128,\n",
    "        epochs=4,\n",
    "        learning_rate=3e-5\n",
    "    )\n",
    "\n",
    "models = [\n",
    "#         {'model':'DeepPavlov/rubert-base-cased',\n",
    "#          'name':'DeepPavlov-rubert-base',\n",
    "#          'type': 'bert'},# Modelo original\n",
    "#         {'model':'bert-base-multilingual-cased',\n",
    "#          'name':'BERT multilingual',\n",
    "#          'type': 'bert'},# BERT multilingüe\n",
    "        {'model':'distilbert-base-multilingual-cased',\n",
    "         'name':'distilbert-base-multilingual',\n",
    "         'type': 'bert'},# Versión ligera de BERT\n",
    "#         {'model':'roberta-base',\n",
    "#          'name':'roberta-base', \n",
    "#         'type': 'bert'}, # RoBERTa \n",
    "        {'model': 'gpt2',\n",
    "         'name': 'gpt2',\n",
    "         'type': 'gpt'},  #  GPT model\n",
    "#         {'model': 'facebook/opt-125m',\n",
    "#          'name': 'facebook',\n",
    "#          'type': 'gpt'},  #  GPT model\n",
    "#         {'model': 'sberbank-ai/rugpt3small_based_on_gpt2',\n",
    "#          'name': 'rugpt3',\n",
    "#          'type': 'gpt'}\n",
    "    ]\n",
    "\n",
    "datasets = [\n",
    "        {\n",
    "            'path1': '../dataset/сокращение по частотности/1б_Изъяты лексемы с частотой выше 100.txt',\n",
    "            'path2': '../dataset/сокращение по частотности/2б_Изъяты лексемы с частотой выше 100.txt',\n",
    "            'name': 'Изъяты лексемы с частотой выше 100',\n",
    "            'type': 'freq',\n",
    "            'freq': 100\n",
    "        },\n",
    "#         {\n",
    "#             'path1': '../dataset/сокращение по частотности/1в_Изъяты лексемы с частотой выше 49.txt',\n",
    "#             'path2': '../dataset/сокращение по частотности/2в_Изъяты лексемы с частотой выше 49.txt',\n",
    "#             'name': 'Изъяты лексемы с частотой выше 49',\n",
    "#             'type': 'freq',\n",
    "#             'freq': 49\n",
    "#         },\n",
    "#         {\n",
    "#             'path1': '../dataset/сокращение по частотности/1г_Изъяты лексемы с частотой выше 29.txt',\n",
    "#             'path2': '../dataset/сокращение по частотности/2г_Изъяты лексемы с частотой выше 29.txt',\n",
    "#             'name': 'Изъяты лексемы с частотой выше 29',\n",
    "#             'type': 'freq',\n",
    "#             'freq': 29\n",
    "#         },\n",
    "#         {\n",
    "#             'path1': '../dataset/сокращение по частотности/1д_Изъяты лексемы с частотой выше 9.txt',\n",
    "#             'path2': '../dataset/сокращение по частотности/2д_Изъяты лексемы с частотой выше 9.txt',\n",
    "#             'name': 'Изъяты лексемы с частотой выше 9',\n",
    "#             'type': 'freq',\n",
    "#             'freq': 9\n",
    "#         },\n",
    "#         {\n",
    "#             'path1': '../dataset/сокращение по частотности/1е_Изъяты лексемы с частотой выше 5.txt',\n",
    "#             'path2': '../dataset/сокращение по частотности/2е_Изъяты лексемы с частотой выше 5.txt',\n",
    "#             'name': 'Изъяты лексемы с частотой выше 5',\n",
    "#             'type': 'freq',\n",
    "#             'freq': 5\n",
    "#         },\n",
    "#         {\n",
    "#             'path1': '../dataset/сокращение по частотности/1ё_Изъяты лексемы с частотой выше 3.txt',\n",
    "#             'path2': '../dataset/сокращение по частотности/2ё_Изъяты лексемы с частотой выше 3.txt',\n",
    "#             'name': 'Изъяты лексемы с частотой выше 3',\n",
    "#             'type': 'freq',\n",
    "#             'freq': 3\n",
    "#         },\n",
    "\n",
    "#             {\n",
    "#             'path1': '../dataset/Сокращение по частям речи/Без прилагательных первый жанр.txt',\n",
    "#             'path2': '../dataset/Сокращение по частям речи/Без прилагательных второй жанр.txt',\n",
    "#             'name': 'Без прилагательных второй жанр',\n",
    "#             'type': 'pos',\n",
    "#             'freq': None\n",
    "#             },\n",
    "#           {\n",
    "#             'path1': '../dataset/Сокращение по частям речи/1.Первый жанр исходная выборка.txt',\n",
    "#             'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "#             'name': '1.Первый жанр исходная выборка',\n",
    "#             'type': 'pos',\n",
    "#             'freq': None\n",
    "\n",
    "#         },\n",
    "#         {\n",
    "#             'path1': '../dataset/Сокращение по частям речи/2.Первый жанр без клауз, включающих наречия.txt',\n",
    "#             'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "#             'name': '2.Первый жанр без клауз, включающих наречия',\n",
    "#             'type': 'pos',\n",
    "#             'freq': None\n",
    "#         },\n",
    "#         {\n",
    "#             'path1': '../dataset/Сокращение по частям речи/3.Первый жанр без клауз, включающих глаголы.txt',\n",
    "#             'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "#             'name': '3.Первый жанр без клауз, включающих глаголы',\n",
    "#             'type': 'pos',\n",
    "#             'freq': None\n",
    "#         },\n",
    "#         {\n",
    "#             'path1': '../dataset/Сокращение по частям речи/4.Первый жанр без клауз, включающих глаголы и наречия.txt',\n",
    "#             'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "#             'name': '4.Первый жанр без клауз, включающих глаголы и наречия',\n",
    "#             'type': 'pos',\n",
    "#             'freq': None\n",
    "#         },\n",
    "    ]\n",
    "\n",
    "def main():\n",
    "\n",
    "    results = []\n",
    "    for model in models:\n",
    "        for dataset in datasets:\n",
    "            config = create_custom_config(model['model'], model['type'], dataset)\n",
    "            \n",
    "            # Configurar wandb para este experimento\n",
    "#             wandb_run = wandb.init(\n",
    "#                 project=\"model-comparison\",\n",
    "#                 name=f\"{dataset['name']} - {model['name']}\",\n",
    "#                 group=f\"{dataset['type']}_{dataset.get('freq', 'base')}\",\n",
    "#                 config={\n",
    "#                     \"model\": model['model'],\n",
    "#                     \"model_type\": model['type'],\n",
    "#                     \"dataset\": dataset['name'],\n",
    "#                     \"type\": dataset['type'],\n",
    "#                     \"freq\": dataset.get('freq', None),\n",
    "#                     **config.__dict__\n",
    "#                 },\n",
    "#                 reinit=True\n",
    "#             )\n",
    "            \n",
    "            # Entrenar y limpiar memoria\n",
    "            torch.cuda.empty_cache()\n",
    "            result = funciones.train_and_evaluate_dataset(\n",
    "                dataset['path1'],\n",
    "                dataset['path2'],\n",
    "                config,\n",
    "                dataset['name'],\n",
    "                dataset['type']\n",
    "                \n",
    "#                 wandb_run=wandb_run\n",
    "            )\n",
    "            results.append(result) \n",
    "            \n",
    "#             wandb.log({\n",
    "#                 \"final_avg_accuracy\": result['avg_accuracy'],\n",
    "#                 \"final_std_accuracy\": result['std_accuracy'],\n",
    "#                 \"final_avg_loss\": result['avg_loss']\n",
    "#             })\n",
    "            \n",
    "            # Crear tablas de resumen\n",
    "#             summary_table = wandb.Table(columns=[\"Metric\", \"Value\"])\n",
    "#             summary_table.add_data(\"Avg Accuracy\", result['avg_accuracy'])\n",
    "#             summary_table.add_data(\"Std Accuracy\", result['std_accuracy'])\n",
    "#             summary_table.add_data(\"Avg Loss\", result['avg_loss'])\n",
    "            \n",
    "#             wandb.log({\n",
    "#                 f\"summary/{dataset['name']}_{clean_model_name}\": summary_table,\n",
    "#                 \"epoch\": config.epochs  # Para alinear con otras métricas\n",
    "#             })\n",
    "            \n",
    "#             wandb_run.finish()\n",
    "        \n",
    "    \n",
    "    save_results(results, 'model_results_GPT.json')    \n",
    "#     funciones.plot_model_performance(datasets,\n",
    "#                                      models,\n",
    "#                                      results,\n",
    "#                                      save_path='frecuencia_profesional.png'\n",
    "#                                     )\n",
    "\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd31457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dfbb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1ac659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import IPython\n",
    "\n",
    "# Reiniciar el kernel\n",
    "IPython.display.display(IPython.display.Javascript(\"Jupyter.notebook.kernel.restart()\"))\n",
    "\n",
    "# Apagar el kernel después de reiniciar\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f207b0e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5ccc6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09298908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_results(filename):\n",
    "    \"\"\"Load results from JSON file\"\"\"\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def prepare_data(results):\n",
    "    \"\"\"Organize data for visualization\"\"\"\n",
    "    dataset_names = sorted(list(set(item['dataset_name'] for item in results)))\n",
    "    model_names = sorted(list(set(item['model_name'] for item in results)))\n",
    "    \n",
    "    accuracy_matrix = np.zeros((len(dataset_names), len(model_names)))\n",
    "    \n",
    "    for item in results:\n",
    "        dataset_idx = dataset_names.index(item['dataset_name'])\n",
    "        model_idx = model_names.index(item['model_name'])\n",
    "        accuracy_matrix[dataset_idx, model_idx] = item['avg_accuracy']\n",
    "    \n",
    "    return dataset_names, model_names, accuracy_matrix\n",
    "\n",
    "def create_comparison_plot(datasets, models, accuracies, title_suffix, save_path):\n",
    "    \"\"\"Generate a single comparison plot\"\"\"\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    fig, ax = plt.subplots(figsize=(20, 10), dpi=100)\n",
    "    \n",
    "    n_models = len(models)\n",
    "    bar_width = min(0.8 / n_models, 0.15)\n",
    "    index = np.arange(len(datasets))\n",
    "    colors = plt.cm.Set2(np.linspace(0, 1, n_models))\n",
    "    \n",
    "    for i, (model, color) in enumerate(zip(models, colors)):\n",
    "        bars = plt.bar(\n",
    "            index + i * bar_width,\n",
    "            accuracies[:, i],\n",
    "            bar_width,\n",
    "            label=model,\n",
    "            color=color,\n",
    "            edgecolor='black',\n",
    "            alpha=0.8\n",
    "        )\n",
    "        \n",
    "        if n_models <= 10:\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax.text(\n",
    "                    bar.get_x() + bar.get_width()/2., \n",
    "                    height,\n",
    "                    f'{height:.3f}',\n",
    "                    ha='center', \n",
    "                    va='bottom', \n",
    "                    fontsize=9\n",
    "                )\n",
    "    \n",
    "    plt.xlabel('Dataset', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Average Accuracy', fontsize=12, fontweight='bold')\n",
    "    plt.title(f'Model Performance Comparison ({title_suffix})', fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.xticks(\n",
    "        index + bar_width * (n_models - 1) / 2,\n",
    "        datasets,\n",
    "        rotation=45,\n",
    "        ha='right',\n",
    "        fontsize=10\n",
    "    )\n",
    "    \n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.legend(\n",
    "        title='Models',\n",
    "        title_fontsize=12,\n",
    "        fontsize=10,\n",
    "        loc='center left',\n",
    "        bbox_to_anchor=(1.05, 0.5),\n",
    "        frameon=True\n",
    "    )\n",
    "    \n",
    "    plt.ylim(0, max(accuracies.max() * 1.1, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def create_separate_plots(dataset_names, model_names, accuracy_matrix):\n",
    "    \"\"\"Create two separate plots: frequency-based and other datasets\"\"\"\n",
    "    # Split datasets into two groups\n",
    "    freq_indices = [i for i, name in enumerate(dataset_names) if 'frequency' in name.lower()]\n",
    "    other_indices = [i for i, name in enumerate(dataset_names) if 'frequency' not in name.lower()]\n",
    "    \n",
    "    # Frequency-based datasets plot\n",
    "    if freq_indices:\n",
    "        freq_datasets = [dataset_names[i] for i in freq_indices]\n",
    "        freq_accuracies = accuracy_matrix[freq_indices, :]\n",
    "        create_comparison_plot(\n",
    "            freq_datasets, \n",
    "            model_names, \n",
    "            freq_accuracies,\n",
    "            'Frequency-based Datasets',\n",
    "            'performance_comparison_frequency.png'\n",
    "        )\n",
    "    \n",
    "    # Other datasets plot\n",
    "    if other_indices:\n",
    "        other_datasets = [dataset_names[i] for i in other_indices]\n",
    "        other_accuracies = accuracy_matrix[other_indices, :]\n",
    "        create_comparison_plot(\n",
    "            other_datasets, \n",
    "            model_names, \n",
    "            other_accuracies,\n",
    "            'Other Datasets',\n",
    "            'performance_comparison_other.png'\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = read_results('model_results_gpt_bert.json')\n",
    "    dataset_names, model_names, accuracy_matrix = prepare_data(results)\n",
    "    \n",
    "    print(\"Analyzed datasets:\", dataset_names)\n",
    "    print(\"Evaluated models:\", model_names)\n",
    "    print(\"Accuracy matrix:\\n\", accuracy_matrix)\n",
    "    \n",
    "    create_separate_plots(dataset_names, model_names, accuracy_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427290b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "def load_and_preprocess_data(filename):\n",
    "    \"\"\"Carga y organiza los datos del JSON\"\"\"\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Organizar por tipo de dataset\n",
    "    freq_data = [d for d in data if d['dataset_type'] == 'freq']\n",
    "    pos_data = [d for d in data if d['dataset_type'] == 'pos']\n",
    "    \n",
    "    return freq_data, pos_data\n",
    "\n",
    "def plot_freq_comparison(freq_data):\n",
    "    \"\"\"Gráfico comparativo para datasets de frecuencia\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    palette = sns.color_palette(\"husl\", len(freq_data))\n",
    "    \n",
    "    # Preparar datos\n",
    "    datasets = [d['dataset_name'].split('выше ')[1] for d in freq_data]\n",
    "    avg_acc = [d['avg_accuracy'] for d in freq_data]\n",
    "    std_acc = [d['std_accuracy'] for d in freq_data]\n",
    "    \n",
    "    # Gráfico de barras principal\n",
    "    bars = plt.bar(datasets, avg_acc, yerr=std_acc, \n",
    "                   color=palette, alpha=0.8, capsize=5)\n",
    "    \n",
    "    # Añadir valores exactos\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                 f'{height:.2%}',\n",
    "                 ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Gráfico de puntos para ejecuciones individuales\n",
    "    for i, dataset in enumerate(freq_data):\n",
    "        x_points = np.random.normal(i, 0.05, size=len(dataset['accuracies']))\n",
    "        plt.scatter(x_points, dataset['accuracies'], color='black', alpha=0.6, s=60)\n",
    "    \n",
    "    plt.title('Comparación de Precisión por Frecuencia de Lexemas', fontsize=14, pad=20)\n",
    "    plt.xlabel('Frecuencia mínima de lexemas', fontsize=12)\n",
    "    plt.ylabel('Precisión', fontsize=12)\n",
    "    plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "    plt.ylim(0.95, 1.01)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('freq_comparison.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def plot_pos_analysis(pos_data):\n",
    "    \"\"\"Gráfico de análisis para POS (si hubiera datos)\"\"\"\n",
    "    if not pos_data:\n",
    "        print(\"No hay datos POS para graficar\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    # Ejemplo de cómo sería con datos (adaptar a tus datos reales)\n",
    "    for dataset in pos_data:\n",
    "        plt.plot(dataset['accuracies'], marker='o', label=dataset['dataset_name'])\n",
    "    \n",
    "    plt.title('Evolución de Precisión por Época (POS)', fontsize=14, pad=20)\n",
    "    plt.xlabel('Época', fontsize=12)\n",
    "    plt.ylabel('Precisión', fontsize=12)\n",
    "    plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pos_analysis.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss_comparison(freq_data):\n",
    "    \"\"\"Gráfico comparativo de pérdida\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    datasets = [d['dataset_name'].split('выше ')[1] for d in freq_data]\n",
    "    avg_loss = [d['avg_loss'] for d in freq_data]\n",
    "    \n",
    "    bars = plt.bar(datasets, avg_loss, color=sns.color_palette(\"coolwarm\", len(freq_data)))\n",
    "    \n",
    "    # Añadir valores exactos\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                 f'{height:.4f}',\n",
    "                 ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.title('Comparación de Pérdida por Dataset', fontsize=14, pad=20)\n",
    "    plt.xlabel('Dataset', fontsize=12)\n",
    "    plt.ylabel('Pérdida Promedio', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('loss_comparison.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    freq_data, pos_data = load_and_preprocess_data('model_results.json')\n",
    "    \n",
    "    # 1. Gráfico comparativo de precisión por frecuencia\n",
    "    plot_freq_comparison(freq_data)\n",
    "    \n",
    "    # 2. Gráfico de evolución para POS (si existe)\n",
    "    plot_pos_analysis(pos_data)\n",
    "    \n",
    "    # 3. Gráfico comparativo de pérdida\n",
    "    plot_loss_comparison(freq_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9859a0a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "def load_data(filename):\n",
    "    \"\"\"Carga y clasifica los datos\"\"\"\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Separar y ordenar los datos\n",
    "    freq_data = sorted([d for d in data if d['dataset_type'] == 'freq'], \n",
    "                      key=lambda x: int(x['dataset_name'].split('выше ')[1]))\n",
    "    pos_data = sorted([d for d in data if d['dataset_type'] == 'pos'],\n",
    "                     key=lambda x: x['dataset_name'])\n",
    "    \n",
    "    return freq_data, pos_data\n",
    "\n",
    "def plot_freq_data(freq_data):\n",
    "    \"\"\"Gráfico profesional para datos de frecuencia\"\"\"\n",
    "    if not freq_data:\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    palette = sns.color_palette(\"Blues_d\", len(freq_data))\n",
    "    \n",
    "    # Preparar datos ordenados\n",
    "    thresholds = [int(d['dataset_name'].split('выше ')[1]) for d in freq_data]\n",
    "    avg_acc = [d['avg_accuracy'] for d in freq_data]\n",
    "    std_acc = [d['std_accuracy'] for d in freq_data]\n",
    "    \n",
    "    # Gráfico principal\n",
    "    bars = plt.bar(range(len(thresholds)), avg_acc, yerr=std_acc,\n",
    "                  color=palette, alpha=0.8, capsize=5)\n",
    "    \n",
    "    # Añadir detalles\n",
    "    for bar, acc in zip(bars, avg_acc):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height - 0.015,\n",
    "                f'{acc:.2%}',\n",
    "                ha='center', va='top', color='white', fontweight='bold')\n",
    "    \n",
    "    # Personalizar ejes\n",
    "    plt.xticks(range(len(thresholds)), [f'Freq > {t}' for t in thresholds])\n",
    "    plt.title('Precisión por Frecuencia de Lexemas', fontsize=14, pad=20)\n",
    "    plt.ylabel('Precisión Promedio', fontsize=12)\n",
    "    plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "    plt.ylim(0.96, 1.0)\n",
    "    \n",
    "    # Añadir puntos individuales\n",
    "    for i, dataset in enumerate(freq_data):\n",
    "        x_points = np.random.normal(i, 0.08, size=len(dataset['accuracies']))\n",
    "        plt.scatter(x_points, dataset['accuracies'], color='darkred', alpha=0.6, \n",
    "                   s=60, edgecolor='white', label='Ejecuciones' if i == 0 else \"\")\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('freq_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_pos_data(pos_data):\n",
    "    \"\"\"Gráfico para datos POS (estructura lista)\"\"\"\n",
    "    if not pos_data:\n",
    "        print(\"\\n⚠️ No se encontraron datos POS en el archivo\")\n",
    "        # Crear gráfico de ejemplo (eliminar cuando tengas datos reales)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.text(0.5, 0.5, 'Datos POS no disponibles\\n\\n(Estructura lista para cuando tengas datos)',\n",
    "                ha='center', va='center', fontsize=12)\n",
    "        plt.title('Análisis POS (Datos no disponibles)', fontsize=14)\n",
    "        plt.axis('off')\n",
    "        plt.savefig('pos_analysis_placeholder.png', dpi=300)\n",
    "        plt.show()\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    # Ejemplo con datos reales (adaptar)\n",
    "    for i, dataset in enumerate(pos_data):\n",
    "        plt.plot(dataset['accuracies'], marker='o', linestyle='--',\n",
    "                label=dataset['dataset_name'], \n",
    "                color=sns.color_palette(\"viridis\", len(pos_data))[i])\n",
    "    \n",
    "    plt.title('Evolución de Precisión (POS)', fontsize=14, pad=20)\n",
    "    plt.xlabel('Época', fontsize=12)\n",
    "    plt.ylabel('Precisión', fontsize=12)\n",
    "    plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pos_analysis.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def plot_combined_metrics(freq_data):\n",
    "    \"\"\"Gráfico combinado de precisión y pérdida\"\"\"\n",
    "    if not freq_data:\n",
    "        return\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    thresholds = [int(d['dataset_name'].split('выше ')[1]) for d in freq_data]\n",
    "    x = range(len(thresholds))\n",
    "    \n",
    "    # Precisión (eje izquierdo)\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Umbral de Frecuencia', fontsize=12)\n",
    "    ax1.set_ylabel('Precisión', color=color, fontsize=12)\n",
    "    acc_bars = ax1.bar(x, [d['avg_accuracy'] for d in freq_data], \n",
    "                       color=color, alpha=0.6, label='Precisión')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels([f'> {t}' for t in thresholds])\n",
    "    ax1.set_ylim(0.95, 1.0)\n",
    "    \n",
    "    # Pérdida (eje derecho)\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('Pérdida', color=color, fontsize=12)\n",
    "    loss_line = ax2.plot(x, [d['avg_loss'] for d in freq_data], \n",
    "                        color=color, marker='o', linestyle='--', label='Pérdida')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.set_ylim(0, max([d['avg_loss'] for d in freq_data]) * 1.5)\n",
    "    \n",
    "    # Leyenda unificada\n",
    "    lines = acc_bars + loss_line\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax1.legend(lines, labels, loc='upper right')\n",
    "    \n",
    "    plt.title('Relación Precisión-Pérdida por Frecuencia', fontsize=14, pad=20)\n",
    "    fig.tight_layout()\n",
    "    plt.savefig('combined_metrics.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"📊 Procesando datos...\")\n",
    "    freq_data, pos_data = load_data('model_results.json')\n",
    "    \n",
    "    print(\"\\n🔵 Generando gráfico de frecuencia...\")\n",
    "    plot_freq_data(freq_data)\n",
    "    \n",
    "    print(\"\\n🟢 Generando gráfico POS...\")\n",
    "    plot_pos_data(pos_data)\n",
    "    \n",
    "    print(\"\\n🟣 Generando gráfico combinado...\")\n",
    "    plot_combined_metrics(freq_data)\n",
    "    \n",
    "    print(\"\\n✅ Análisis completado. Gráficos guardados:\")\n",
    "    print(f\"- freq_analysis.png\\n- pos_analysis.png\\n- combined_metrics.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581ea60b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
