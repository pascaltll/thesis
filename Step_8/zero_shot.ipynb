{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1e2b1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import razdel\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch  # Importar torchfrom torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from isanlp.pipeline_common import PipelineCommon\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BertForSequenceClassification, pipeline\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from razdel import sentenize\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import logging\n",
    "from transformers import AutoTokenizer\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57ac1a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig:\n",
    "    def __init__(self,\n",
    "                 model_name,\n",
    "                 max_length=128,\n",
    "                 batch_size=64,\n",
    "                 epochs=3,\n",
    "                 learning_rate=2e-5,\n",
    "                 num_repeats=6,\n",
    "                 test_size=0.2,\n",
    "                 threshold=0.5,\n",
    "                 model_type='bert'):\n",
    "        \"\"\"\n",
    "        Configuración para el entrenamiento del modelo.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Nombre del modelo preentrenado\n",
    "            max_length (int): Longitud máxima de las secuencias\n",
    "            batch_size (int): Tamaño del batch\n",
    "            epochs (int): Número de épocas\n",
    "            learning_rate (float): Tasa de aprendizaje\n",
    "            num_repeats (int): Número de repeticiones con diferentes seeds\n",
    "            test_size (float): Proporción del conjunto de prueba\n",
    "            threshold (float): Umbral de similitud\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_repeats = num_repeats\n",
    "        self.test_size = test_size\n",
    "        self.threshold = threshold\n",
    "        self.model_type = model_type\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        #self.tokenizer = BertTokenizer.from_pretrained(model_name)#creo que hay que usar el tokenizer de ruso\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name) \n",
    "        if self.model_type == 'gpt':\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            self.tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f9e87c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_datos(archivo, etiqueta):\n",
    "    with open(archivo, 'r', encoding='utf-8') as f:\n",
    "        textos = f.readlines()\n",
    "    return [(texto.strip(), etiqueta) for texto in textos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e80c6d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizar_fuction(texto):\n",
    "    \"\"\"Tokeniza el texto eliminando puntuación y convirtiendo a minúsculas.\"\"\"\n",
    "    texto_preprocesado = re.sub(r'[^\\w\\s]', '', texto.lower())\n",
    "    tokens = nltk.word_tokenize(texto_preprocesado)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fe9941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_similitud_mayoria_optimizado(textos1, textos2, threshold):\n",
    "    \"\"\"\n",
    "        Calcula similitudes entre dos listas de textos usando GPU.\n",
    "        Args:\n",
    "            textos1: Lista de textos (original_datos_procesados).\n",
    "            textos2: Lista de textos (train_data).\n",
    "        Returns:\n",
    "            Matriz booleana [len(textos1), len(textos2)] indicando similitudes.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "    # Preprocesar todos los textos en batch\n",
    "    textos1_preprocesados = [' '.join(tokenizar_fuction(t)) for t in textos1]\n",
    "    textos2_preprocesados = [' '.join(tokenizar_fuction(t)) for t in textos2]\n",
    "\n",
    "    # Crear representación binaria de tokens\n",
    "    vectorizer = CountVectorizer(binary=True, tokenizer=nltk.word_tokenize)\n",
    "    vectorizer.fit(textos1_preprocesados + textos2_preprocesados)\n",
    "\n",
    "    # Convertir a matrices de tokens\n",
    "    matriz_tokens1 = vectorizer.transform(textos1_preprocesados).toarray()\n",
    "    matriz_tokens2 = vectorizer.transform(textos2_preprocesados).toarray()\n",
    "\n",
    "    # Mover a GPU\n",
    "    matriz_tokens1 = torch.from_numpy(matriz_tokens1).to(device)\n",
    "    matriz_tokens2 = torch.from_numpy(matriz_tokens2).to(device)\n",
    "\n",
    "     # Calcular intersecciones\n",
    "    interseccion = torch.matmul(matriz_tokens1.float(), matriz_tokens2.T.float())\n",
    "\n",
    "    # Tamaños de los conjuntos de tokens\n",
    "    tamano_tokens1 = matriz_tokens1.sum(dim=1).unsqueeze(1)\n",
    "    tamano_tokens2 = matriz_tokens2.sum(dim=1).unsqueeze(0)\n",
    "\n",
    "    # Evitar división por cero\n",
    "    tamano_tokens1 = tamano_tokens1.clamp(min=1)\n",
    "    tamano_tokens2 = tamano_tokens2.clamp(min=1)\n",
    "\n",
    "     # Calcular proporciones\n",
    "    prop_1_en_2 = interseccion / tamano_tokens2\n",
    "    prop_2_en_1 = interseccion / tamano_tokens1\n",
    "\n",
    "    similitud = (prop_1_en_2 > threshold) | (prop_2_en_1 > threshold)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return similitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc2d307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader_raw:\n",
    "    def __init__(self, path1, path2):\n",
    "        self.path1 = path1\n",
    "        self.path2 = path2\n",
    "        self.path_original1 = '../dataset/Первый_жанр_исходная.txt'\n",
    "        self.path_original2 = '../dataset/Второй_жанр_исходная.txt'    \n",
    "    \n",
    "    def __call__(self):\n",
    "        datos_genero1 = cargar_datos(self.path1, 0)\n",
    "        datos_genero2 = cargar_datos(self.path2, 1)\n",
    "        datos = datos_genero1 + datos_genero2\n",
    "        \n",
    "        datos_original1 = cargar_datos(self.path_original1, 0)\n",
    "        datos_original2 = cargar_datos(self.path_original2, 1)\n",
    "        datos_originales = datos_original1 + datos_original2\n",
    "        \n",
    "        return {\n",
    "            'datos_raw': datos,\n",
    "            'original_datos_raw': datos_originales\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed51d773",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceSplitterAndCleaner:\n",
    "    def __init__(self, tokenizer, min_length_threshold=6):\n",
    "        \"\"\"Inicializa el procesador con un tokenizador y un umbral de longitud mínima.\"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.min_length_threshold = min_length_threshold\n",
    "    \n",
    "    def _clean_and_split_text(self, texto):\n",
    "        \"\"\"\n",
    "        Limpia y divide un texto en oraciones procesadas.\n",
    "        \n",
    "        Args:\n",
    "            texto (str): Texto crudo a procesar.\n",
    "        \n",
    "        Returns:\n",
    "            list: Lista de oraciones limpias.\n",
    "        \"\"\"\n",
    "        # Paso 1: Marcar puntos para evitar fusiones no deseadas\n",
    "        texto_limpio = re.sub(r'\\.,', '. Ok999999999 ,', texto)  # Caso 1: Punto seguido de coma\n",
    "        texto_limpio = re.sub(r'\\.;', '. Ok999999999 ', texto_limpio)  # Caso 2: Punto seguido de punto y coma\n",
    "        texto_limpio = re.sub(r'\\. ([a-zа-я])', r'. Ok999999999 \\1', texto_limpio)  # Caso 3: Punto seguido de minúscula\n",
    "        texto_limpio = re.sub(r'(\\w)([А-Я])', r'\\1. \\2', texto_limpio)  # Caso 4: Minúscula seguida de mayúscula\n",
    "        \n",
    "        #creo que hay que agregar una nueva condivion  дл.,0 \n",
    "        #\", 2-3 см шир.\",0 \n",
    "        #\", 1 см шир.\",0 si hay un punto uego numeros quitar el punto\n",
    "        \n",
    "        # Paso 2: Eliminar corchetes y dividir en oraciones\n",
    "        texto_sin_corchetes = re.sub(r'\\[.*?\\]', '', texto_limpio).strip()\n",
    "        oraciones = [oracion.text for oracion in sentenize(texto_sin_corchetes)]\n",
    "        \n",
    "        # Paso 3: Restaurar espacios y limpiar marcadores temporales\n",
    "        oraciones_limpias = [re.sub(r'\\s*Ok999999999', ' ', oracion).strip() for oracion in oraciones]\n",
    "        \n",
    "        return oraciones_limpias\n",
    "    \n",
    "    def _process_sentences(self, datos, target_list):\n",
    "        \"\"\"\n",
    "        Procesa un conjunto de datos y agrega oraciones válidas a la lista objetivo.\n",
    "        \n",
    "        Args:\n",
    "            datos (list): Lista de tuplas (texto, etiqueta).\n",
    "            target_list (list): Lista donde se almacenarán las oraciones procesadas.\n",
    "        \"\"\"\n",
    "        for texto, etiqueta in datos:\n",
    "            oraciones = self._clean_and_split_text(texto)\n",
    "            for oracion in oraciones:\n",
    "                # Filtrar oraciones cortas basadas en el número de tokens\n",
    "                if len(self.tokenizer.encode(oracion, truncation=False)) >= self.min_length_threshold:\n",
    "                    target_list.append((oracion, etiqueta))\n",
    "    \n",
    "    def __call__(self, data_raw, original_data_raw):\n",
    "        \"\"\"\n",
    "        Procesa los datos crudos y originales, devolviendo dos conjuntos limpios.\n",
    "        \n",
    "        Args:\n",
    "            data_raw (list): Datos modificados crudos.\n",
    "            original_data_raw (list): Datos originales crudos.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Diccionario con datos procesados y originales procesados.\n",
    "        \"\"\"\n",
    "        datos_procesados = []\n",
    "        original_datos_procesados = []\n",
    "        \n",
    "        # Procesar datos modificados\n",
    "        self._process_sentences(data_raw, datos_procesados)\n",
    "        \n",
    "        # Procesar datos originales\n",
    "        self._process_sentences(original_data_raw, original_datos_procesados)\n",
    "        \n",
    "        return {\n",
    "            'datos_procesados': datos_procesados,\n",
    "            'original_datos_procesados': original_datos_procesados\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77d3cb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self,\n",
    "                 tokenizer,\n",
    "                 max_length,\n",
    "                 random_state,\n",
    "                 test_size=0.2,\n",
    "                 name='data_set_name',\n",
    "                 threshold=0.5,\n",
    "                 model_type='bert'):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.name = name\n",
    "        self.threshold = threshold\n",
    "        self.model_type = model_type\n",
    "        \n",
    "    def __call__(self, datos_procesados, original_datos_procesados):\n",
    "        df = pd.DataFrame(datos_procesados, columns=[\"text\", \"label\"])\n",
    "        train_data, test_data = train_test_split(df,\n",
    "                                                 test_size=self.test_size,\n",
    "                                                 random_state=self.random_state)\n",
    "        \n",
    "        if self.model_type == 'gpt':\n",
    "            train_texts = train_data[\"text\"].tolist()\n",
    "            train_labels = train_data[\"label\"].tolist()\n",
    "            \n",
    "            original_texts = [oracion for oracion, _ in original_datos_procesados]\n",
    "            original_labels = [etiqueta for _, etiqueta in original_datos_procesados]\n",
    "            \n",
    "            # Calcular similitudes para filtrar el conjunto de prueba\n",
    "            similitudes = calcular_similitud_mayoria_optimizado(original_texts, train_texts, self.threshold)\n",
    "            interseccion = similitudes.any(dim=1)\n",
    "            mask = ~interseccion.cpu().numpy()\n",
    "            nuevo_test = [(texto, etiqueta) for texto, etiqueta, keep in zip(original_texts, original_labels, mask) if keep]\n",
    "            \n",
    "            # Tokenizar entrenamiento\n",
    "            train_encodings = self.tokenizer(\n",
    "                train_texts,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            train_labels = torch.tensor(train_labels)\n",
    "            \n",
    "            # Tokenizar prueba\n",
    "            test_encodings = self.tokenizer(\n",
    "                [texto for texto, _ in nuevo_test],\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            test_labels = torch.tensor([etiqueta for _, etiqueta in nuevo_test])\n",
    "            \n",
    "            # Guardar datos si random_state == 0\n",
    "            if self.random_state == 0:\n",
    "                train_data.to_csv(f'сокращение по частотности/train_{self.name}.csv', index=False)\n",
    "                print(f\"train_{self.name}.csv\")\n",
    "                nuevo_test_df = pd.DataFrame(nuevo_test, columns=[\"text\", \"label\"])\n",
    "                nuevo_test_df.to_csv(f'сокращение по частотности/test_{self.name}.csv', index=False)\n",
    "                print(f\"test_{self.name}.csv\")\n",
    "            \n",
    "        else:\n",
    "            train_texts = train_data[\"text\"].tolist()\n",
    "            original_texts = [oracion for oracion, _ in original_datos_procesados]\n",
    "            original_labels = [etiqueta for _, etiqueta in original_datos_procesados]\n",
    "\n",
    "            similitudes = calcular_similitud_mayoria_optimizado(original_texts,\n",
    "                                                                train_texts,\n",
    "                                                                self.threshold)\n",
    "            interseccion = similitudes.any(dim=1)\n",
    "            mask = ~interseccion.cpu().numpy()\n",
    "            nuevo_test = [(texto, etiqueta) for texto, etiqueta, keep in zip(original_texts, original_labels, mask) if keep]\n",
    "\n",
    "            if self.random_state == 0:\n",
    "                train_data.to_csv(f'сокращение по частотности/train_{self.name}.csv', index=False)\n",
    "                print(f\"train_{self.name}.csv\")\n",
    "                nuevo_test_df = pd.DataFrame(nuevo_test, columns=[\"text\", \"label\"])\n",
    "                nuevo_test_df.to_csv(f'сокращение по частотности/test_{self.name}.csv', index=False)\n",
    "                print(f\"test_{self.name}.csv\")\n",
    "\n",
    "            # Tokenizar entrenamiento\n",
    "            train_encodings = self.tokenizer(\n",
    "                train_data[\"text\"].tolist(),\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            train_labels = torch.tensor(train_data[\"label\"].values)\n",
    "            test_encodings = self.tokenizer(\n",
    "                [texto for texto, _ in nuevo_test],\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            test_labels = torch.tensor([etiqueta for _, etiqueta in nuevo_test])\n",
    "        \n",
    "        # Depuración: Verificar forma y contenido de las etiquetas\n",
    "        print(f\"Train labels sample: {train_labels[:5]}, Shape: {train_labels.shape}\")\n",
    "        print(f\"Test labels sample: {test_labels[:5]}, Shape: {test_labels.shape}\")\n",
    "        \n",
    "        return {\n",
    "            'train_encodings': train_encodings,\n",
    "            'train_labels': train_labels,\n",
    "            'test_encodings': test_encodings,\n",
    "            'test_labels': test_labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7d81be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a12a500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetCreator:\n",
    "    def __init__(self, batch_size=8):\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __call__(self, train_encodings, train_labels, test_encodings, test_labels):\n",
    "        train_dataset = TextDataset(train_encodings, train_labels)\n",
    "        test_dataset = TextDataset(test_encodings, test_labels)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        return {'train_loader': train_loader, 'test_loader': test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef939118",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroShotConfig:\n",
    "    def __init__(self,\n",
    "                 model_name,\n",
    "                 candidate_labels=[\"жанр0\", \"жанр1\"],\n",
    "                 hypothesis_template=\"Este texto es sobre {}.\"):\n",
    "        self.model_name = model_name\n",
    "        self.candidate_labels = candidate_labels\n",
    "        self.hypothesis_template = hypothesis_template\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "662c7c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_evaluate(config, test_loader, tokenizer):\n",
    "    # Cargar el pipeline de zero-shot\n",
    "    classifier = pipeline(\n",
    "        \"zero-shot-classification\",\n",
    "        model=config.model_name,\n",
    "        device=config.device\n",
    "    )\n",
    "    \n",
    "    all_true_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    \n",
    "    # Procesar el test_loader\n",
    "    for batch in test_loader:\n",
    "        texts = [tokenizer.decode(ids, skip_special_tokens=True) \n",
    "                for ids in batch['input_ids']]\n",
    "        true_labels = batch['labels'].cpu().numpy()\n",
    "        \n",
    "        # Clasificación zero-shot\n",
    "        results = classifier(\n",
    "            texts,\n",
    "            candidate_labels=config.candidate_labels,\n",
    "            hypothesis_template=config.hypothesis_template\n",
    "        )\n",
    "        \n",
    "        # Procesar resultados\n",
    "        for result, true_label in zip(results, true_labels):\n",
    "            pred_label = config.candidate_labels.index(result['labels'][0])\n",
    "            prob = result['scores'][0]\n",
    "            \n",
    "            all_true_labels.append(true_label)\n",
    "            all_preds.append(pred_label)\n",
    "            all_probs.append(prob)\n",
    "    \n",
    "    # Convertir a arrays numpy\n",
    "    all_true_labels = np.array(all_true_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    # Calcular métricas (igual que en tu evaluate_model)\n",
    "    accuracy = np.mean(all_preds == all_true_labels)\n",
    "    report = classification_report(all_true_labels, all_preds, \n",
    "                                  target_names=config.candidate_labels, output_dict=True)\n",
    "    conf_matrix = confusion_matrix(all_true_labels, all_preds)\n",
    "    roc_auc = roc_auc_score(all_true_labels, all_probs)\n",
    "    pr_auc = average_precision_score(all_true_labels, all_probs)\n",
    "    log_loss_val = log_loss(all_true_labels, all_probs)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_weighted\": report['weighted avg']['f1-score'],\n",
    "        \"f1_macro\": report['macro avg']['f1-score'],\n",
    "        \"f1_class0\": report[config.candidate_labels[0]]['f1-score'],\n",
    "        \"f1_class1\": report[config.candidate_labels[1]]['f1-score'],\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"pr_auc\": pr_auc,\n",
    "        \"log_loss\": log_loss_val,\n",
    "        \"report\": report,\n",
    "        \"conf_matrix\": conf_matrix,\n",
    "        \"predictions\": all_preds.tolist(),\n",
    "        \"true_labels\": all_true_labels.tolist(),\n",
    "        \"probs\": all_probs.tolist()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "139a8db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_zero_shot(path1, path2, config, dataset_name, dataset_type):\n",
    "    seeds = list(range(config.num_repeats))\n",
    "    metrics = {\n",
    "        'accuracies': [],\n",
    "        'f1_weighteds': [],\n",
    "        'f1_macros': [],\n",
    "        'f1_class0s': [],\n",
    "        'f1_class1s': [],\n",
    "        'roc_aucs': [],\n",
    "        'pr_aucs': [],\n",
    "        'log_losses': [],\n",
    "        'confusion_matrices': []\n",
    "    }\n",
    "    \n",
    "    for seed in seeds:\n",
    "        # Pipeline común (igual que antes para preparar los datos)\n",
    "        ppl = PipelineCommon([\n",
    "            (DataLoader_raw(path1, path2), [], {'datos_raw': 'datos_raw', 'original_datos_raw': 'original_datos_raw'}),\n",
    "            (SentenceSplitterAndCleaner(config.tokenizer), ['datos_raw', 'original_datos_raw'], \n",
    "             {'datos_procesados': 'datos_procesados', 'original_datos_procesados': 'original_datos_procesados'}),\n",
    "            (DataProcessor(config.tokenizer, config.max_length, seed, name=dataset_name, \n",
    "                           threshold=config.threshold, model_type=config.model_type), \n",
    "             ['datos_procesados', 'original_datos_procesados'], \n",
    "             {'train_encodings': 'train_encodings', 'train_labels': 'train_labels',\n",
    "              'test_encodings': 'test_encodings', 'test_labels': 'test_labels'}),\n",
    "            (DatasetCreator(batch_size=config.batch_size), \n",
    "             ['train_encodings', 'train_labels', 'test_encodings', 'test_labels'], \n",
    "             {'train_loader': 'train_loader', 'test_loader': 'test_loader'})\n",
    "        ])\n",
    "        \n",
    "        result = ppl()\n",
    "        test_loader = result['test_loader']\n",
    "        \n",
    "        # Configuración Zero-Shot\n",
    "        zs_config = ZeroShotConfig(\n",
    "            model_name=config.model_name,\n",
    "            candidate_labels=[\"жанр0\", \"жанр1\"],\n",
    "            hypothesis_template=\"Este fragmento literario pertenece al género {}.\"\n",
    "        )\n",
    "        \n",
    "        # Evaluación Zero-Shot\n",
    "        results = zero_shot_evaluate(zs_config, test_loader, config.tokenizer)\n",
    "        \n",
    "        # Acumular métricas\n",
    "        metrics['accuracies'].append(results['accuracy'])\n",
    "        metrics['f1_weighteds'].append(results['f1_weighted'])\n",
    "        metrics['f1_macros'].append(results['f1_macro'])\n",
    "        metrics['f1_class0s'].append(results['f1_class0'])\n",
    "        metrics['f1_class1s'].append(results['f1_class1'])\n",
    "        metrics['roc_aucs'].append(results['roc_auc'])\n",
    "        metrics['pr_aucs'].append(results['pr_auc'])\n",
    "        metrics['log_losses'].append(results['log_loss'])\n",
    "        metrics['confusion_matrices'].append(results['conf_matrix'])\n",
    "    \n",
    "    # Calcular promedios\n",
    "    avg_conf_matrix = np.mean(metrics['confusion_matrices'], axis=0)\n",
    "    \n",
    "    return {\n",
    "        'dataset_name': dataset_name,\n",
    "        'dataset_type': dataset_type,\n",
    "        'model_name': config.model_name,\n",
    "        'avg_accuracy': float(np.mean(metrics['accuracies'])),\n",
    "        'std_accuracy': float(np.std(metrics['accuracies'])),\n",
    "        'avg_f1_weighted': float(np.mean(metrics['f1_weighteds'])),\n",
    "        'std_f1_weighted': float(np.std(metrics['f1_weighteds'])),\n",
    "        'avg_f1_macro': float(np.mean(metrics['f1_macros'])),\n",
    "        'std_f1_macro': float(np.std(metrics['f1_macros'])),\n",
    "        'avg_f1_class0': float(np.mean(metrics['f1_class0s'])),\n",
    "        'std_f1_class0': float(np.std(metrics['f1_class0s'])),\n",
    "        'avg_f1_class1': float(np.mean(metrics['f1_class1s'])),\n",
    "        'std_f1_class1': float(np.std(metrics['f1_class1s'])),\n",
    "        'avg_roc_auc': float(np.mean(metrics['roc_aucs'])),\n",
    "        'std_roc_auc': float(np.std(metrics['roc_aucs'])),\n",
    "        'avg_pr_auc': float(np.mean(metrics['pr_aucs'])),\n",
    "        'std_pr_auc': float(np.std(metrics['pr_aucs'])),\n",
    "        'avg_log_loss': float(np.mean(metrics['log_losses'])),\n",
    "        'std_log_loss': float(np.std(metrics['log_losses'])),\n",
    "        'avg_confusion_matrix': avg_conf_matrix.tolist(),\n",
    "        'confusion_matrices': [cm.tolist() for cm in metrics['confusion_matrices']],\n",
    "        'type': 'zero-shot',\n",
    "        'true_labels': results['true_labels'],\n",
    "        'predictions': results['predictions']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fe942d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_models = [\n",
    "    {'model': 'facebook/bart-large-mnli', 'name': 'BART-MNLI'},\n",
    "    {'model': 'MoritzLaurer/mDeBERTa-v3-base-mnli-xnli', 'name': 'mDeBERTa-XNLI'},\n",
    "    {'model': 'joeddav/xlm-roberta-large-xnli', 'name': 'XLM-RoBERTa-XNLI'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33119314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    results = []\n",
    "    \n",
    "    # Modo Zero-Shot\n",
    "    for model in zero_shot_models:\n",
    "        for dataset in datasets:\n",
    "            config = create_custom_config(model['model'], 'zero-shot', dataset)\n",
    "            result = train_and_evaluate_zero_shot(\n",
    "                dataset['path1'],\n",
    "                dataset['path2'],\n",
    "                config,\n",
    "                dataset['name'],\n",
    "                dataset['type']\n",
    "            )\n",
    "            results.append(result)\n",
    "\n",
    "    \n",
    "    save_results(results, 'model_results_complete.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a1440dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def plot_combined_metrics(results, save_path=None):\n",
    "    # Convertir resultados a DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Configurar estilo\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Gráfico de accuracy por modelo y tipo de dataset\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.barplot(data=df, x='model_name', y='avg_accuracy', hue='dataset_type', \n",
    "                palette='viridis', errorbar='sd')\n",
    "    plt.title('Accuracy por Modelo y Tipo de Dataset')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Modelo')\n",
    "    plt.legend(title='Tipo de Dataset')\n",
    "    \n",
    "    # Gráfico de F1-macro por tipo de modelo\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.boxplot(data=df, x='type', y='avg_f1_macro', palette='Set2')\n",
    "    plt.title('Distribución de F1-Macro por Tipo de Modelo')\n",
    "    plt.ylabel('F1 Macro')\n",
    "    plt.xlabel('Tipo de Modelo')\n",
    "    \n",
    "    # Gráfico de ROC-AUC comparando enfoques\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.scatterplot(data=df, x='model_name', y='avg_roc_auc', hue='type', \n",
    "                   style='dataset_type', s=150, palette='dark')\n",
    "    plt.title('ROC-AUC por Modelo y Enfoque')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylabel('ROC-AUC')\n",
    "    plt.xlabel('Modelo')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8601c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(results, ncols=3, figsize=(20, 15), save_path=None):\n",
    "    # Filtrar solo resultados con matrices de confusión\n",
    "    valid_results = [r for r in results if 'avg_confusion_matrix' in r]\n",
    "    \n",
    "    nrows = int(np.ceil(len(valid_results) / ncols))\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    for i, result in enumerate(valid_results, 1):\n",
    "        plt.subplot(nrows, ncols, i)\n",
    "        \n",
    "        # Obtener etiquetas según el tipo de modelo\n",
    "        labels = [\"жанр0\", \"жанр1\"]\n",
    "        if result['type'] == 'zero-shot':\n",
    "            labels = result.get('candidate_labels', labels)\n",
    "        \n",
    "        sns.heatmap(result['avg_confusion_matrix'], annot=True, fmt='.1f',\n",
    "                   xticklabels=labels, yticklabels=labels,\n",
    "                   cmap='Blues', cbar=False)\n",
    "        \n",
    "        title = f\"{result['model_name']}\\n{result['dataset_name']}\"\n",
    "        if result['type'] == 'zero-shot':\n",
    "            title += \" (Zero-Shot)\"\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Predicho')\n",
    "        plt.ylabel('Real')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4a7f11c",
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/requests/models.py:974\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/json/__init__.py:357\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    355\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    356\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     15\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mМолекула — это наименьшая частица вещества, которая сохраняет его химические свойства.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassify_text_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mclassify_text_gpt\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#deberia ser 0 \u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Ejemplo con una API hipotética\u001b[39;00m\n\u001b[1;32m     12\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.x.ai/grok\u001b[39m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt})\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/requests/models.py:978\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[0;32m--> 978\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def classify_text_gpt(text):\n",
    "    prompt = f\"\"\"\n",
    "    Ты — эксперт по классификации текста на русском языке. Тебе дан текст, и ты должен определить, является ли он **Описанием** (описывает что-то, содержит детали, примеры) или **Определением** (дает краткое и формальное определение термина). Ответь только с меткой: \"Описание\" или \"Определение\".\n",
    "\n",
    "    Текст: {text}\n",
    "    Метка:\n",
    "    \"\"\"\n",
    "    #deberia ser 0 \n",
    "    # Ejemplo con una API hipotética\n",
    "    response = requests.post(\"https://api.x.ai/grok\", json={\"prompt\": prompt})\n",
    "    return response.json()[\"completion\"].strip()\n",
    "\n",
    "text = \"Молекула — это наименьшая частица вещества, которая сохраняет его химические свойства.\"\n",
    "print(classify_text_gpt(text))  # Salida: Определение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b833247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка: Определение, Similitud Определение: 0.998, Similitud Описание: 0.998\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Cargar modelo y tokenizador\n",
    "model_name = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "\n",
    "# Plantillas\n",
    "template_def = \"Это краткое и формальное определение термина или понятия.\"\n",
    "template_desc = \"Это подробное описание с деталями, примерами или характеристиками.\"\n",
    "\n",
    "# Texto a clasificar\n",
    "# text = \"Молекула — это наименьшая частица вещества, которая сохраняет его химические свойства.\"\n",
    "text = \"Термин образ автора предложен В.В. Виноградовым как важнейший из инструментов стилистического анализа художественной речи;\"\n",
    "# Obtener embeddings\n",
    "emb_text = get_embedding(text)\n",
    "emb_def = get_embedding(template_def)\n",
    "emb_desc = get_embedding(template_desc)\n",
    "\n",
    "# Calcular similitud coseno\n",
    "sim_def = 1 - cosine(emb_text, emb_def)\n",
    "sim_desc = 1 - cosine(emb_text, emb_desc)\n",
    "\n",
    "# Clasificar\n",
    "label = \"Определение\" if sim_def > sim_desc else \"Описание\"\n",
    "print(f\"Метка: {label}, Similitud Определение: {sim_def:.3f}, Similitud Описание: {sim_desc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e5b2a42",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "meta-llama/Llama-3-8b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/meta-llama/Llama-3-8b/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/file_download.py:860\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/file_download.py:967\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m--> 967\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/file_download.py:1482\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1481\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1484\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/file_download.py:1374\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/file_download.py:1294\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1294\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1303\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/file_download.py:278\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 278\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/file_download.py:302\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    301\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 302\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/utils/_http.py:454\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    446\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m     )\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-68178527-230b7e004512747008ee8db3;908827e8-d360-4fdb-b07f-201f47d28bb8)\n\nRepository Not Found for url: https://huggingface.co/meta-llama/Llama-3-8b/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m      3\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-3-8b\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Ejemplo, requiere acceso\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124mТы — эксперт по классификации текста на русском языке. Тебе дан текст, и ты должен определить, является ли он **Описанием** (текст, который описывает что-то с деталями, примерами или характеристиками) или **Определением** (текст, который дает краткое и формальное определение термина или понятия). Ответь только с меткой: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mОписание\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m или \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mОпределение\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;124mТекст: Молекула — это наименьшая частица вещества, которая сохраняет его химические свойства.\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124mМетка:\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     11\u001b[0m result \u001b[38;5;241m=\u001b[39m generator(prompt, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/pipelines/__init__.py:798\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    794\u001b[0m     pretrained_model_name_or_path \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig) \u001b[38;5;129;01mand\u001b[39;00m pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m     \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m--> 798\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcache_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    807\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/utils/hub.py:426\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: meta-llama/Llama-3-8b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"meta-llama/Llama-3-8b\"  # Ejemplo, requiere acceso\n",
    "generator = pipeline(\"text-generation\", model=model_name)\n",
    "prompt = f\"\"\"\n",
    "Ты — эксперт по классификации текста на русском языке. Тебе дан текст, и ты должен определить, является ли он **Описанием** (текст, который описывает что-то с деталями, примерами или характеристиками) или **Определением** (текст, который дает краткое и формальное определение термина или понятия). Ответь только с меткой: \"Описание\" или \"Определение\".\n",
    "\n",
    "Текст: Молекула — это наименьшая частица вещества, которая сохраняет его химические свойства.\n",
    "Метка:\n",
    "\"\"\"\n",
    "result = generator(prompt, max_length=50)[0][\"generated_text\"].strip()\n",
    "label = result.split(\"Метка:\")[-1].strip()\n",
    "label_num = 1 if label == \"Описание\" else 0\n",
    "print(f\"Метка: {label}, Label numérico: {label_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6790024b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
