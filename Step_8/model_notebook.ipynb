{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e66a70c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m../dataset\u001b[00m\r\n",
      "├── \u001b[01;34men_espanol\u001b[00m\r\n",
      "│   ├── docx2txt.py\r\n",
      "│   ├── Второй_жанр_исходная.txt\r\n",
      "│   └── Первый_жанр_исходная.txt\r\n",
      "├── Второй_жанр_исходная.txt\r\n",
      "├── Первый_жанр_исходная.txt\r\n",
      "├── \u001b[01;34mСокращение по частям речи\u001b[00m\r\n",
      "│   ├── 1.Первый жанр исходная выборка.txt\r\n",
      "│   ├── 2.Первый жанр без клауз, включающих наречия.txt\r\n",
      "│   ├── 3.Первый жанр без клауз, включающих глаголы.txt\r\n",
      "│   ├── 4.Первый жанр без клауз, включающих глаголы и наречия.txt\r\n",
      "│   ├── 5.без клауз, включающих местоимения.txt\r\n",
      "│   ├── 6.без слов функциональных.txt\r\n",
      "│   ├── Без прилагательных второй жанр.txt\r\n",
      "│   ├── Без прилагательных первый жанр.txt\r\n",
      "│   ├── Второй_жанр без клауз, включающих местоимения.txt\r\n",
      "│   ├── Второй_жанр без слов функциональных.txt\r\n",
      "│   └── Случайные выборки.txt\r\n",
      "└── \u001b[01;34mсокращение по частотности\u001b[00m\r\n",
      "    ├── 1а_ без сокращений.txt\r\n",
      "    ├── 1б_Изъяты лексемы с частотой выше 100.txt\r\n",
      "    ├── 1в_Изъяты лексемы с частотой выше 49.txt\r\n",
      "    ├── 1г_Изъяты лексемы с частотой выше 29.txt\r\n",
      "    ├── 1д_Изъяты лексемы с частотой выше 9.txt\r\n",
      "    ├── 1е_Изъяты лексемы с частотой выше 5.txt\r\n",
      "    ├── 1ё_Изъяты лексемы с частотой выше 3.txt\r\n",
      "    ├── 2а_ без сокращений.txt\r\n",
      "    ├── 2б_Изъяты лексемы с частотой выше 100.txt\r\n",
      "    ├── 2в_Изъяты лексемы с частотой выше 49.txt\r\n",
      "    ├── 2г_Изъяты лексемы с частотой выше 29.txt\r\n",
      "    ├── 2д_Изъяты лексемы с частотой выше 9.txt\r\n",
      "    ├── 2е_Изъяты лексемы с частотой выше 5.txt\r\n",
      "    └── 2ё_Изъяты лексемы с частотой выше 3.txt\r\n",
      "\r\n",
      "3 directories, 30 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree ../dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b299fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_results(results, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c11038bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 18:51:06.725752: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Изъяты лексемы с частотой выше 100.csv\n",
      "test_Изъяты лексемы с частотой выше 100.csv\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([864])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([195])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.15260525305617462\n",
      "Epoch 2, Loss: 0.04537013787086363\n",
      "Epoch 3, Loss: 0.010898670166541167\n",
      "Epoch 4, Loss: 0.002245299813458351\n",
      "Epoch 5, Loss: 0.08411089717669205\n",
      "Epoch 6, Loss: 0.006139527890977829\n",
      "all_preds shape: (195,)\n",
      "all_labels shape: (195,)\n",
      "all_probs shape: (195, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 0]), Shape: torch.Size([864])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([198])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.1516093977778736\n",
      "Epoch 2, Loss: 0.027587906804573896\n",
      "Epoch 3, Loss: 0.0316382534090533\n",
      "Epoch 4, Loss: 0.029210287937463296\n",
      "Epoch 5, Loss: 0.019230314090641007\n",
      "Epoch 6, Loss: 0.0034761606948450208\n",
      "all_preds shape: (198,)\n",
      "all_labels shape: (198,)\n",
      "all_probs shape: (198, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 1]), Shape: torch.Size([864])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([196])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.15931806784054195\n",
      "Epoch 2, Loss: 0.044260742724873126\n",
      "Epoch 3, Loss: 0.016828602749026485\n",
      "Epoch 4, Loss: 0.0008765793733160805\n",
      "Epoch 5, Loss: 0.0004856276823880358\n",
      "Epoch 6, Loss: 0.00033380192029721067\n",
      "all_preds shape: (196,)\n",
      "all_labels shape: (196,)\n",
      "all_probs shape: (196, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 1, 0]), Shape: torch.Size([864])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([194])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.15443007179742885\n",
      "Epoch 2, Loss: 0.03325957185114493\n",
      "Epoch 3, Loss: 0.002976811285070316\n",
      "Epoch 4, Loss: 0.0005903536889322654\n",
      "Epoch 5, Loss: 0.00039305916666139466\n",
      "Epoch 6, Loss: 0.00029441425592974864\n",
      "all_preds shape: (194,)\n",
      "all_labels shape: (194,)\n",
      "all_probs shape: (194, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([864])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([201])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.16279103931177547\n",
      "Epoch 2, Loss: 0.11110334175949295\n",
      "Epoch 3, Loss: 0.06280565627471164\n",
      "Epoch 4, Loss: 0.015624687696496645\n",
      "Epoch 5, Loss: 0.07049914676678816\n",
      "Epoch 6, Loss: 0.006399743735932538\n",
      "all_preds shape: (201,)\n",
      "all_labels shape: (201,)\n",
      "all_probs shape: (201, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 0]), Shape: torch.Size([864])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([184])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.12881094802826368\n",
      "Epoch 2, Loss: 0.028823812975099793\n",
      "Epoch 3, Loss: 0.009793349328295639\n",
      "Epoch 4, Loss: 0.028143446742669092\n",
      "Epoch 5, Loss: 0.011224307646949051\n",
      "Epoch 6, Loss: 0.00048300847885440344\n",
      "all_preds shape: (184,)\n",
      "all_labels shape: (184,)\n",
      "all_probs shape: (184, 2)\n",
      "train_Изъяты лексемы с частотой выше 49.csv\n",
      "test_Изъяты лексемы с частотой выше 49.csv\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([887])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([177])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.19361730532753946\n",
      "Epoch 2, Loss: 0.024315195733964044\n",
      "Epoch 3, Loss: 0.013528681578463875\n",
      "Epoch 4, Loss: 0.01533981328923671\n",
      "Epoch 5, Loss: 0.005921172139226526\n",
      "Epoch 6, Loss: 0.0004548219294520095\n",
      "all_preds shape: (177,)\n",
      "all_labels shape: (177,)\n",
      "all_probs shape: (177, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([887])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([200])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.14223432574986614\n",
      "Epoch 2, Loss: 0.025047138293822564\n",
      "Epoch 3, Loss: 0.009227741924405564\n",
      "Epoch 4, Loss: 0.0366764826248982\n",
      "Epoch 5, Loss: 0.0018956193349107967\n",
      "Epoch 6, Loss: 0.00048464540252877796\n",
      "all_preds shape: (200,)\n",
      "all_labels shape: (200,)\n",
      "all_probs shape: (200, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([887])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([196])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.15985337954147585\n",
      "Epoch 2, Loss: 0.03644003955248211\n",
      "Epoch 3, Loss: 0.008766366890215847\n",
      "Epoch 4, Loss: 0.0054868283197200595\n",
      "Epoch 5, Loss: 0.017893170649975736\n",
      "Epoch 6, Loss: 0.008187652080128569\n",
      "all_preds shape: (196,)\n",
      "all_labels shape: (196,)\n",
      "all_probs shape: (196, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([887])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([182])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.12318670695198566\n",
      "Epoch 2, Loss: 0.09598069503304682\n",
      "Epoch 3, Loss: 0.018926650273247754\n",
      "Epoch 4, Loss: 0.016703622322113785\n",
      "Epoch 5, Loss: 0.015858547381607684\n",
      "Epoch 6, Loss: 0.016050480256256248\n",
      "all_preds shape: (182,)\n",
      "all_labels shape: (182,)\n",
      "all_probs shape: (182, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([887])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([193])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.13866789580788463\n",
      "Epoch 2, Loss: 0.002006663463751985\n",
      "Epoch 3, Loss: 0.0006205943172972184\n",
      "Epoch 4, Loss: 0.0004004167827328534\n",
      "Epoch 5, Loss: 0.0002917341592235191\n",
      "Epoch 6, Loss: 0.0002246997945413958\n",
      "all_preds shape: (193,)\n",
      "all_labels shape: (193,)\n",
      "all_probs shape: (193, 2)\n",
      "Train labels sample: tensor([1, 1, 1, 0, 0]), Shape: torch.Size([887])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([186])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.2566101236900847\n",
      "Epoch 2, Loss: 0.06716953066643327\n",
      "Epoch 3, Loss: 0.027681785552496358\n",
      "Epoch 4, Loss: 0.0037727082131563555\n",
      "Epoch 5, Loss: 0.0006480967624936186\n",
      "Epoch 6, Loss: 0.0004535737326867612\n",
      "all_preds shape: (186,)\n",
      "all_labels shape: (186,)\n",
      "all_probs shape: (186, 2)\n",
      "train_Изъяты лексемы с частотой выше 29.csv\n",
      "test_Изъяты лексемы с частотой выше 29.csv\n",
      "Train labels sample: tensor([0, 1, 1, 0, 0]), Shape: torch.Size([869])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([197])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.15010523034937\n",
      "Epoch 2, Loss: 0.016594804886897855\n",
      "Epoch 3, Loss: 0.04309592176736756\n",
      "Epoch 4, Loss: 0.006759239553304559\n",
      "Epoch 5, Loss: 0.0005867627057754858\n",
      "Epoch 6, Loss: 0.0003713385217865421\n",
      "all_preds shape: (197,)\n",
      "all_labels shape: (197,)\n",
      "all_probs shape: (197, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 1]), Shape: torch.Size([869])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([214])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.16252526252699848\n",
      "Epoch 2, Loss: 0.04049180644691329\n",
      "Epoch 3, Loss: 0.0009498519341419028\n",
      "Epoch 4, Loss: 0.0005887824872677977\n",
      "Epoch 5, Loss: 0.0003941809832626446\n",
      "Epoch 6, Loss: 0.00026588424992620606\n",
      "all_preds shape: (214,)\n",
      "all_labels shape: (214,)\n",
      "all_probs shape: (214, 2)\n",
      "Train labels sample: tensor([1, 1, 1, 0, 1]), Shape: torch.Size([869])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([210])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.13763574345714666\n",
      "Epoch 2, Loss: 0.07320273625241085\n",
      "Epoch 3, Loss: 0.0225733987711878\n",
      "Epoch 4, Loss: 0.011273034152955833\n",
      "Epoch 5, Loss: 0.002551689758812162\n",
      "Epoch 6, Loss: 0.0004745268993164328\n",
      "all_preds shape: (210,)\n",
      "all_labels shape: (210,)\n",
      "all_probs shape: (210, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 1]), Shape: torch.Size([869])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([211])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.18802121577107095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.07085742538964207\n",
      "Epoch 3, Loss: 0.04996554768982936\n",
      "Epoch 4, Loss: 0.0027789535064419563\n",
      "Epoch 5, Loss: 0.0012219659126871689\n",
      "Epoch 6, Loss: 0.0061402032733894885\n",
      "all_preds shape: (211,)\n",
      "all_labels shape: (211,)\n",
      "all_probs shape: (211, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([869])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([209])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.20310765788467092\n",
      "Epoch 2, Loss: 0.05638093565252017\n",
      "Epoch 3, Loss: 0.009596149157732724\n",
      "Epoch 4, Loss: 0.0012757855308750136\n",
      "Epoch 5, Loss: 0.0005365629775703631\n",
      "Epoch 6, Loss: 0.04999562518628822\n",
      "all_preds shape: (209,)\n",
      "all_labels shape: (209,)\n",
      "all_probs shape: (209, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([869])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([203])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.1328801965044642\n",
      "Epoch 2, Loss: 0.04094979789912362\n",
      "Epoch 3, Loss: 0.017572806688787586\n",
      "Epoch 4, Loss: 0.015936888434754853\n",
      "Epoch 5, Loss: 0.0005949326081794093\n",
      "Epoch 6, Loss: 0.00032731669595126403\n",
      "all_preds shape: (203,)\n",
      "all_labels shape: (203,)\n",
      "all_probs shape: (203, 2)\n",
      "train_Изъяты лексемы с частотой выше 9.csv\n",
      "test_Изъяты лексемы с частотой выше 9.csv\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([744])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([280])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.2911125054815784\n",
      "Epoch 2, Loss: 0.03080528328428045\n",
      "Epoch 3, Loss: 0.011613052435374508\n",
      "Epoch 4, Loss: 0.004008764318617371\n",
      "all_preds shape: (280,)\n",
      "all_labels shape: (280,)\n",
      "all_probs shape: (280, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([744])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([273])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.29128290992230177\n",
      "Epoch 2, Loss: 0.07660265111674865\n",
      "Epoch 3, Loss: 0.018411582432842504\n",
      "Epoch 4, Loss: 0.003669722873989182\n",
      "all_preds shape: (273,)\n",
      "all_labels shape: (273,)\n",
      "all_probs shape: (273, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([744])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([274])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.2803856253934403\n",
      "Epoch 2, Loss: 0.044968781197288386\n",
      "Epoch 3, Loss: 0.01367884743376635\n",
      "Epoch 4, Loss: 0.003215775558298143\n",
      "all_preds shape: (274,)\n",
      "all_labels shape: (274,)\n",
      "all_probs shape: (274, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 1]), Shape: torch.Size([744])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([284])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.2931657135486603\n",
      "Epoch 2, Loss: 0.04703637375496328\n",
      "Epoch 3, Loss: 0.028767115843947977\n",
      "Epoch 4, Loss: 0.006828431874358405\n",
      "all_preds shape: (284,)\n",
      "all_labels shape: (284,)\n",
      "all_probs shape: (284, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 1]), Shape: torch.Size([744])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([348])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.28722521709278226\n",
      "Epoch 2, Loss: 0.06101258644290889\n",
      "Epoch 3, Loss: 0.018854086122397955\n",
      "Epoch 4, Loss: 0.002834308043626758\n",
      "all_preds shape: (348,)\n",
      "all_labels shape: (348,)\n",
      "all_probs shape: (348, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 0]), Shape: torch.Size([744])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([275])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.2823248063214123\n",
      "Epoch 2, Loss: 0.03036607734975405\n",
      "Epoch 3, Loss: 0.011301024700514972\n",
      "Epoch 4, Loss: 0.0037987487982415282\n",
      "all_preds shape: (275,)\n",
      "all_labels shape: (275,)\n",
      "all_probs shape: (275, 2)\n",
      "train_Изъяты лексемы с частотой выше 5.csv\n",
      "test_Изъяты лексемы с частотой выше 5.csv\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([96])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([527])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.5553007970253626\n",
      "Epoch 2, Loss: 0.26980874687433243\n",
      "Epoch 3, Loss: 0.06847795906166236\n",
      "Epoch 4, Loss: 0.038704282604157925\n",
      "Epoch 5, Loss: 0.009039574147512516\n",
      "Epoch 6, Loss: 0.003555250780967375\n",
      "all_preds shape: (527,)\n",
      "all_labels shape: (527,)\n",
      "all_probs shape: (527, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 1, 1]), Shape: torch.Size([96])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([507])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.5087625583012899\n",
      "Epoch 2, Loss: 0.24180592969059944\n",
      "Epoch 3, Loss: 0.05233296876152357\n",
      "Epoch 4, Loss: 0.010825689261158308\n",
      "Epoch 5, Loss: 0.002893343335017562\n",
      "Epoch 6, Loss: 0.0016407226018297176\n",
      "all_preds shape: (507,)\n",
      "all_labels shape: (507,)\n",
      "all_probs shape: (507, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 1]), Shape: torch.Size([96])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([513])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.663967157403628\n",
      "Epoch 2, Loss: 0.41740326831738156\n",
      "Epoch 3, Loss: 0.11565776666005452\n",
      "Epoch 4, Loss: 0.09556670642147462\n",
      "Epoch 5, Loss: 0.054850117303431034\n",
      "Epoch 6, Loss: 0.01640884787775576\n",
      "all_preds shape: (513,)\n",
      "all_labels shape: (513,)\n",
      "all_probs shape: (513, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([96])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([575])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.3947350357969602\n",
      "Epoch 2, Loss: 0.07944101219375928\n",
      "Epoch 3, Loss: 0.018867545140286286\n",
      "Epoch 4, Loss: 0.003920146458161374\n",
      "Epoch 5, Loss: 0.015391186927445233\n",
      "Epoch 6, Loss: 0.07409140521970888\n",
      "all_preds shape: (575,)\n",
      "all_labels shape: (575,)\n",
      "all_probs shape: (575, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([96])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([491])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.5945424983898798\n",
      "Epoch 2, Loss: 0.35362885395685834\n",
      "Epoch 3, Loss: 0.1254932495454947\n",
      "Epoch 4, Loss: 0.06721898044149081\n",
      "Epoch 5, Loss: 0.018875181830177706\n",
      "Epoch 6, Loss: 0.011855944525450468\n",
      "all_preds shape: (491,)\n",
      "all_labels shape: (491,)\n",
      "all_probs shape: (491, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([96])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([456])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.5118935406208038\n",
      "Epoch 2, Loss: 0.10881388063232104\n",
      "Epoch 3, Loss: 0.0965107916854322\n",
      "Epoch 4, Loss: 0.009902000272025665\n",
      "Epoch 5, Loss: 0.06799456865216295\n",
      "Epoch 6, Loss: 0.06582310050725937\n",
      "all_preds shape: (456,)\n",
      "all_labels shape: (456,)\n",
      "all_probs shape: (456, 2)\n",
      "train_Изъяты лексемы с частотой выше 3.csv\n",
      "test_Изъяты лексемы с частотой выше 3.csv\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([90])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([700])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.4059263641635577\n",
      "Epoch 2, Loss: 0.10852393011252086\n",
      "Epoch 3, Loss: 0.02647430496290326\n",
      "Epoch 4, Loss: 0.012609201017767191\n",
      "Epoch 5, Loss: 0.004076294096497198\n",
      "Epoch 6, Loss: 0.0024878193507902324\n",
      "all_preds shape: (700,)\n",
      "all_labels shape: (700,)\n",
      "all_probs shape: (700, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([90])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([732])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.3923833593726158\n",
      "Epoch 2, Loss: 0.07437429515024026\n",
      "Epoch 3, Loss: 0.027204584951202076\n",
      "Epoch 4, Loss: 0.03558702138252556\n",
      "Epoch 5, Loss: 0.004934590659104288\n",
      "Epoch 6, Loss: 0.003844479564577341\n",
      "all_preds shape: (732,)\n",
      "all_labels shape: (732,)\n",
      "all_probs shape: (732, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([90])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([739])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.45187195142110187\n",
      "Epoch 2, Loss: 0.0923530130336682\n",
      "Epoch 3, Loss: 0.06562221903974812\n",
      "Epoch 4, Loss: 0.12892841904734573\n",
      "Epoch 5, Loss: 0.005226854079713424\n",
      "Epoch 6, Loss: 0.00703642494045198\n",
      "all_preds shape: (739,)\n",
      "all_labels shape: (739,)\n",
      "all_probs shape: (739, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([90])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([718])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.4313325087229411\n",
      "Epoch 2, Loss: 0.10302954353392124\n",
      "Epoch 3, Loss: 0.024288286299755175\n",
      "Epoch 4, Loss: 0.006094369649266203\n",
      "Epoch 5, Loss: 0.0029655122198164463\n",
      "Epoch 6, Loss: 0.00162203829192246\n",
      "all_preds shape: (718,)\n",
      "all_labels shape: (718,)\n",
      "all_probs shape: (718, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([90])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([715])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.46023185302813846\n",
      "Epoch 2, Loss: 0.05466688455392917\n",
      "Epoch 3, Loss: 0.009467203402891755\n",
      "Epoch 4, Loss: 0.0036041635321453214\n",
      "Epoch 5, Loss: 0.001989529021860411\n",
      "Epoch 6, Loss: 0.0013299401034601033\n",
      "all_preds shape: (715,)\n",
      "all_labels shape: (715,)\n",
      "all_probs shape: (715, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([90])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([690])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.5234932800134023\n",
      "Epoch 2, Loss: 0.1361775497595469\n",
      "Epoch 3, Loss: 0.0197645405617853\n",
      "Epoch 4, Loss: 0.004940160394956668\n",
      "Epoch 5, Loss: 0.0024265960479776063\n",
      "Epoch 6, Loss: 0.0015485288652901847\n",
      "all_preds shape: (690,)\n",
      "all_labels shape: (690,)\n",
      "all_probs shape: (690, 2)\n",
      "train_Без прилагательных первый-второй жанр.csv\n",
      "test_Без прилагательных первый-второй жанр.csv\n",
      "Train labels sample: tensor([0, 1, 0, 1, 1]), Shape: torch.Size([828])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([210])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.24483581080746192\n",
      "Epoch 2, Loss: 0.04587647173768626\n",
      "Epoch 3, Loss: 0.016023679734924093\n",
      "Epoch 4, Loss: 0.010102179465600504\n",
      "all_preds shape: (210,)\n",
      "all_labels shape: (210,)\n",
      "all_probs shape: (210, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 1, 0]), Shape: torch.Size([828])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([196])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.2554773804373466\n",
      "Epoch 2, Loss: 0.03204253050856865\n",
      "Epoch 3, Loss: 0.03691134703023216\n",
      "Epoch 4, Loss: 0.005260537727735937\n",
      "all_preds shape: (196,)\n",
      "all_labels shape: (196,)\n",
      "all_probs shape: (196, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 1, 0]), Shape: torch.Size([828])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([192])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.30317486793949056\n",
      "Epoch 2, Loss: 0.08806392335547851\n",
      "Epoch 3, Loss: 0.023817961301224735\n",
      "Epoch 4, Loss: 0.014369106041088413\n",
      "all_preds shape: (192,)\n",
      "all_labels shape: (192,)\n",
      "all_probs shape: (192, 2)\n",
      "Train labels sample: tensor([1, 1, 1, 0, 1]), Shape: torch.Size([828])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([178])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.20316555845336273\n",
      "Epoch 2, Loss: 0.0508556068301774\n",
      "Epoch 3, Loss: 0.010163093585735904\n",
      "Epoch 4, Loss: 0.008213517694877317\n",
      "all_preds shape: (178,)\n",
      "all_labels shape: (178,)\n",
      "all_probs shape: (178, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 0]), Shape: torch.Size([828])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([191])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.19074661410055482\n",
      "Epoch 2, Loss: 0.03331420857172746\n",
      "Epoch 3, Loss: 0.012364830427731458\n",
      "Epoch 4, Loss: 0.0077608823373269\n",
      "all_preds shape: (191,)\n",
      "all_labels shape: (191,)\n",
      "all_probs shape: (191, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 1, 1]), Shape: torch.Size([828])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([196])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.2147181941769444\n",
      "Epoch 2, Loss: 0.051989998256501094\n",
      "Epoch 3, Loss: 0.024456522821520384\n",
      "Epoch 4, Loss: 0.01554214028426661\n",
      "all_preds shape: (196,)\n",
      "all_labels shape: (196,)\n",
      "all_probs shape: (196, 2)\n",
      "train_1.Первый жанр исходная выборка.csv\n",
      "test_1.Первый жанр исходная выборка.csv\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([888])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([175])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.144001227928259\n",
      "Epoch 2, Loss: 0.03179722879680672\n",
      "Epoch 3, Loss: 0.019200859167280475\n",
      "Epoch 4, Loss: 0.001836397566616402\n",
      "Epoch 5, Loss: 0.00043989629141703645\n",
      "Epoch 6, Loss: 0.0003065737988176157\n",
      "all_preds shape: (175,)\n",
      "all_labels shape: (175,)\n",
      "all_probs shape: (175, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 1]), Shape: torch.Size([888])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([166])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.14771381236745842\n",
      "Epoch 2, Loss: 0.031575055160958855\n",
      "Epoch 3, Loss: 0.022010724619446722\n",
      "Epoch 4, Loss: 0.0050614574127913715\n",
      "Epoch 5, Loss: 0.00042610276744068997\n",
      "Epoch 6, Loss: 0.00030014793810551055\n",
      "all_preds shape: (166,)\n",
      "all_labels shape: (166,)\n",
      "all_probs shape: (166, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([888])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([165])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.11877137163537554\n",
      "Epoch 2, Loss: 0.015719939638594433\n",
      "Epoch 3, Loss: 0.010528035597027545\n",
      "Epoch 4, Loss: 0.0015102998253756336\n",
      "Epoch 5, Loss: 0.0005311813334368967\n",
      "Epoch 6, Loss: 0.00032344087230740115\n",
      "all_preds shape: (165,)\n",
      "all_labels shape: (165,)\n",
      "all_probs shape: (165, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 0]), Shape: torch.Size([888])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([165])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.18513600519924825\n",
      "Epoch 2, Loss: 0.03596555133117363\n",
      "Epoch 3, Loss: 0.0015625520827597938\n",
      "Epoch 4, Loss: 0.0007650885651985716\n",
      "Epoch 5, Loss: 0.0005120527047048588\n",
      "Epoch 6, Loss: 0.0003781253799388651\n",
      "all_preds shape: (165,)\n",
      "all_labels shape: (165,)\n",
      "all_probs shape: (165, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([888])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([165])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.1302313066719632\n",
      "Epoch 2, Loss: 0.009038309616568898\n",
      "Epoch 3, Loss: 0.018533388801318194\n",
      "Epoch 4, Loss: 0.001478586214943789\n",
      "Epoch 5, Loss: 0.00037245933812560646\n",
      "Epoch 6, Loss: 0.0002704516560437956\n",
      "all_preds shape: (165,)\n",
      "all_labels shape: (165,)\n",
      "all_probs shape: (165, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 1]), Shape: torch.Size([888])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([161])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.20284393991875863\n",
      "Epoch 2, Loss: 0.04669097753191766\n",
      "Epoch 3, Loss: 0.02359521213760932\n",
      "Epoch 4, Loss: 0.04476984449554168\n",
      "Epoch 5, Loss: 0.00865664733698525\n",
      "Epoch 6, Loss: 0.0007909271350529577\n",
      "all_preds shape: (161,)\n",
      "all_labels shape: (161,)\n",
      "all_probs shape: (161, 2)\n",
      "train_2.Первый жанр без клауз, включающих наречия.csv\n",
      "test_2.Первый жанр без клауз, включающих наречия.csv\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([876])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([174])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.12289655950259079\n",
      "Epoch 2, Loss: 0.02773738152940165\n",
      "Epoch 3, Loss: 0.001533306574194946\n",
      "Epoch 4, Loss: 0.03812040504152802\n",
      "Epoch 5, Loss: 0.0009426833114544438\n",
      "Epoch 6, Loss: 0.00038603948143480177\n",
      "all_preds shape: (174,)\n",
      "all_labels shape: (174,)\n",
      "all_probs shape: (174, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([876])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([170])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.12213258830491792\n",
      "Epoch 2, Loss: 0.011068819162689827\n",
      "Epoch 3, Loss: 0.007977277514609424\n",
      "Epoch 4, Loss: 0.015369299854765732\n",
      "Epoch 5, Loss: 0.1411484064110978\n",
      "Epoch 6, Loss: 0.04644605520350689\n",
      "all_preds shape: (170,)\n",
      "all_labels shape: (170,)\n",
      "all_probs shape: (170, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([876])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([172])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.16040348815308375\n",
      "Epoch 2, Loss: 0.09498271236873486\n",
      "Epoch 3, Loss: 0.028476511576974933\n",
      "Epoch 4, Loss: 0.01040956681509587\n",
      "Epoch 5, Loss: 0.002934862109197473\n",
      "Epoch 6, Loss: 0.0016896222728643227\n",
      "all_preds shape: (172,)\n",
      "all_labels shape: (172,)\n",
      "all_probs shape: (172, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([876])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([163])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.1430518848228861\n",
      "Epoch 2, Loss: 0.03757168697193265\n",
      "Epoch 3, Loss: 0.015210564009083266\n",
      "Epoch 4, Loss: 0.02019871162695133\n",
      "Epoch 5, Loss: 0.002452393697405403\n",
      "Epoch 6, Loss: 0.00041425858984108676\n",
      "all_preds shape: (163,)\n",
      "all_labels shape: (163,)\n",
      "all_probs shape: (163, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 1]), Shape: torch.Size([876])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([175])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.13196643389422785\n",
      "Epoch 2, Loss: 0.0164261506320062\n",
      "Epoch 3, Loss: 0.0013386211220429024\n",
      "Epoch 4, Loss: 0.0005074471351690591\n",
      "Epoch 5, Loss: 0.000345531920902431\n",
      "Epoch 6, Loss: 0.00037442786163989115\n",
      "all_preds shape: (175,)\n",
      "all_labels shape: (175,)\n",
      "all_probs shape: (175, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 1, 0]), Shape: torch.Size([876])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([165])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.13155341623350977\n",
      "Epoch 2, Loss: 0.013970901423387907\n",
      "Epoch 3, Loss: 0.001271743181330914\n",
      "Epoch 4, Loss: 0.0005559563821985979\n",
      "Epoch 5, Loss: 0.00037026227166114204\n",
      "Epoch 6, Loss: 0.0002865920470371334\n",
      "all_preds shape: (165,)\n",
      "all_labels shape: (165,)\n",
      "all_probs shape: (165, 2)\n",
      "train_3.Первый жанр без клауз, включающих глаголы.csv\n",
      "test_3.Первый жанр без клауз, включающих глаголы.csv\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([820])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([222])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.13816696520930585\n",
      "Epoch 2, Loss: 0.014699109057591368\n",
      "Epoch 3, Loss: 0.0009746644825029832\n",
      "Epoch 4, Loss: 0.00047108640753252146\n",
      "Epoch 5, Loss: 0.00032842221928429074\n",
      "Epoch 6, Loss: 0.0002424068345415155\n",
      "all_preds shape: (222,)\n",
      "all_labels shape: (222,)\n",
      "all_probs shape: (222, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 1, 1]), Shape: torch.Size([820])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([216])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.12239279918587552\n",
      "Epoch 2, Loss: 0.03325245477697955\n",
      "Epoch 3, Loss: 0.006056408438150986\n",
      "Epoch 4, Loss: 0.0006426947629034447\n",
      "Epoch 5, Loss: 0.00042877337378181086\n",
      "Epoch 6, Loss: 0.00029735606641714607\n",
      "all_preds shape: (216,)\n",
      "all_labels shape: (216,)\n",
      "all_probs shape: (216, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 1]), Shape: torch.Size([820])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([212])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.293786557794262\n",
      "Epoch 2, Loss: 0.04613053770467209\n",
      "Epoch 3, Loss: 0.007414233810357893\n",
      "Epoch 4, Loss: 0.012244898384848896\n",
      "Epoch 5, Loss: 0.0010872940788081346\n",
      "Epoch 6, Loss: 0.0005722508923589395\n",
      "all_preds shape: (212,)\n",
      "all_labels shape: (212,)\n",
      "all_probs shape: (212, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([820])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([213])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.18397003337024495\n",
      "Epoch 2, Loss: 0.04139983443579135\n",
      "Epoch 3, Loss: 0.014969721114119658\n",
      "Epoch 4, Loss: 0.0024262421659211842\n",
      "Epoch 5, Loss: 0.0005188183998912931\n",
      "Epoch 6, Loss: 0.0003674075179822886\n",
      "all_preds shape: (213,)\n",
      "all_labels shape: (213,)\n",
      "all_probs shape: (213, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 1]), Shape: torch.Size([820])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([207])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.14247798575804785\n",
      "Epoch 2, Loss: 0.01699530724615145\n",
      "Epoch 3, Loss: 0.0026427592674735934\n",
      "Epoch 4, Loss: 0.02267774793458207\n",
      "Epoch 5, Loss: 0.018305813441447053\n",
      "Epoch 6, Loss: 0.0019929895508819474\n",
      "all_preds shape: (207,)\n",
      "all_labels shape: (207,)\n",
      "all_probs shape: (207, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([820])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([222])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.2586703549963064\n",
      "Epoch 2, Loss: 0.14641786142825508\n",
      "Epoch 3, Loss: 0.07837764071658827\n",
      "Epoch 4, Loss: 0.04109030893036666\n",
      "Epoch 5, Loss: 0.04688061611243309\n",
      "Epoch 6, Loss: 0.01919957886611183\n",
      "all_preds shape: (222,)\n",
      "all_labels shape: (222,)\n",
      "all_probs shape: (222, 2)\n",
      "train_4.Первый жанр без клауз, включающих глаголы и наречия.csv\n",
      "test_4.Первый жанр без клауз, включающих глаголы и наречия.csv\n",
      "Train labels sample: tensor([1, 0, 0, 1, 1]), Shape: torch.Size([800])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([237])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.16173380505293608\n",
      "Epoch 2, Loss: 0.033831591363996265\n",
      "Epoch 3, Loss: 0.0021367271232884378\n",
      "Epoch 4, Loss: 0.02312401502393186\n",
      "Epoch 5, Loss: 0.04121449095662683\n",
      "Epoch 6, Loss: 0.0007450312311993912\n",
      "all_preds shape: (237,)\n",
      "all_labels shape: (237,)\n",
      "all_probs shape: (237, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 0]), Shape: torch.Size([800])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([236])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.11309501023963094\n",
      "Epoch 2, Loss: 0.0765790633764118\n",
      "Epoch 3, Loss: 0.01052346512209624\n",
      "Epoch 4, Loss: 0.0013133667362853884\n",
      "Epoch 5, Loss: 0.000621439553797245\n",
      "Epoch 6, Loss: 0.0004490151262143627\n",
      "all_preds shape: (236,)\n",
      "all_labels shape: (236,)\n",
      "all_probs shape: (236, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 1, 0]), Shape: torch.Size([800])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([232])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.1138315919181332\n",
      "Epoch 2, Loss: 0.029483395460993052\n",
      "Epoch 3, Loss: 0.0022204879962373523\n",
      "Epoch 4, Loss: 0.0004987848829478025\n",
      "Epoch 5, Loss: 0.010044139157980681\n",
      "Epoch 6, Loss: 0.016300708599737847\n",
      "all_preds shape: (232,)\n",
      "all_labels shape: (232,)\n",
      "all_probs shape: (232, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([800])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([231])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.15637398905586453\n",
      "Epoch 2, Loss: 0.008145222422899678\n",
      "Epoch 3, Loss: 0.0033238550380337985\n",
      "Epoch 4, Loss: 0.0006053511897334829\n",
      "Epoch 5, Loss: 0.0002740338337025605\n",
      "Epoch 6, Loss: 0.012823401949717664\n",
      "all_preds shape: (231,)\n",
      "all_labels shape: (231,)\n",
      "all_probs shape: (231, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels sample: tensor([1, 0, 1, 0, 0]), Shape: torch.Size([800])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([233])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.12049354643095285\n",
      "Epoch 2, Loss: 0.02085316415177658\n",
      "Epoch 3, Loss: 0.04771038487786427\n",
      "Epoch 4, Loss: 0.01508319441927597\n",
      "Epoch 5, Loss: 0.014618693875381722\n",
      "Epoch 6, Loss: 0.0005797650513704866\n",
      "all_preds shape: (233,)\n",
      "all_labels shape: (233,)\n",
      "all_probs shape: (233, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 1, 1]), Shape: torch.Size([800])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([231])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.17068211104720832\n",
      "Epoch 2, Loss: 0.05468850147910416\n",
      "Epoch 3, Loss: 0.021826258045621216\n",
      "Epoch 4, Loss: 0.008195388703607023\n",
      "Epoch 5, Loss: 0.0007552510534878821\n",
      "Epoch 6, Loss: 0.00048322645598091184\n",
      "all_preds shape: (231,)\n",
      "all_labels shape: (231,)\n",
      "all_probs shape: (231, 2)\n",
      "train_5.без клауз, включающих местоимения.txt.csv\n",
      "test_5.без клауз, включающих местоимения.txt.csv\n",
      "Train labels sample: tensor([0, 1, 1, 1, 1]), Shape: torch.Size([853])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([189])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.16791789761865167\n",
      "Epoch 2, Loss: 0.037753197879754695\n",
      "Epoch 3, Loss: 0.0020216414866606807\n",
      "Epoch 4, Loss: 0.0006308713259554848\n",
      "Epoch 5, Loss: 0.0004346347090581225\n",
      "Epoch 6, Loss: 0.0003340261345901699\n",
      "all_preds shape: (189,)\n",
      "all_labels shape: (189,)\n",
      "all_probs shape: (189, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 0]), Shape: torch.Size([853])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([183])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.14678664901293814\n",
      "Epoch 2, Loss: 0.02915500749454454\n",
      "Epoch 3, Loss: 0.0015311092709587818\n",
      "Epoch 4, Loss: 0.04376434839300432\n",
      "Epoch 5, Loss: 0.009617383071195541\n",
      "Epoch 6, Loss: 0.0015333085493357092\n",
      "all_preds shape: (183,)\n",
      "all_labels shape: (183,)\n",
      "all_probs shape: (183, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([853])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([192])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.1354027165065485\n",
      "Epoch 2, Loss: 0.04690091525144116\n",
      "Epoch 3, Loss: 0.02156993714635502\n",
      "Epoch 4, Loss: 0.0024895574919086088\n",
      "Epoch 5, Loss: 0.0006111954878886334\n",
      "Epoch 6, Loss: 0.0004076398089873018\n",
      "all_preds shape: (192,)\n",
      "all_labels shape: (192,)\n",
      "all_probs shape: (192, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([853])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([185])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.18245297903195024\n",
      "Epoch 2, Loss: 0.04950452289388826\n",
      "Epoch 3, Loss: 0.010161918458632298\n",
      "Epoch 4, Loss: 0.00942755542712769\n",
      "Epoch 5, Loss: 0.0025567996242359557\n",
      "Epoch 6, Loss: 0.0005919790667205773\n",
      "all_preds shape: (185,)\n",
      "all_labels shape: (185,)\n",
      "all_probs shape: (185, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 1, 0]), Shape: torch.Size([853])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([190])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.12241658495946063\n",
      "Epoch 2, Loss: 0.014692101099407646\n",
      "Epoch 3, Loss: 0.022676292185982067\n",
      "Epoch 4, Loss: 0.0007492213297626693\n",
      "Epoch 5, Loss: 0.0008354691063114269\n",
      "Epoch 6, Loss: 0.0002986795343868262\n",
      "all_preds shape: (190,)\n",
      "all_labels shape: (190,)\n",
      "all_probs shape: (190, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([853])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([188])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.13714731836915706\n",
      "Epoch 2, Loss: 0.0195068454982161\n",
      "Epoch 3, Loss: 0.023589869727233977\n",
      "Epoch 4, Loss: 0.03183087320670624\n",
      "Epoch 5, Loss: 0.0006901786842650768\n",
      "Epoch 6, Loss: 0.0003925968377626742\n",
      "all_preds shape: (188,)\n",
      "all_labels shape: (188,)\n",
      "all_probs shape: (188, 2)\n",
      "train_6.без слов функциональных.txt.csv\n",
      "test_6.без слов функциональных.txt.csv\n",
      "Train labels sample: tensor([0, 1, 1, 1, 0]), Shape: torch.Size([916])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([187])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.16210128299506574\n",
      "Epoch 2, Loss: 0.048639037453665816\n",
      "Epoch 3, Loss: 0.006648624977217345\n",
      "Epoch 4, Loss: 0.0006777436135272527\n",
      "Epoch 5, Loss: 0.0004308252567264797\n",
      "Epoch 6, Loss: 0.0003073380972511649\n",
      "all_preds shape: (187,)\n",
      "all_labels shape: (187,)\n",
      "all_probs shape: (187, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([916])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([183])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.16371054655535083\n",
      "Epoch 2, Loss: 0.02252052380735504\n",
      "Epoch 3, Loss: 0.04305778095771655\n",
      "Epoch 4, Loss: 0.0044348546131192865\n",
      "Epoch 5, Loss: 0.004645359564917804\n",
      "Epoch 6, Loss: 0.027634984573774876\n",
      "all_preds shape: (183,)\n",
      "all_labels shape: (183,)\n",
      "all_probs shape: (183, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 1]), Shape: torch.Size([916])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([172])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.1744490327345657\n",
      "Epoch 2, Loss: 0.03396159465071456\n",
      "Epoch 3, Loss: 0.01214463502100412\n",
      "Epoch 4, Loss: 0.03956449527297458\n",
      "Epoch 5, Loss: 0.005343052144624004\n",
      "Epoch 6, Loss: 0.0005896938166878154\n",
      "all_preds shape: (172,)\n",
      "all_labels shape: (172,)\n",
      "all_probs shape: (172, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 1]), Shape: torch.Size([916])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([174])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.09694918907841603\n",
      "Epoch 2, Loss: 0.02868603141416378\n",
      "Epoch 3, Loss: 0.037186058585387496\n",
      "Epoch 4, Loss: 0.0013551894005323793\n",
      "Epoch 5, Loss: 0.009480364060185946\n",
      "Epoch 6, Loss: 0.004018825971609752\n",
      "all_preds shape: (174,)\n",
      "all_labels shape: (174,)\n",
      "all_probs shape: (174, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 0]), Shape: torch.Size([916])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([180])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.12395117376079975\n",
      "Epoch 2, Loss: 0.02951963494555896\n",
      "Epoch 3, Loss: 0.022662541273066068\n",
      "Epoch 4, Loss: 0.015220930998550793\n",
      "Epoch 5, Loss: 0.015629048803459916\n",
      "Epoch 6, Loss: 0.028695812538177868\n",
      "all_preds shape: (180,)\n",
      "all_labels shape: (180,)\n",
      "all_probs shape: (180, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 1]), Shape: torch.Size([916])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([177])\n",
      "##########################DeepPavlov/rubert-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.13825201415777977\n",
      "Epoch 2, Loss: 0.02318765952829914\n",
      "Epoch 3, Loss: 0.010666600394399902\n",
      "Epoch 4, Loss: 0.022544199377017352\n",
      "Epoch 5, Loss: 0.0007891758096179571\n",
      "Epoch 6, Loss: 0.00045225301010791083\n",
      "all_preds shape: (177,)\n",
      "all_labels shape: (177,)\n",
      "all_probs shape: (177, 2)\n",
      "train_Изъяты лексемы с частотой выше 100.csv\n",
      "test_Изъяты лексемы с частотой выше 100.csv\n",
      "Train labels sample: tensor([1, 1, 1, 0, 0]), Shape: torch.Size([871])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([190])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.19445850911804222\n",
      "Epoch 2, Loss: 0.07113376090472395\n",
      "Epoch 3, Loss: 0.031305534020066264\n",
      "Epoch 4, Loss: 0.052961886396885594\n",
      "Epoch 5, Loss: 0.0385604165748439\n",
      "Epoch 6, Loss: 0.02863082340706817\n",
      "all_preds shape: (190,)\n",
      "all_labels shape: (190,)\n",
      "all_probs shape: (190, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([871])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([192])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.23093821555376054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.0415452288379046\n",
      "Epoch 3, Loss: 0.010046951582824643\n",
      "Epoch 4, Loss: 0.09768632102165033\n",
      "Epoch 5, Loss: 0.05925903084941886\n",
      "Epoch 6, Loss: 0.023558724756267937\n",
      "all_preds shape: (192,)\n",
      "all_labels shape: (192,)\n",
      "all_probs shape: (192, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 1]), Shape: torch.Size([871])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([190])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.2500280161472884\n",
      "Epoch 2, Loss: 0.07359184354374354\n",
      "Epoch 3, Loss: 0.01981952577744695\n",
      "Epoch 4, Loss: 0.0017114111042412167\n",
      "Epoch 5, Loss: 0.000654984766151756\n",
      "Epoch 6, Loss: 0.0004435944423841482\n",
      "all_preds shape: (190,)\n",
      "all_labels shape: (190,)\n",
      "all_probs shape: (190, 2)\n",
      "Train labels sample: tensor([1, 1, 1, 0, 1]), Shape: torch.Size([871])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([200])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.18178426560691813\n",
      "Epoch 2, Loss: 0.019463046900504693\n",
      "Epoch 3, Loss: 0.015399332034443929\n",
      "Epoch 4, Loss: 0.01707341605585746\n",
      "Epoch 5, Loss: 0.008154556282203306\n",
      "Epoch 6, Loss: 0.00031826528535351494\n",
      "all_preds shape: (200,)\n",
      "all_labels shape: (200,)\n",
      "all_probs shape: (200, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([871])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([189])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.17661162259226496\n",
      "Epoch 2, Loss: 0.06868668385273353\n",
      "Epoch 3, Loss: 0.09307016982188956\n",
      "Epoch 4, Loss: 0.02628409045071087\n",
      "Epoch 5, Loss: 0.004571360694667833\n",
      "Epoch 6, Loss: 0.016822816904591906\n",
      "all_preds shape: (189,)\n",
      "all_labels shape: (189,)\n",
      "all_probs shape: (189, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([871])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([191])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.15023290789262814\n",
      "Epoch 2, Loss: 0.01732544923328202\n",
      "Epoch 3, Loss: 0.03694728305774995\n",
      "Epoch 4, Loss: 0.03793335489039733\n",
      "Epoch 5, Loss: 0.017689912575720387\n",
      "Epoch 6, Loss: 0.0007200138362928886\n",
      "all_preds shape: (191,)\n",
      "all_labels shape: (191,)\n",
      "all_probs shape: (191, 2)\n",
      "train_Изъяты лексемы с частотой выше 49.csv\n",
      "test_Изъяты лексемы с частотой выше 49.csv\n",
      "Train labels sample: tensor([0, 1, 1, 0, 1]), Shape: torch.Size([900])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([176])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.24304176197109514\n",
      "Epoch 2, Loss: 0.05492896276215712\n",
      "Epoch 3, Loss: 0.10766229309190653\n",
      "Epoch 4, Loss: 0.06461608680291918\n",
      "Epoch 5, Loss: 0.016070647994383125\n",
      "Epoch 6, Loss: 0.01676937809066945\n",
      "all_preds shape: (176,)\n",
      "all_labels shape: (176,)\n",
      "all_probs shape: (176, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 1]), Shape: torch.Size([900])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([180])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.13470077830807944\n",
      "Epoch 2, Loss: 0.04465680513653512\n",
      "Epoch 3, Loss: 0.06973253415995523\n",
      "Epoch 4, Loss: 0.014373301023927828\n",
      "Epoch 5, Loss: 0.00854179373709485\n",
      "Epoch 6, Loss: 0.004379184537755097\n",
      "all_preds shape: (180,)\n",
      "all_labels shape: (180,)\n",
      "all_probs shape: (180, 2)\n",
      "Train labels sample: tensor([1, 1, 1, 0, 0]), Shape: torch.Size([900])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([184])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.16503134054507604\n",
      "Epoch 2, Loss: 0.06315187208779287\n",
      "Epoch 3, Loss: 0.0836397319492933\n",
      "Epoch 4, Loss: 0.07883852095294155\n",
      "Epoch 5, Loss: 0.09445618659159855\n",
      "Epoch 6, Loss: 0.06550941340167794\n",
      "all_preds shape: (184,)\n",
      "all_labels shape: (184,)\n",
      "all_probs shape: (184, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 0]), Shape: torch.Size([900])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([191])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.16639399010557354\n",
      "Epoch 2, Loss: 0.0558290613708985\n",
      "Epoch 3, Loss: 0.03940031400947064\n",
      "Epoch 4, Loss: 0.016975196454216513\n",
      "Epoch 5, Loss: 0.016587526325071066\n",
      "Epoch 6, Loss: 0.016231994659296776\n",
      "all_preds shape: (191,)\n",
      "all_labels shape: (191,)\n",
      "all_probs shape: (191, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([900])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([200])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.19967923620552347\n",
      "Epoch 2, Loss: 0.03427551654902728\n",
      "Epoch 3, Loss: 0.011388162639106443\n",
      "Epoch 4, Loss: 0.011684512065952285\n",
      "Epoch 5, Loss: 0.0008561738691161992\n",
      "Epoch 6, Loss: 0.00035881405959739104\n",
      "all_preds shape: (200,)\n",
      "all_labels shape: (200,)\n",
      "all_probs shape: (200, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 0]), Shape: torch.Size([900])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([196])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.20892610630477992\n",
      "Epoch 2, Loss: 0.06318839015275762\n",
      "Epoch 3, Loss: 0.04075702248557814\n",
      "Epoch 4, Loss: 0.04844160463146277\n",
      "Epoch 5, Loss: 0.037750278244187176\n",
      "Epoch 6, Loss: 0.01739304655092654\n",
      "all_preds shape: (196,)\n",
      "all_labels shape: (196,)\n",
      "all_probs shape: (196, 2)\n",
      "train_Изъяты лексемы с частотой выше 29.csv\n",
      "test_Изъяты лексемы с частотой выше 29.csv\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([887])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([197])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.2164988438432504\n",
      "Epoch 2, Loss: 0.06469256768884536\n",
      "Epoch 3, Loss: 0.04165740921494684\n",
      "Epoch 4, Loss: 0.006777108017039219\n",
      "Epoch 5, Loss: 0.11375968814640405\n",
      "Epoch 6, Loss: 0.0244268946615713\n",
      "all_preds shape: (197,)\n",
      "all_labels shape: (197,)\n",
      "all_probs shape: (197, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([887])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([191])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.1832961995262719\n",
      "Epoch 2, Loss: 0.1554868733510375\n",
      "Epoch 3, Loss: 0.007198782070190646\n",
      "Epoch 4, Loss: 0.02979711780998124\n",
      "Epoch 5, Loss: 0.016964809699857142\n",
      "Epoch 6, Loss: 0.002686168444467642\n",
      "all_preds shape: (191,)\n",
      "all_labels shape: (191,)\n",
      "all_probs shape: (191, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([887])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([201])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.18157674969240492\n",
      "Epoch 2, Loss: 0.04432248417288065\n",
      "Epoch 3, Loss: 0.02905731820127195\n",
      "Epoch 4, Loss: 0.0015055220865178853\n",
      "Epoch 5, Loss: 0.0004969624408529073\n",
      "Epoch 6, Loss: 0.00034958721257031096\n",
      "all_preds shape: (201,)\n",
      "all_labels shape: (201,)\n",
      "all_probs shape: (201, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([887])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([189])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.16168644214381597\n",
      "Epoch 2, Loss: 0.046118438235550584\n",
      "Epoch 3, Loss: 0.047394551582069004\n",
      "Epoch 4, Loss: 0.11043219385984619\n",
      "Epoch 5, Loss: 0.015320627960526119\n",
      "Epoch 6, Loss: 0.08252383761906198\n",
      "all_preds shape: (189,)\n",
      "all_labels shape: (189,)\n",
      "all_probs shape: (189, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([887])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([204])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.18379786223106617\n",
      "Epoch 2, Loss: 0.07067580799671955\n",
      "Epoch 3, Loss: 0.025685628144336597\n",
      "Epoch 4, Loss: 0.0024903963182753485\n",
      "Epoch 5, Loss: 0.022765994069881605\n",
      "Epoch 6, Loss: 0.014127844019510252\n",
      "all_preds shape: (204,)\n",
      "all_labels shape: (204,)\n",
      "all_probs shape: (204, 2)\n",
      "Train labels sample: tensor([1, 1, 1, 0, 0]), Shape: torch.Size([887])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([188])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.1955484803433397\n",
      "Epoch 2, Loss: 0.0569864892833201\n",
      "Epoch 3, Loss: 0.04798125894324455\n",
      "Epoch 4, Loss: 0.025838699347722077\n",
      "Epoch 5, Loss: 0.008841259369676533\n",
      "Epoch 6, Loss: 0.008986556716471179\n",
      "all_preds shape: (188,)\n",
      "all_labels shape: (188,)\n",
      "all_probs shape: (188, 2)\n",
      "train_Изъяты лексемы с частотой выше 9.csv\n",
      "test_Изъяты лексемы с частотой выше 9.csv\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([766])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([350])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.3074260363355279\n",
      "Epoch 2, Loss: 0.12086865142919123\n",
      "Epoch 3, Loss: 0.05718107733021801\n",
      "Epoch 4, Loss: 0.027791581485265244\n",
      "all_preds shape: (350,)\n",
      "all_labels shape: (350,)\n",
      "all_probs shape: (350, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 1, 1]), Shape: torch.Size([766])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([343])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.27117346149558824\n",
      "Epoch 2, Loss: 0.045552167537001274\n",
      "Epoch 3, Loss: 0.04641086338475967\n",
      "Epoch 4, Loss: 0.019491860778847087\n",
      "all_preds shape: (343,)\n",
      "all_labels shape: (343,)\n",
      "all_probs shape: (343, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([766])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([259])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.27034671356280643\n",
      "Epoch 2, Loss: 0.12400661502033472\n",
      "Epoch 3, Loss: 0.05989208029738317\n",
      "Epoch 4, Loss: 0.01358330932756265\n",
      "all_preds shape: (259,)\n",
      "all_labels shape: (259,)\n",
      "all_probs shape: (259, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 1]), Shape: torch.Size([766])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([274])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.3037744245181481\n",
      "Epoch 2, Loss: 0.08604913026404877\n",
      "Epoch 3, Loss: 0.023073533462593332\n",
      "Epoch 4, Loss: 0.03367012117814738\n",
      "all_preds shape: (274,)\n",
      "all_labels shape: (274,)\n",
      "all_probs shape: (274, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([766])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([261])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.335032753025492\n",
      "Epoch 2, Loss: 0.09894330004074921\n",
      "Epoch 3, Loss: 0.043088463154466204\n",
      "Epoch 4, Loss: 0.02023424720391631\n",
      "all_preds shape: (261,)\n",
      "all_labels shape: (261,)\n",
      "all_probs shape: (261, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([766])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([264])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.2750605439456801\n",
      "Epoch 2, Loss: 0.07352838610919814\n",
      "Epoch 3, Loss: 0.04349759634351358\n",
      "Epoch 4, Loss: 0.045454154547769576\n",
      "all_preds shape: (264,)\n",
      "all_labels shape: (264,)\n",
      "all_probs shape: (264, 2)\n",
      "train_Изъяты лексемы с частотой выше 5.csv\n",
      "test_Изъяты лексемы с частотой выше 5.csv\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([96])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([528])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.5827696621417999\n",
      "Epoch 2, Loss: 0.23560085147619247\n",
      "Epoch 3, Loss: 0.03116965914765994\n",
      "Epoch 4, Loss: 0.2621511744800955\n",
      "Epoch 5, Loss: 0.07064536415661375\n",
      "Epoch 6, Loss: 0.14815244497731328\n",
      "all_preds shape: (528,)\n",
      "all_labels shape: (528,)\n",
      "all_probs shape: (528, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 1, 1]), Shape: torch.Size([96])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([507])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.5464421957731247\n",
      "Epoch 2, Loss: 0.2769487177332242\n",
      "Epoch 3, Loss: 0.19453518092632294\n",
      "Epoch 4, Loss: 0.10789975089331467\n",
      "Epoch 5, Loss: 0.029584339664628107\n",
      "Epoch 6, Loss: 0.011788510795061788\n",
      "all_preds shape: (507,)\n",
      "all_labels shape: (507,)\n",
      "all_probs shape: (507, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 1]), Shape: torch.Size([96])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([513])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.5802063594261805\n",
      "Epoch 2, Loss: 0.17048896476626396\n",
      "Epoch 3, Loss: 0.01622185033435623\n",
      "Epoch 4, Loss: 0.005155442166142166\n",
      "Epoch 5, Loss: 0.0023297646548599005\n",
      "Epoch 6, Loss: 0.0013640189815002184\n",
      "all_preds shape: (513,)\n",
      "all_labels shape: (513,)\n",
      "all_probs shape: (513, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([96])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([576])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.6034687558809916\n",
      "Epoch 2, Loss: 0.4481329371531804\n",
      "Epoch 3, Loss: 0.18050498887896538\n",
      "Epoch 4, Loss: 0.038878148421645164\n",
      "Epoch 5, Loss: 0.013652274617925286\n",
      "Epoch 6, Loss: 0.006160825879002611\n",
      "all_preds shape: (576,)\n",
      "all_labels shape: (576,)\n",
      "all_probs shape: (576, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([96])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([491])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.5600795596837997\n",
      "Epoch 2, Loss: 0.22590360728402933\n",
      "Epoch 3, Loss: 0.0370883682432274\n",
      "Epoch 4, Loss: 0.00773927530584236\n",
      "Epoch 5, Loss: 0.06068232092851152\n",
      "Epoch 6, Loss: 0.24879474332556129\n",
      "all_preds shape: (491,)\n",
      "all_labels shape: (491,)\n",
      "all_probs shape: (491, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([96])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([456])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.5640816589196523\n",
      "Epoch 2, Loss: 0.3495241403579712\n",
      "Epoch 3, Loss: 0.10852545003096263\n",
      "Epoch 4, Loss: 0.01850482091928522\n",
      "Epoch 5, Loss: 0.005697042332030833\n",
      "Epoch 6, Loss: 0.0021724156298053763\n",
      "all_preds shape: (456,)\n",
      "all_labels shape: (456,)\n",
      "all_probs shape: (456, 2)\n",
      "train_Изъяты лексемы с частотой выше 3.csv\n",
      "test_Изъяты лексемы с частотой выше 3.csv\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([90])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([700])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.5003189593553543\n",
      "Epoch 2, Loss: 0.12919392747183642\n",
      "Epoch 3, Loss: 0.07995889941230416\n",
      "Epoch 4, Loss: 0.01829740055836737\n",
      "Epoch 5, Loss: 0.013563814107328653\n",
      "Epoch 6, Loss: 0.0035587475867941976\n",
      "all_preds shape: (700,)\n",
      "all_labels shape: (700,)\n",
      "all_probs shape: (700, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([90])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([732])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.48279064893722534\n",
      "Epoch 2, Loss: 0.2618274539709091\n",
      "Epoch 3, Loss: 0.17201673611998558\n",
      "Epoch 4, Loss: 0.019801019225269556\n",
      "Epoch 5, Loss: 0.004654460198556383\n",
      "Epoch 6, Loss: 0.002217299615343412\n",
      "all_preds shape: (732,)\n",
      "all_labels shape: (732,)\n",
      "all_probs shape: (732, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([90])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([739])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.6180529296398163\n",
      "Epoch 2, Loss: 0.35381291309992474\n",
      "Epoch 3, Loss: 0.2438050409158071\n",
      "Epoch 4, Loss: 0.14264032741387686\n",
      "Epoch 5, Loss: 0.08054885640740395\n",
      "Epoch 6, Loss: 0.04656520547966162\n",
      "all_preds shape: (739,)\n",
      "all_labels shape: (739,)\n",
      "all_probs shape: (739, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([90])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([720])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.5625097354253134\n",
      "Epoch 2, Loss: 0.3176409850517909\n",
      "Epoch 3, Loss: 0.1594801712781191\n",
      "Epoch 4, Loss: 0.07475194210807483\n",
      "Epoch 5, Loss: 0.06342063254366319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.20388219210629663\n",
      "all_preds shape: (720,)\n",
      "all_labels shape: (720,)\n",
      "all_probs shape: (720, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([90])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([715])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.5988524804512659\n",
      "Epoch 2, Loss: 0.3155803208549817\n",
      "Epoch 3, Loss: 0.22727027659614882\n",
      "Epoch 4, Loss: 0.19461923775573572\n",
      "Epoch 5, Loss: 0.08215143686781327\n",
      "Epoch 6, Loss: 0.25419609403858584\n",
      "all_preds shape: (715,)\n",
      "all_labels shape: (715,)\n",
      "all_probs shape: (715, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([90])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([691])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.6154021620750427\n",
      "Epoch 2, Loss: 0.370647390683492\n",
      "Epoch 3, Loss: 0.18033655236164728\n",
      "Epoch 4, Loss: 0.026207703010489542\n",
      "Epoch 5, Loss: 0.009103977509463826\n",
      "Epoch 6, Loss: 0.004873743746429682\n",
      "all_preds shape: (691,)\n",
      "all_labels shape: (691,)\n",
      "all_probs shape: (691, 2)\n",
      "train_Без прилагательных первый-второй жанр.csv\n",
      "test_Без прилагательных первый-второй жанр.csv\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([846])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([193])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.24294998086299058\n",
      "Epoch 2, Loss: 0.03903866842113159\n",
      "Epoch 3, Loss: 0.032905594997659875\n",
      "Epoch 4, Loss: 0.012541652674338332\n",
      "all_preds shape: (193,)\n",
      "all_labels shape: (193,)\n",
      "all_probs shape: (193, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 0]), Shape: torch.Size([846])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([171])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.17148504871875048\n",
      "Epoch 2, Loss: 0.026231773404611483\n",
      "Epoch 3, Loss: 0.01709781778131232\n",
      "Epoch 4, Loss: 0.022825642221572774\n",
      "all_preds shape: (171,)\n",
      "all_labels shape: (171,)\n",
      "all_probs shape: (171, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 0]), Shape: torch.Size([846])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([183])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.2250939796299294\n",
      "Epoch 2, Loss: 0.10185628818968932\n",
      "Epoch 3, Loss: 0.015954705645088798\n",
      "Epoch 4, Loss: 0.005160002412998842\n",
      "all_preds shape: (183,)\n",
      "all_labels shape: (183,)\n",
      "all_probs shape: (183, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([846])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([178])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.2783095599324615\n",
      "Epoch 2, Loss: 0.051744082338970016\n",
      "Epoch 3, Loss: 0.03565925359725952\n",
      "Epoch 4, Loss: 0.015763429368639156\n",
      "all_preds shape: (178,)\n",
      "all_labels shape: (178,)\n",
      "all_probs shape: (178, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([846])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([173])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.21524173366250815\n",
      "Epoch 2, Loss: 0.06253347304408197\n",
      "Epoch 3, Loss: 0.010733158594963176\n",
      "Epoch 4, Loss: 0.002225553582387942\n",
      "all_preds shape: (173,)\n",
      "all_labels shape: (173,)\n",
      "all_probs shape: (173, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 1, 0]), Shape: torch.Size([846])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([171])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.21666416277488074\n",
      "Epoch 2, Loss: 0.04091990875356175\n",
      "Epoch 3, Loss: 0.021817838876611657\n",
      "Epoch 4, Loss: 0.00754344596579257\n",
      "all_preds shape: (171,)\n",
      "all_labels shape: (171,)\n",
      "all_probs shape: (171, 2)\n",
      "train_1.Первый жанр исходная выборка.csv\n",
      "test_1.Первый жанр исходная выборка.csv\n",
      "Train labels sample: tensor([1, 1, 1, 0, 0]), Shape: torch.Size([893])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([169])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.3073214979376644\n",
      "Epoch 2, Loss: 0.5934822601931435\n",
      "Epoch 3, Loss: 0.21951200320784534\n",
      "Epoch 4, Loss: 0.1475728307850659\n",
      "Epoch 5, Loss: 0.12700854885458415\n",
      "Epoch 6, Loss: 0.08399969617104423\n",
      "all_preds shape: (169,)\n",
      "all_labels shape: (169,)\n",
      "all_probs shape: (169, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 0]), Shape: torch.Size([893])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([164])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.15176272466279833\n",
      "Epoch 2, Loss: 0.0345873137682377\n",
      "Epoch 3, Loss: 0.01331479442680055\n",
      "Epoch 4, Loss: 0.012672589166738493\n",
      "Epoch 5, Loss: 0.008337831085165297\n",
      "Epoch 6, Loss: 0.0004733420708881957\n",
      "all_preds shape: (164,)\n",
      "all_labels shape: (164,)\n",
      "all_probs shape: (164, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 0]), Shape: torch.Size([893])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([164])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.23126406989259912\n",
      "Epoch 2, Loss: 0.12975996797987527\n",
      "Epoch 3, Loss: 0.06181090538406612\n",
      "Epoch 4, Loss: 0.024540884229021946\n",
      "Epoch 5, Loss: 0.01749526316832219\n",
      "Epoch 6, Loss: 0.01731395289763376\n",
      "all_preds shape: (164,)\n",
      "all_labels shape: (164,)\n",
      "all_probs shape: (164, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 1]), Shape: torch.Size([893])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([169])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.1548456578207801\n",
      "Epoch 2, Loss: 0.015582942865356537\n",
      "Epoch 3, Loss: 0.0035413642773554394\n",
      "Epoch 4, Loss: 0.0004845785874100069\n",
      "Epoch 5, Loss: 0.0003058507653414771\n",
      "Epoch 6, Loss: 0.00023676710458987924\n",
      "all_preds shape: (169,)\n",
      "all_labels shape: (169,)\n",
      "all_probs shape: (169, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 1]), Shape: torch.Size([893])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([158])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.1606927642548856\n",
      "Epoch 2, Loss: 0.05690078451464485\n",
      "Epoch 3, Loss: 0.01818799992906861\n",
      "Epoch 4, Loss: 0.0007769773494926215\n",
      "Epoch 5, Loss: 0.00850463213594464\n",
      "Epoch 6, Loss: 0.04155518441777011\n",
      "all_preds shape: (158,)\n",
      "all_labels shape: (158,)\n",
      "all_probs shape: (158, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 1, 0]), Shape: torch.Size([893])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([160])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.16761704474421485\n",
      "Epoch 2, Loss: 0.08250031840205858\n",
      "Epoch 3, Loss: 0.018439238920109347\n",
      "Epoch 4, Loss: 0.0037846051699099398\n",
      "Epoch 5, Loss: 0.0007284980643557251\n",
      "Epoch 6, Loss: 0.011564402805691185\n",
      "all_preds shape: (160,)\n",
      "all_labels shape: (160,)\n",
      "all_probs shape: (160, 2)\n",
      "train_2.Первый жанр без клауз, включающих наречия.csv\n",
      "test_2.Первый жанр без клауз, включающих наречия.csv\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([880])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([169])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.1617128507488153\n",
      "Epoch 2, Loss: 0.07176950503648682\n",
      "Epoch 3, Loss: 0.09867337920745327\n",
      "Epoch 4, Loss: 0.0732077882794494\n",
      "Epoch 5, Loss: 0.021530589878305115\n",
      "Epoch 6, Loss: 0.01938145179936493\n",
      "all_preds shape: (169,)\n",
      "all_labels shape: (169,)\n",
      "all_probs shape: (169, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([880])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([163])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.13922710290009324\n",
      "Epoch 2, Loss: 0.04131506122648716\n",
      "Epoch 3, Loss: 0.04389876389706677\n",
      "Epoch 4, Loss: 0.01628146155060015\n",
      "Epoch 5, Loss: 0.008790113496467132\n",
      "Epoch 6, Loss: 0.008916003154379062\n",
      "all_preds shape: (163,)\n",
      "all_labels shape: (163,)\n",
      "all_probs shape: (163, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([880])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([173])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.19247008507055316\n",
      "Epoch 2, Loss: 0.04639240598560057\n",
      "Epoch 3, Loss: 0.035782056488096715\n",
      "Epoch 4, Loss: 0.019584092942320486\n",
      "Epoch 5, Loss: 0.0014732524488036604\n",
      "Epoch 6, Loss: 0.0005160005879588425\n",
      "all_preds shape: (173,)\n",
      "all_labels shape: (173,)\n",
      "all_probs shape: (173, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 1, 0]), Shape: torch.Size([880])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([168])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.18790923231704668\n",
      "Epoch 2, Loss: 0.0707456738603386\n",
      "Epoch 3, Loss: 0.018094009086913006\n",
      "Epoch 4, Loss: 0.006546822997783734\n",
      "Epoch 5, Loss: 0.009492768392771144\n",
      "Epoch 6, Loss: 0.0005619523623450236\n",
      "all_preds shape: (168,)\n",
      "all_labels shape: (168,)\n",
      "all_probs shape: (168, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([880])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([167])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.1860249351202087\n",
      "Epoch 2, Loss: 0.05818121900189329\n",
      "Epoch 3, Loss: 0.014209911225109615\n",
      "Epoch 4, Loss: 0.06634805293076418\n",
      "Epoch 5, Loss: 0.03146175436717882\n",
      "Epoch 6, Loss: 0.03834862930446186\n",
      "all_preds shape: (167,)\n",
      "all_labels shape: (167,)\n",
      "all_probs shape: (167, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 1, 0]), Shape: torch.Size([880])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([180])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.15144473266025835\n",
      "Epoch 2, Loss: 0.03701964301819151\n",
      "Epoch 3, Loss: 0.03424293042837896\n",
      "Epoch 4, Loss: 0.015595668338408525\n",
      "Epoch 5, Loss: 0.02367320272394202\n",
      "Epoch 6, Loss: 0.20798452912237156\n",
      "all_preds shape: (180,)\n",
      "all_labels shape: (180,)\n",
      "all_probs shape: (180, 2)\n",
      "train_3.Первый жанр без клауз, включающих глаголы.csv\n",
      "test_3.Первый жанр без клауз, включающих глаголы.csv\n",
      "Train labels sample: tensor([1, 1, 1, 0, 1]), Shape: torch.Size([824])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([228])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.15482933381393266\n",
      "Epoch 2, Loss: 0.030494093995702524\n",
      "Epoch 3, Loss: 0.021986931784060568\n",
      "Epoch 4, Loss: 0.03580538233374962\n",
      "Epoch 5, Loss: 0.03768658604978835\n",
      "Epoch 6, Loss: 0.021503123179382574\n",
      "all_preds shape: (228,)\n",
      "all_labels shape: (228,)\n",
      "all_probs shape: (228, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 1, 0]), Shape: torch.Size([824])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([219])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.15801455598109618\n",
      "Epoch 2, Loss: 0.07832318445434794\n",
      "Epoch 3, Loss: 0.012051317760219367\n",
      "Epoch 4, Loss: 0.0008875310371737354\n",
      "Epoch 5, Loss: 0.0006095570911733935\n",
      "Epoch 6, Loss: 0.00040454158554964053\n",
      "all_preds shape: (219,)\n",
      "all_labels shape: (219,)\n",
      "all_probs shape: (219, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([824])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([211])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.23898977941224495\n",
      "Epoch 2, Loss: 0.0612464846076014\n",
      "Epoch 3, Loss: 0.044263997804731704\n",
      "Epoch 4, Loss: 0.02628441233313284\n",
      "Epoch 5, Loss: 0.01635755358550411\n",
      "Epoch 6, Loss: 0.016784365284435738\n",
      "all_preds shape: (211,)\n",
      "all_labels shape: (211,)\n",
      "all_probs shape: (211, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 1]), Shape: torch.Size([824])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([220])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.16210289877982667\n",
      "Epoch 2, Loss: 0.02955614554677875\n",
      "Epoch 3, Loss: 0.002327760747553279\n",
      "Epoch 4, Loss: 0.024200392875802487\n",
      "Epoch 5, Loss: 0.0017352713067129326\n",
      "Epoch 6, Loss: 0.0004001318341649424\n",
      "all_preds shape: (220,)\n",
      "all_labels shape: (220,)\n",
      "all_probs shape: (220, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 0]), Shape: torch.Size([824])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([216])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.13603692276116747\n",
      "Epoch 2, Loss: 0.049729369146427\n",
      "Epoch 3, Loss: 0.06364384896005504\n",
      "Epoch 4, Loss: 0.014212746515673084\n",
      "Epoch 5, Loss: 0.001021063930238597\n",
      "Epoch 6, Loss: 0.0005367375074106698\n",
      "all_preds shape: (216,)\n",
      "all_labels shape: (216,)\n",
      "all_probs shape: (216, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 1]), Shape: torch.Size([824])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([220])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.15913837757677987\n",
      "Epoch 2, Loss: 0.03008830337337433\n",
      "Epoch 3, Loss: 0.03404419955376607\n",
      "Epoch 4, Loss: 0.0007833236349352563\n",
      "Epoch 5, Loss: 0.0004394418988690282\n",
      "Epoch 6, Loss: 0.0003072343494680424\n",
      "all_preds shape: (220,)\n",
      "all_labels shape: (220,)\n",
      "all_probs shape: (220, 2)\n",
      "train_4.Первый жанр без клауз, включающих глаголы и наречия.csv\n",
      "test_4.Первый жанр без клауз, включающих глаголы и наречия.csv\n",
      "Train labels sample: tensor([1, 1, 1, 0, 0]), Shape: torch.Size([805])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([239])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.17343758236543805\n",
      "Epoch 2, Loss: 0.04744665220598964\n",
      "Epoch 3, Loss: 0.00875964279368739\n",
      "Epoch 4, Loss: 0.001326867843768103\n",
      "Epoch 5, Loss: 0.0005507200885582351\n",
      "Epoch 6, Loss: 0.000380279088858515\n",
      "all_preds shape: (239,)\n",
      "all_labels shape: (239,)\n",
      "all_probs shape: (239, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 1]), Shape: torch.Size([805])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([240])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.21743326727300882\n",
      "Epoch 2, Loss: 0.04261844773210731\n",
      "Epoch 3, Loss: 0.04384027313733218\n",
      "Epoch 4, Loss: 0.004666437303610877\n",
      "Epoch 5, Loss: 0.003972502393926512\n",
      "Epoch 6, Loss: 0.04294981361896384\n",
      "all_preds shape: (240,)\n",
      "all_labels shape: (240,)\n",
      "all_probs shape: (240, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 0]), Shape: torch.Size([805])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([248])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.15813742187239377\n",
      "Epoch 2, Loss: 0.009311714211898837\n",
      "Epoch 3, Loss: 0.03234267177964177\n",
      "Epoch 4, Loss: 0.010833692221044033\n",
      "Epoch 5, Loss: 0.0006671181381177888\n",
      "Epoch 6, Loss: 0.00044210238825055024\n",
      "all_preds shape: (248,)\n",
      "all_labels shape: (248,)\n",
      "all_probs shape: (248, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 1, 0]), Shape: torch.Size([805])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([233])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.1452030834796674\n",
      "Epoch 2, Loss: 0.07199515298228054\n",
      "Epoch 3, Loss: 0.060358907867624774\n",
      "Epoch 4, Loss: 0.027422043344225076\n",
      "Epoch 5, Loss: 0.01114064926514402\n",
      "Epoch 6, Loss: 0.0016141419319490738\n",
      "all_preds shape: (233,)\n",
      "all_labels shape: (233,)\n",
      "all_probs shape: (233, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 1]), Shape: torch.Size([805])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([233])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.14003567732687966\n",
      "Epoch 2, Loss: 0.021151142363783484\n",
      "Epoch 3, Loss: 0.04628431273386905\n",
      "Epoch 4, Loss: 0.05252224563474895\n",
      "Epoch 5, Loss: 0.05667916007851269\n",
      "Epoch 6, Loss: 0.022589501844919927\n",
      "all_preds shape: (233,)\n",
      "all_labels shape: (233,)\n",
      "all_probs shape: (233, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([805])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([236])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.20745927036977282\n",
      "Epoch 2, Loss: 0.0985641657995681\n",
      "Epoch 3, Loss: 0.07815652263953406\n",
      "Epoch 4, Loss: 0.08328977074729753\n",
      "Epoch 5, Loss: 0.03503254100716874\n",
      "Epoch 6, Loss: 0.02145660285582291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_preds shape: (236,)\n",
      "all_labels shape: (236,)\n",
      "all_probs shape: (236, 2)\n",
      "train_5.без клауз, включающих местоимения.txt.csv\n",
      "test_5.без клауз, включающих местоимения.txt.csv\n",
      "Train labels sample: tensor([0, 1, 0, 0, 1]), Shape: torch.Size([858])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([188])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.1750069074763882\n",
      "Epoch 2, Loss: 0.05541516790011277\n",
      "Epoch 3, Loss: 0.01178642002546608\n",
      "Epoch 4, Loss: 0.18735519561192227\n",
      "Epoch 5, Loss: 0.10767391678894421\n",
      "Epoch 6, Loss: 0.040604254179116755\n",
      "all_preds shape: (188,)\n",
      "all_labels shape: (188,)\n",
      "all_probs shape: (188, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 1, 1]), Shape: torch.Size([858])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([181])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.11988334700085775\n",
      "Epoch 2, Loss: 0.009769075058607591\n",
      "Epoch 3, Loss: 0.0008888792621696161\n",
      "Epoch 4, Loss: 0.0004812502539371727\n",
      "Epoch 5, Loss: 0.000323594451064882\n",
      "Epoch 6, Loss: 0.0002415374390828354\n",
      "all_preds shape: (181,)\n",
      "all_labels shape: (181,)\n",
      "all_probs shape: (181, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 1, 0]), Shape: torch.Size([858])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([189])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.1353627834641547\n",
      "Epoch 2, Loss: 0.07238711600398852\n",
      "Epoch 3, Loss: 0.033137457772951434\n",
      "Epoch 4, Loss: 0.04060128813230053\n",
      "Epoch 5, Loss: 0.025246104975748394\n",
      "Epoch 6, Loss: 0.014292891379833842\n",
      "all_preds shape: (189,)\n",
      "all_labels shape: (189,)\n",
      "all_probs shape: (189, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([858])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([178])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.17978512516452205\n",
      "Epoch 2, Loss: 0.0445315900163863\n",
      "Epoch 3, Loss: 0.21673101999934902\n",
      "Epoch 4, Loss: 0.21644221213473766\n",
      "Epoch 5, Loss: 0.06981138951852228\n",
      "Epoch 6, Loss: 0.034937137177352\n",
      "all_preds shape: (178,)\n",
      "all_labels shape: (178,)\n",
      "all_probs shape: (178, 2)\n",
      "Train labels sample: tensor([1, 1, 1, 1, 1]), Shape: torch.Size([858])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([170])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.17119294989647138\n",
      "Epoch 2, Loss: 0.03842616271813987\n",
      "Epoch 3, Loss: 0.014595403985974068\n",
      "Epoch 4, Loss: 0.00071218617409209\n",
      "Epoch 5, Loss: 0.00040704990560361357\n",
      "Epoch 6, Loss: 0.0003109950528057568\n",
      "all_preds shape: (170,)\n",
      "all_labels shape: (170,)\n",
      "all_probs shape: (170, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([858])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([187])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.23299718596455124\n",
      "Epoch 2, Loss: 0.04515105745048021\n",
      "Epoch 3, Loss: 0.015718168859301065\n",
      "Epoch 4, Loss: 0.05804450307213874\n",
      "Epoch 5, Loss: 0.03278919847906326\n",
      "Epoch 6, Loss: 0.0024413605311161112\n",
      "all_preds shape: (187,)\n",
      "all_labels shape: (187,)\n",
      "all_probs shape: (187, 2)\n",
      "train_6.без слов функциональных.txt.csv\n",
      "test_6.без слов функциональных.txt.csv\n",
      "Train labels sample: tensor([0, 1, 1, 1, 1]), Shape: torch.Size([922])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([182])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.365785988497323\n",
      "Epoch 2, Loss: 0.08771434495354007\n",
      "Epoch 3, Loss: 0.07297991158777913\n",
      "Epoch 4, Loss: 0.15800068077856097\n",
      "Epoch 5, Loss: 0.020839903856916673\n",
      "Epoch 6, Loss: 0.0113790324921238\n",
      "all_preds shape: (182,)\n",
      "all_labels shape: (182,)\n",
      "all_probs shape: (182, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 1, 0]), Shape: torch.Size([922])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([177])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.21104622932536335\n",
      "Epoch 2, Loss: 0.038094203080179107\n",
      "Epoch 3, Loss: 0.039598415505365823\n",
      "Epoch 4, Loss: 0.030740612393229044\n",
      "Epoch 5, Loss: 0.05882266328613884\n",
      "Epoch 6, Loss: 0.06123943438006822\n",
      "all_preds shape: (177,)\n",
      "all_labels shape: (177,)\n",
      "all_probs shape: (177, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([922])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([183])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.21753289295231987\n",
      "Epoch 2, Loss: 0.06590834987381923\n",
      "Epoch 3, Loss: 0.03664189667826326\n",
      "Epoch 4, Loss: 0.03154172831423709\n",
      "Epoch 5, Loss: 0.009643317571568206\n",
      "Epoch 6, Loss: 0.008361106076085105\n",
      "all_preds shape: (183,)\n",
      "all_labels shape: (183,)\n",
      "all_probs shape: (183, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 0]), Shape: torch.Size([922])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([179])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.1621190373352247\n",
      "Epoch 2, Loss: 0.020272707143108006\n",
      "Epoch 3, Loss: 0.004546118734589103\n",
      "Epoch 4, Loss: 0.03584577783075129\n",
      "Epoch 5, Loss: 0.03871092017426894\n",
      "Epoch 6, Loss: 0.010968129848057789\n",
      "all_preds shape: (179,)\n",
      "all_labels shape: (179,)\n",
      "all_probs shape: (179, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 1]), Shape: torch.Size([922])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([177])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.15850864671138598\n",
      "Epoch 2, Loss: 0.04055458298836161\n",
      "Epoch 3, Loss: 0.15429676171584888\n",
      "Epoch 4, Loss: 0.07037381423990531\n",
      "Epoch 5, Loss: 0.029770001822255615\n",
      "Epoch 6, Loss: 0.05410456757767704\n",
      "all_preds shape: (177,)\n",
      "all_labels shape: (177,)\n",
      "all_probs shape: (177, 2)\n",
      "Train labels sample: tensor([1, 1, 1, 0, 1]), Shape: torch.Size([922])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([179])\n",
      "##########################bert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.14852033122763808\n",
      "Epoch 2, Loss: 0.04846147426548574\n",
      "Epoch 3, Loss: 0.03048741246838572\n",
      "Epoch 4, Loss: 0.023771863396616716\n",
      "Epoch 5, Loss: 0.026865955989141467\n",
      "Epoch 6, Loss: 0.03036723707517577\n",
      "all_preds shape: (179,)\n",
      "all_labels shape: (179,)\n",
      "all_probs shape: (179, 2)\n",
      "train_Изъяты лексемы с частотой выше 100.csv\n",
      "test_Изъяты лексемы с частотой выше 100.csv\n",
      "Train labels sample: tensor([1, 1, 1, 0, 0]), Shape: torch.Size([871])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([190])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.729622561823238\n",
      "Epoch 2, Loss: 0.6686308102174239\n",
      "Epoch 3, Loss: 0.6755114176056601\n",
      "Epoch 4, Loss: 0.6622848364439877\n",
      "Epoch 5, Loss: 0.6673853408206593\n",
      "Epoch 6, Loss: 0.6626257956027984\n",
      "all_preds shape: (190,)\n",
      "all_labels shape: (190,)\n",
      "all_probs shape: (190, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([871])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([192])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7046768177639354\n",
      "Epoch 2, Loss: 0.6618102079088037\n",
      "Epoch 3, Loss: 0.34973494156517765\n",
      "Epoch 4, Loss: 0.15611509572375903\n",
      "Epoch 5, Loss: 0.04117808152328838\n",
      "Epoch 6, Loss: 0.02006098225458779\n",
      "all_preds shape: (192,)\n",
      "all_labels shape: (192,)\n",
      "all_probs shape: (192, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 1]), Shape: torch.Size([871])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([190])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7015966361219232\n",
      "Epoch 2, Loss: 0.666880505735224\n",
      "Epoch 3, Loss: 0.406616441228173\n",
      "Epoch 4, Loss: 0.32201424010775304\n",
      "Epoch 5, Loss: 0.325390865247358\n",
      "Epoch 6, Loss: 0.3243838589299809\n",
      "all_preds shape: (190,)\n",
      "all_labels shape: (190,)\n",
      "all_probs shape: (190, 2)\n",
      "Train labels sample: tensor([1, 1, 1, 0, 1]), Shape: torch.Size([871])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([200])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6723123501647602\n",
      "Epoch 2, Loss: 0.4096256012266332\n",
      "Epoch 3, Loss: 0.32728001783517274\n",
      "Epoch 4, Loss: 0.333513489771973\n",
      "Epoch 5, Loss: 0.21413132758303122\n",
      "Epoch 6, Loss: 0.1141439978371967\n",
      "all_preds shape: (200,)\n",
      "all_labels shape: (200,)\n",
      "all_probs shape: (200, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([871])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([189])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7269970259883187\n",
      "Epoch 2, Loss: 0.6824198083444075\n",
      "Epoch 3, Loss: 0.6893875945698131\n",
      "Epoch 4, Loss: 0.6749735442074862\n",
      "Epoch 5, Loss: 0.6486195672642101\n",
      "Epoch 6, Loss: 0.6732521653175354\n",
      "all_preds shape: (189,)\n",
      "all_labels shape: (189,)\n",
      "all_probs shape: (189, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([871])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([191])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7631883718750694\n",
      "Epoch 2, Loss: 0.6915615466507998\n",
      "Epoch 3, Loss: 0.6797454151240262\n",
      "Epoch 4, Loss: 0.6822933858091181\n",
      "Epoch 5, Loss: 0.6750554203987121\n",
      "Epoch 6, Loss: 0.680111808126623\n",
      "all_preds shape: (191,)\n",
      "all_labels shape: (191,)\n",
      "all_probs shape: (191, 2)\n",
      "train_Изъяты лексемы с частотой выше 49.csv\n",
      "test_Изъяты лексемы с частотой выше 49.csv\n",
      "Train labels sample: tensor([0, 1, 1, 0, 1]), Shape: torch.Size([900])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([176])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.5981531426833387\n",
      "Epoch 2, Loss: 0.34805112358248025\n",
      "Epoch 3, Loss: 0.34346040338277817\n",
      "Epoch 4, Loss: 0.34202814357061134\n",
      "Epoch 5, Loss: 0.336208506372937\n",
      "Epoch 6, Loss: 0.40687699962341994\n",
      "all_preds shape: (176,)\n",
      "all_labels shape: (176,)\n",
      "all_probs shape: (176, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 1]), Shape: torch.Size([900])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([180])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.5735398452532919\n",
      "Epoch 2, Loss: 0.24072785254796608\n",
      "Epoch 3, Loss: 0.09308033484736816\n",
      "Epoch 4, Loss: 0.03191701543880953\n",
      "Epoch 5, Loss: 0.01858687612203587\n",
      "Epoch 6, Loss: 0.19081721241022215\n",
      "all_preds shape: (180,)\n",
      "all_labels shape: (180,)\n",
      "all_probs shape: (180, 2)\n",
      "Train labels sample: tensor([1, 1, 1, 0, 0]), Shape: torch.Size([900])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([184])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.695277461357284\n",
      "Epoch 2, Loss: 0.45654517314151716\n",
      "Epoch 3, Loss: 0.30448264367224875\n",
      "Epoch 4, Loss: 0.20146021221584656\n",
      "Epoch 5, Loss: 0.10525287156761215\n",
      "Epoch 6, Loss: 0.03925133880897703\n",
      "all_preds shape: (184,)\n",
      "all_labels shape: (184,)\n",
      "all_probs shape: (184, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 0]), Shape: torch.Size([900])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([191])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7030574031043471\n",
      "Epoch 2, Loss: 0.6661147830779093\n",
      "Epoch 3, Loss: 0.6542088948843772\n",
      "Epoch 4, Loss: 0.29511218292540625\n",
      "Epoch 5, Loss: 0.10757423130174477\n",
      "Epoch 6, Loss: 0.03441800662365399\n",
      "all_preds shape: (191,)\n",
      "all_labels shape: (191,)\n",
      "all_probs shape: (191, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([900])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([200])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7091384132703146\n",
      "Epoch 2, Loss: 0.6646885767317655\n",
      "Epoch 3, Loss: 0.6588028758241419\n",
      "Epoch 4, Loss: 0.4646349310090667\n",
      "Epoch 5, Loss: 0.3345447135599036\n",
      "Epoch 6, Loss: 0.3309215385662882\n",
      "all_preds shape: (200,)\n",
      "all_labels shape: (200,)\n",
      "all_probs shape: (200, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 0]), Shape: torch.Size([900])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([196])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.715622610690301\n",
      "Epoch 2, Loss: 0.6695296989198316\n",
      "Epoch 3, Loss: 0.6544739834049291\n",
      "Epoch 4, Loss: 0.3589198495212354\n",
      "Epoch 5, Loss: 0.3172724345012715\n",
      "Epoch 6, Loss: 0.3233611837290881\n",
      "all_preds shape: (196,)\n",
      "all_labels shape: (196,)\n",
      "all_probs shape: (196, 2)\n",
      "train_Изъяты лексемы с частотой выше 29.csv\n",
      "test_Изъяты лексемы с частотой выше 29.csv\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([887])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([197])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.6985177232750824\n",
      "Epoch 2, Loss: 0.6550753978746278\n",
      "Epoch 3, Loss: 0.6517883273107665\n",
      "Epoch 4, Loss: 0.6447107616279807\n",
      "Epoch 5, Loss: 0.6381189333541053\n",
      "Epoch 6, Loss: 0.399228470799114\n",
      "all_preds shape: (197,)\n",
      "all_labels shape: (197,)\n",
      "all_probs shape: (197, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([887])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([191])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.6728892911757741\n",
      "Epoch 2, Loss: 0.4272443545050919\n",
      "Epoch 3, Loss: 0.3438245650114758\n",
      "Epoch 4, Loss: 0.2991430121473968\n",
      "Epoch 5, Loss: 0.2898071679353182\n",
      "Epoch 6, Loss: 0.15899110206269793\n",
      "all_preds shape: (191,)\n",
      "all_labels shape: (191,)\n",
      "all_probs shape: (191, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([887])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([201])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.6926782567586217\n",
      "Epoch 2, Loss: 0.6697899677923748\n",
      "Epoch 3, Loss: 0.6451694252235549\n",
      "Epoch 4, Loss: 0.6533401347696781\n",
      "Epoch 5, Loss: 0.6561260180813926\n",
      "Epoch 6, Loss: 0.6558863541909626\n",
      "all_preds shape: (201,)\n",
      "all_labels shape: (201,)\n",
      "all_probs shape: (201, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([887])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([189])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.6902099943586758\n",
      "Epoch 2, Loss: 0.5942383044000182\n",
      "Epoch 3, Loss: 0.30715722407746526\n",
      "Epoch 4, Loss: 0.10231953880949211\n",
      "Epoch 5, Loss: 0.09783789625258318\n",
      "Epoch 6, Loss: 0.11937657147687528\n",
      "all_preds shape: (189,)\n",
      "all_labels shape: (189,)\n",
      "all_probs shape: (189, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([887])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([204])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.6991127362208707\n",
      "Epoch 2, Loss: 0.6529152052743095\n",
      "Epoch 3, Loss: 0.5051282864463117\n",
      "Epoch 4, Loss: 0.3633528577962092\n",
      "Epoch 5, Loss: 0.35633784865162205\n",
      "Epoch 6, Loss: 0.3535230243578553\n",
      "all_preds shape: (204,)\n",
      "all_labels shape: (204,)\n",
      "all_probs shape: (204, 2)\n",
      "Train labels sample: tensor([1, 1, 1, 0, 0]), Shape: torch.Size([887])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([188])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7093847974070481\n",
      "Epoch 2, Loss: 0.6603815768446241\n",
      "Epoch 3, Loss: 0.5338896192344171\n",
      "Epoch 4, Loss: 0.20209133536887489\n",
      "Epoch 5, Loss: 0.23515801213216037\n",
      "Epoch 6, Loss: 0.04231784030395959\n",
      "all_preds shape: (188,)\n",
      "all_labels shape: (188,)\n",
      "all_probs shape: (188, 2)\n",
      "train_Изъяты лексемы с частотой выше 9.csv\n",
      "test_Изъяты лексемы с частотой выше 9.csv\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([766])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([350])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.729845034579436\n",
      "Epoch 2, Loss: 0.6551108074684938\n",
      "Epoch 3, Loss: 0.6001077902813753\n",
      "Epoch 4, Loss: 0.45012515286604565\n",
      "all_preds shape: (350,)\n",
      "all_labels shape: (350,)\n",
      "all_probs shape: (350, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 1, 1]), Shape: torch.Size([766])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([343])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7100621635715166\n",
      "Epoch 2, Loss: 0.6293318942189217\n",
      "Epoch 3, Loss: 0.46950412541627884\n",
      "Epoch 4, Loss: 0.2975379464526971\n",
      "all_preds shape: (343,)\n",
      "all_labels shape: (343,)\n",
      "all_probs shape: (343, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([766])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([259])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7220042074720064\n",
      "Epoch 2, Loss: 0.643610509733359\n",
      "Epoch 3, Loss: 0.5970613285899162\n",
      "Epoch 4, Loss: 0.42103638127446175\n",
      "all_preds shape: (259,)\n",
      "all_labels shape: (259,)\n",
      "all_probs shape: (259, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 1]), Shape: torch.Size([766])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([274])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7102878242731094\n",
      "Epoch 2, Loss: 0.6280208801229795\n",
      "Epoch 3, Loss: 0.47549023665487766\n",
      "Epoch 4, Loss: 0.27116478731234867\n",
      "all_preds shape: (274,)\n",
      "all_labels shape: (274,)\n",
      "all_probs shape: (274, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([766])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([261])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7429273029168447\n",
      "Epoch 2, Loss: 0.660905584692955\n",
      "Epoch 3, Loss: 0.6390845899780592\n",
      "Epoch 4, Loss: 0.48486661290129024\n",
      "all_preds shape: (261,)\n",
      "all_labels shape: (261,)\n",
      "all_probs shape: (261, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([766])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([264])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.6830293710033098\n",
      "Epoch 2, Loss: 0.6673017417391142\n",
      "Epoch 3, Loss: 0.5032503586262465\n",
      "Epoch 4, Loss: 0.32134989586969215\n",
      "all_preds shape: (264,)\n",
      "all_labels shape: (264,)\n",
      "all_probs shape: (264, 2)\n",
      "train_Изъяты лексемы с частотой выше 5.csv\n",
      "test_Изъяты лексемы с частотой выше 5.csv\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([96])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([528])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7382636467615763\n",
      "Epoch 2, Loss: 0.5695508668820063\n",
      "Epoch 3, Loss: 0.5791652103265127\n",
      "Epoch 4, Loss: 0.5681139181057612\n",
      "Epoch 5, Loss: 0.5697246839602789\n",
      "Epoch 6, Loss: 0.5624234726031622\n",
      "all_preds shape: (528,)\n",
      "all_labels shape: (528,)\n",
      "all_probs shape: (528, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 1, 1]), Shape: torch.Size([96])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([507])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.8305370310942332\n",
      "Epoch 2, Loss: 0.514919102191925\n",
      "Epoch 3, Loss: 0.5250623921553293\n",
      "Epoch 4, Loss: 0.511014406879743\n",
      "Epoch 5, Loss: 0.513197218378385\n",
      "Epoch 6, Loss: 0.5019500801960627\n",
      "all_preds shape: (507,)\n",
      "all_labels shape: (507,)\n",
      "all_probs shape: (507, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 1]), Shape: torch.Size([96])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([513])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7897018690903982\n",
      "Epoch 2, Loss: 0.6575502852598826\n",
      "Epoch 3, Loss: 0.6228331526120504\n",
      "Epoch 4, Loss: 0.5990743140379587\n",
      "Epoch 5, Loss: 0.6059510956207911\n",
      "Epoch 6, Loss: 0.6036736766497294\n",
      "all_preds shape: (513,)\n",
      "all_labels shape: (513,)\n",
      "all_probs shape: (513, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([96])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([576])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7482175628344218\n",
      "Epoch 2, Loss: 0.644896020491918\n",
      "Epoch 3, Loss: 0.5230484654506048\n",
      "Epoch 4, Loss: 0.5413525452216467\n",
      "Epoch 5, Loss: 0.5444453408320745\n",
      "Epoch 6, Loss: 0.5369353493054708\n",
      "all_preds shape: (576,)\n",
      "all_labels shape: (576,)\n",
      "all_probs shape: (576, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([96])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([491])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.8940147956212362\n",
      "Epoch 2, Loss: 0.6053860684235891\n",
      "Epoch 3, Loss: 0.5899464984734853\n",
      "Epoch 4, Loss: 0.5681643535693487\n",
      "Epoch 5, Loss: 0.5918873846530914\n",
      "Epoch 6, Loss: 0.5889671395222346\n",
      "all_preds shape: (491,)\n",
      "all_labels shape: (491,)\n",
      "all_probs shape: (491, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([96])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([456])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7038560211658478\n",
      "Epoch 2, Loss: 0.6061290005842844\n",
      "Epoch 3, Loss: 0.6278922955195109\n",
      "Epoch 4, Loss: 0.5990217377742132\n",
      "Epoch 5, Loss: 0.6361757516860962\n",
      "Epoch 6, Loss: 0.61148734887441\n",
      "all_preds shape: (456,)\n",
      "all_labels shape: (456,)\n",
      "all_probs shape: (456, 2)\n",
      "train_Изъяты лексемы с частотой выше 3.csv\n",
      "test_Изъяты лексемы с частотой выше 3.csv\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([90])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([700])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.6202165434757868\n",
      "Epoch 2, Loss: 0.5491192291180292\n",
      "Epoch 3, Loss: 0.5460217893123627\n",
      "Epoch 4, Loss: 0.5261760900417963\n",
      "Epoch 5, Loss: 0.5102701783180237\n",
      "Epoch 6, Loss: 0.4986765881379445\n",
      "all_preds shape: (700,)\n",
      "all_labels shape: (700,)\n",
      "all_probs shape: (700, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([90])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([732])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.5780592262744904\n",
      "Epoch 2, Loss: 0.5184316659967104\n",
      "Epoch 3, Loss: 0.5102415084838867\n",
      "Epoch 4, Loss: 0.49205703536669415\n",
      "Epoch 5, Loss: 0.49457646409670514\n",
      "Epoch 6, Loss: 0.5026959180831909\n",
      "all_preds shape: (732,)\n",
      "all_labels shape: (732,)\n",
      "all_probs shape: (732, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([90])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([739])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.9129553486903509\n",
      "Epoch 2, Loss: 0.6399193108081818\n",
      "Epoch 3, Loss: 0.6088771323362986\n",
      "Epoch 4, Loss: 0.6197243829568228\n",
      "Epoch 5, Loss: 0.6388355841239294\n",
      "Epoch 6, Loss: 0.5998087227344513\n",
      "all_preds shape: (739,)\n",
      "all_labels shape: (739,)\n",
      "all_probs shape: (739, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([90])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([720])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7885506550470988\n",
      "Epoch 2, Loss: 0.5133297095696131\n",
      "Epoch 3, Loss: 0.5154156237840652\n",
      "Epoch 4, Loss: 0.4857487678527832\n",
      "Epoch 5, Loss: 0.4959569275379181\n",
      "Epoch 6, Loss: 0.4910698930422465\n",
      "all_preds shape: (720,)\n",
      "all_labels shape: (720,)\n",
      "all_probs shape: (720, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([90])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([715])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.8708348572254181\n",
      "Epoch 2, Loss: 0.7247699399789175\n",
      "Epoch 3, Loss: 0.5734944393237432\n",
      "Epoch 4, Loss: 0.5811866174141566\n",
      "Epoch 5, Loss: 0.5853048861026764\n",
      "Epoch 6, Loss: 0.5938606411218643\n",
      "all_preds shape: (715,)\n",
      "all_labels shape: (715,)\n",
      "all_probs shape: (715, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([90])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([691])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.777759869893392\n",
      "Epoch 2, Loss: 0.618027483423551\n",
      "Epoch 3, Loss: 0.5976639886697134\n",
      "Epoch 4, Loss: 0.5595293343067169\n",
      "Epoch 5, Loss: 0.5469238609075546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.5866635491450628\n",
      "all_preds shape: (691,)\n",
      "all_labels shape: (691,)\n",
      "all_probs shape: (691, 2)\n",
      "train_Без прилагательных первый-второй жанр.csv\n",
      "test_Без прилагательных первый-второй жанр.csv\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([846])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([193])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.5862083247414341\n",
      "Epoch 2, Loss: 0.45882045725981396\n",
      "Epoch 3, Loss: 0.25101166439277156\n",
      "Epoch 4, Loss: 0.10917950832043533\n",
      "all_preds shape: (193,)\n",
      "all_labels shape: (193,)\n",
      "all_probs shape: (193, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 0]), Shape: torch.Size([846])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([171])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7211712775407014\n",
      "Epoch 2, Loss: 0.6200851886360733\n",
      "Epoch 3, Loss: 0.38848407842494825\n",
      "Epoch 4, Loss: 0.16035370135472882\n",
      "all_preds shape: (171,)\n",
      "all_labels shape: (171,)\n",
      "all_probs shape: (171, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 0]), Shape: torch.Size([846])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([183])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.6884810769999469\n",
      "Epoch 2, Loss: 0.4814067814085219\n",
      "Epoch 3, Loss: 0.18891641442422513\n",
      "Epoch 4, Loss: 0.19181888622956145\n",
      "all_preds shape: (183,)\n",
      "all_labels shape: (183,)\n",
      "all_probs shape: (183, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([846])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([178])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7072283934663843\n",
      "Epoch 2, Loss: 0.5965509348445468\n",
      "Epoch 3, Loss: 0.3981357583845103\n",
      "Epoch 4, Loss: 0.1538233645260334\n",
      "all_preds shape: (178,)\n",
      "all_labels shape: (178,)\n",
      "all_probs shape: (178, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([846])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([173])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7214879459804959\n",
      "Epoch 2, Loss: 0.6515882611274719\n",
      "Epoch 3, Loss: 0.4533200263977051\n",
      "Epoch 4, Loss: 0.2212675289699325\n",
      "all_preds shape: (173,)\n",
      "all_labels shape: (173,)\n",
      "all_probs shape: (173, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 1, 0]), Shape: torch.Size([846])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([171])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7580006453726027\n",
      "Epoch 2, Loss: 0.658798822650203\n",
      "Epoch 3, Loss: 0.5075075234527942\n",
      "Epoch 4, Loss: 0.22482107703884444\n",
      "all_preds shape: (171,)\n",
      "all_labels shape: (171,)\n",
      "all_probs shape: (171, 2)\n",
      "train_1.Первый жанр исходная выборка.csv\n",
      "test_1.Первый жанр исходная выборка.csv\n",
      "Train labels sample: tensor([1, 1, 1, 0, 0]), Shape: torch.Size([893])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([169])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7107041552662849\n",
      "Epoch 2, Loss: 0.67388652158635\n",
      "Epoch 3, Loss: 0.6802293839199203\n",
      "Epoch 4, Loss: 0.6755037185336862\n",
      "Epoch 5, Loss: 0.6738628585423742\n",
      "Epoch 6, Loss: 0.6731625571846962\n",
      "all_preds shape: (169,)\n",
      "all_labels shape: (169,)\n",
      "all_probs shape: (169, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 0]), Shape: torch.Size([893])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([164])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7040577488286155\n",
      "Epoch 2, Loss: 0.5399906194901892\n",
      "Epoch 3, Loss: 0.2441250885770257\n",
      "Epoch 4, Loss: 0.10108837351851564\n",
      "Epoch 5, Loss: 0.1438600334388736\n",
      "Epoch 6, Loss: 0.03539165999557424\n",
      "all_preds shape: (164,)\n",
      "all_labels shape: (164,)\n",
      "all_probs shape: (164, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 0]), Shape: torch.Size([893])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([164])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7092089535934585\n",
      "Epoch 2, Loss: 0.7066407459122794\n",
      "Epoch 3, Loss: 0.6782410064978259\n",
      "Epoch 4, Loss: 0.35916243724724545\n",
      "Epoch 5, Loss: 0.08972564575794552\n",
      "Epoch 6, Loss: 0.03128824887763975\n",
      "all_preds shape: (164,)\n",
      "all_labels shape: (164,)\n",
      "all_probs shape: (164, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 1]), Shape: torch.Size([893])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([169])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.6987436850156102\n",
      "Epoch 2, Loss: 0.4622468435471611\n",
      "Epoch 3, Loss: 0.16110339953697153\n",
      "Epoch 4, Loss: 0.04168244227600683\n",
      "Epoch 5, Loss: 0.028602668641334667\n",
      "Epoch 6, Loss: 0.03726336047943083\n",
      "all_preds shape: (169,)\n",
      "all_labels shape: (169,)\n",
      "all_probs shape: (169, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 1]), Shape: torch.Size([893])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([158])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7139675553355899\n",
      "Epoch 2, Loss: 0.6788790619799069\n",
      "Epoch 3, Loss: 0.6741093248128891\n",
      "Epoch 4, Loss: 0.67469843264137\n",
      "Epoch 5, Loss: 0.6765354818531445\n",
      "Epoch 6, Loss: 0.538341214348163\n",
      "all_preds shape: (158,)\n",
      "all_labels shape: (158,)\n",
      "all_probs shape: (158, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 1, 0]), Shape: torch.Size([893])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([160])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7036709609840598\n",
      "Epoch 2, Loss: 0.6822989583015442\n",
      "Epoch 3, Loss: 0.6415030546486378\n",
      "Epoch 4, Loss: 0.6743616722524166\n",
      "Epoch 5, Loss: 0.6782301153455462\n",
      "Epoch 6, Loss: 0.6885575151869229\n",
      "all_preds shape: (160,)\n",
      "all_labels shape: (160,)\n",
      "all_probs shape: (160, 2)\n",
      "train_2.Первый жанр без клауз, включающих наречия.csv\n",
      "test_2.Первый жанр без клауз, включающих наречия.csv\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([880])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([169])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.684270771525123\n",
      "Epoch 2, Loss: 0.3397727183150974\n",
      "Epoch 3, Loss: 0.15652223723855885\n",
      "Epoch 4, Loss: 0.06696181022985415\n",
      "Epoch 5, Loss: 0.23362354002469643\n",
      "Epoch 6, Loss: 0.0982177408666096\n",
      "all_preds shape: (169,)\n",
      "all_labels shape: (169,)\n",
      "all_probs shape: (169, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([880])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([163])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7283178269863129\n",
      "Epoch 2, Loss: 0.6897460157221014\n",
      "Epoch 3, Loss: 0.6784571051597595\n",
      "Epoch 4, Loss: 0.6714775356379422\n",
      "Epoch 5, Loss: 0.6010569642890583\n",
      "Epoch 6, Loss: 0.3325317264280536\n",
      "all_preds shape: (163,)\n",
      "all_labels shape: (163,)\n",
      "all_probs shape: (163, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([880])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([173])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7492994487285614\n",
      "Epoch 2, Loss: 0.6779981017112732\n",
      "Epoch 3, Loss: 0.6752699868245559\n",
      "Epoch 4, Loss: 0.6871770576997237\n",
      "Epoch 5, Loss: 0.6799709146673029\n",
      "Epoch 6, Loss: 0.6656379634683782\n",
      "all_preds shape: (173,)\n",
      "all_labels shape: (173,)\n",
      "all_probs shape: (173, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 1, 0]), Shape: torch.Size([880])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([168])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7050909432497892\n",
      "Epoch 2, Loss: 0.6217088038271124\n",
      "Epoch 3, Loss: 0.20557273313911123\n",
      "Epoch 4, Loss: 0.3551109063693068\n",
      "Epoch 5, Loss: 0.34289381255141715\n",
      "Epoch 6, Loss: 0.20562920390882275\n",
      "all_preds shape: (168,)\n",
      "all_labels shape: (168,)\n",
      "all_probs shape: (168, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([880])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([167])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7296995813196355\n",
      "Epoch 2, Loss: 0.6752563763748516\n",
      "Epoch 3, Loss: 0.6398378187959844\n",
      "Epoch 4, Loss: 0.3624812447211959\n",
      "Epoch 5, Loss: 0.28863015760752286\n",
      "Epoch 6, Loss: 0.27459684508767995\n",
      "all_preds shape: (167,)\n",
      "all_labels shape: (167,)\n",
      "all_probs shape: (167, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 1, 0]), Shape: torch.Size([880])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([180])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7167043512517756\n",
      "Epoch 2, Loss: 0.676545588536696\n",
      "Epoch 3, Loss: 0.6687205119566484\n",
      "Epoch 4, Loss: 0.49497840038754726\n",
      "Epoch 5, Loss: 0.18962646163153377\n",
      "Epoch 6, Loss: 0.07384139918298883\n",
      "all_preds shape: (180,)\n",
      "all_labels shape: (180,)\n",
      "all_probs shape: (180, 2)\n",
      "train_3.Первый жанр без клауз, включающих глаголы.csv\n",
      "test_3.Первый жанр без клауз, включающих глаголы.csv\n",
      "Train labels sample: tensor([1, 1, 1, 0, 1]), Shape: torch.Size([824])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([228])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7157236761771716\n",
      "Epoch 2, Loss: 0.7024809362796637\n",
      "Epoch 3, Loss: 0.5207450621976302\n",
      "Epoch 4, Loss: 0.2659074462806949\n",
      "Epoch 5, Loss: 0.14097725121358123\n",
      "Epoch 6, Loss: 0.11032372864428908\n",
      "all_preds shape: (228,)\n",
      "all_labels shape: (228,)\n",
      "all_probs shape: (228, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 1, 0]), Shape: torch.Size([824])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([219])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7473685867511309\n",
      "Epoch 2, Loss: 0.6375040566691985\n",
      "Epoch 3, Loss: 0.2106288614539573\n",
      "Epoch 4, Loss: 0.10066639800341083\n",
      "Epoch 5, Loss: 0.1430518543616367\n",
      "Epoch 6, Loss: 0.041875034149807804\n",
      "all_preds shape: (219,)\n",
      "all_labels shape: (219,)\n",
      "all_probs shape: (219, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([824])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([211])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7009546814056543\n",
      "Epoch 2, Loss: 0.404098091664939\n",
      "Epoch 3, Loss: 0.12665814645319748\n",
      "Epoch 4, Loss: 0.08252288303964843\n",
      "Epoch 5, Loss: 0.036708128052011416\n",
      "Epoch 6, Loss: 0.07650212596994467\n",
      "all_preds shape: (211,)\n",
      "all_labels shape: (211,)\n",
      "all_probs shape: (211, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 1]), Shape: torch.Size([824])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([220])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7330358177423477\n",
      "Epoch 2, Loss: 0.7138217595907358\n",
      "Epoch 3, Loss: 0.6841760724782944\n",
      "Epoch 4, Loss: 0.6799129339364859\n",
      "Epoch 5, Loss: 0.6852771089627192\n",
      "Epoch 6, Loss: 0.688122380238313\n",
      "all_preds shape: (220,)\n",
      "all_labels shape: (220,)\n",
      "all_probs shape: (220, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 0]), Shape: torch.Size([824])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([216])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7511762323287817\n",
      "Epoch 2, Loss: 0.681363202058352\n",
      "Epoch 3, Loss: 0.6916591651164569\n",
      "Epoch 4, Loss: 0.6400938538404611\n",
      "Epoch 5, Loss: 0.5802531295384352\n",
      "Epoch 6, Loss: 0.42029805717846525\n",
      "all_preds shape: (216,)\n",
      "all_labels shape: (216,)\n",
      "all_probs shape: (216, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 1]), Shape: torch.Size([824])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([220])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.6870792106940196\n",
      "Epoch 2, Loss: 0.503268117013459\n",
      "Epoch 3, Loss: 0.3639705271388476\n",
      "Epoch 4, Loss: 0.3555838674879991\n",
      "Epoch 5, Loss: 0.30000823148741174\n",
      "Epoch 6, Loss: 0.17387691140174866\n",
      "all_preds shape: (220,)\n",
      "all_labels shape: (220,)\n",
      "all_probs shape: (220, 2)\n",
      "train_4.Первый жанр без клауз, включающих глаголы и наречия.csv\n",
      "test_4.Первый жанр без клауз, включающих глаголы и наречия.csv\n",
      "Train labels sample: tensor([1, 1, 1, 0, 0]), Shape: torch.Size([805])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([239])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7557025247929143\n",
      "Epoch 2, Loss: 0.6916498705452564\n",
      "Epoch 3, Loss: 0.6770380501653633\n",
      "Epoch 4, Loss: 0.45427103077664094\n",
      "Epoch 5, Loss: 0.20520501883298742\n",
      "Epoch 6, Loss: 0.10337318385512952\n",
      "all_preds shape: (239,)\n",
      "all_labels shape: (239,)\n",
      "all_probs shape: (239, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 1]), Shape: torch.Size([805])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([240])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7841720183690389\n",
      "Epoch 2, Loss: 0.6911065484963211\n",
      "Epoch 3, Loss: 0.6826141406508053\n",
      "Epoch 4, Loss: 0.6984539639716055\n",
      "Epoch 5, Loss: 0.6931619048118591\n",
      "Epoch 6, Loss: 0.642741217917087\n",
      "all_preds shape: (240,)\n",
      "all_labels shape: (240,)\n",
      "all_probs shape: (240, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 0]), Shape: torch.Size([805])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([248])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7321678738968045\n",
      "Epoch 2, Loss: 0.5933718947218913\n",
      "Epoch 3, Loss: 0.22307295061867027\n",
      "Epoch 4, Loss: 0.04661818246777151\n",
      "Epoch 5, Loss: 0.03736768194529064\n",
      "Epoch 6, Loss: 0.048585226702248206\n",
      "all_preds shape: (248,)\n",
      "all_labels shape: (248,)\n",
      "all_probs shape: (248, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 1, 0]), Shape: torch.Size([805])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([233])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.6678666618524813\n",
      "Epoch 2, Loss: 0.26508349340920356\n",
      "Epoch 3, Loss: 0.09383818627718617\n",
      "Epoch 4, Loss: 0.05237706042323992\n",
      "Epoch 5, Loss: 0.06662578053096783\n",
      "Epoch 6, Loss: 0.00997220172533108\n",
      "all_preds shape: (233,)\n",
      "all_labels shape: (233,)\n",
      "all_probs shape: (233, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 1]), Shape: torch.Size([805])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([233])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7062022872999603\n",
      "Epoch 2, Loss: 0.4928026213949802\n",
      "Epoch 3, Loss: 0.2510358378218085\n",
      "Epoch 4, Loss: 0.34555073588283036\n",
      "Epoch 5, Loss: 0.3400118666536668\n",
      "Epoch 6, Loss: 0.22168518754416236\n",
      "all_preds shape: (233,)\n",
      "all_labels shape: (233,)\n",
      "all_probs shape: (233, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([805])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([236])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7422178037026349\n",
      "Epoch 2, Loss: 0.6950977339464075\n",
      "Epoch 3, Loss: 0.6565614795567942\n",
      "Epoch 4, Loss: 0.32747187038116593\n",
      "Epoch 5, Loss: 0.08107325628253759\n",
      "Epoch 6, Loss: 0.08947412555525992\n",
      "all_preds shape: (236,)\n",
      "all_labels shape: (236,)\n",
      "all_probs shape: (236, 2)\n",
      "train_5.без клауз, включающих местоимения.txt.csv\n",
      "test_5.без клауз, включающих местоимения.txt.csv\n",
      "Train labels sample: tensor([0, 1, 0, 0, 1]), Shape: torch.Size([858])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([188])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.6169274383121066\n",
      "Epoch 2, Loss: 0.3528414466590793\n",
      "Epoch 3, Loss: 0.34640225975049865\n",
      "Epoch 4, Loss: 0.3001119049817876\n",
      "Epoch 5, Loss: 0.34165083844628596\n",
      "Epoch 6, Loss: 0.21798824723292556\n",
      "all_preds shape: (188,)\n",
      "all_labels shape: (188,)\n",
      "all_probs shape: (188, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 1, 1]), Shape: torch.Size([858])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([181])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7022889940826981\n",
      "Epoch 2, Loss: 0.46535549050679914\n",
      "Epoch 3, Loss: 0.20541319656358273\n",
      "Epoch 4, Loss: 0.14390564712488818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.03436297543036441\n",
      "Epoch 6, Loss: 0.04887210535040746\n",
      "all_preds shape: (181,)\n",
      "all_labels shape: (181,)\n",
      "all_probs shape: (181, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 1, 0]), Shape: torch.Size([858])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([189])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7292536441926603\n",
      "Epoch 2, Loss: 0.678876519203186\n",
      "Epoch 3, Loss: 0.6840854044313784\n",
      "Epoch 4, Loss: 0.4563403637320907\n",
      "Epoch 5, Loss: 0.3256669605357779\n",
      "Epoch 6, Loss: 0.2858864794899192\n",
      "all_preds shape: (189,)\n",
      "all_labels shape: (189,)\n",
      "all_probs shape: (189, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([858])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([178])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7236017590319669\n",
      "Epoch 2, Loss: 0.6963157891123383\n",
      "Epoch 3, Loss: 0.6602529778524682\n",
      "Epoch 4, Loss: 0.38188945412359854\n",
      "Epoch 5, Loss: 0.4883997909448765\n",
      "Epoch 6, Loss: 0.3645459311427893\n",
      "all_preds shape: (178,)\n",
      "all_labels shape: (178,)\n",
      "all_probs shape: (178, 2)\n",
      "Train labels sample: tensor([1, 1, 1, 1, 1]), Shape: torch.Size([858])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([170])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7133313207714645\n",
      "Epoch 2, Loss: 0.6922580318318473\n",
      "Epoch 3, Loss: 0.6758761626702768\n",
      "Epoch 4, Loss: 0.447917260505535\n",
      "Epoch 5, Loss: 0.27484338565005195\n",
      "Epoch 6, Loss: 0.1668726444175398\n",
      "all_preds shape: (170,)\n",
      "all_labels shape: (170,)\n",
      "all_probs shape: (170, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([858])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([187])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.6908606815117376\n",
      "Epoch 2, Loss: 0.5604435547634408\n",
      "Epoch 3, Loss: 0.2614247998981564\n",
      "Epoch 4, Loss: 0.30847243326543655\n",
      "Epoch 5, Loss: 0.30603902171262437\n",
      "Epoch 6, Loss: 0.3615649908919025\n",
      "all_preds shape: (187,)\n",
      "all_labels shape: (187,)\n",
      "all_probs shape: (187, 2)\n",
      "train_6.без слов функциональных.txt.csv\n",
      "test_6.без слов функциональных.txt.csv\n",
      "Train labels sample: tensor([0, 1, 1, 1, 1]), Shape: torch.Size([922])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([182])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.6834950030877672\n",
      "Epoch 2, Loss: 0.46750756288910733\n",
      "Epoch 3, Loss: 0.1266625419154699\n",
      "Epoch 4, Loss: 0.026246815751274598\n",
      "Epoch 5, Loss: 0.13580772260420731\n",
      "Epoch 6, Loss: 0.3294964379406181\n",
      "all_preds shape: (182,)\n",
      "all_labels shape: (182,)\n",
      "all_probs shape: (182, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 1, 0]), Shape: torch.Size([922])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([177])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.6953885904673872\n",
      "Epoch 2, Loss: 0.6625688605267426\n",
      "Epoch 3, Loss: 0.4460054285310466\n",
      "Epoch 4, Loss: 0.1573068461830503\n",
      "Epoch 5, Loss: 0.17196469798941036\n",
      "Epoch 6, Loss: 0.06580025677023263\n",
      "all_preds shape: (177,)\n",
      "all_labels shape: (177,)\n",
      "all_probs shape: (177, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([922])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([183])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7169816935884541\n",
      "Epoch 2, Loss: 0.6556289360441011\n",
      "Epoch 3, Loss: 0.5982453038851763\n",
      "Epoch 4, Loss: 0.25044990409615225\n",
      "Epoch 5, Loss: 0.08825054739055962\n",
      "Epoch 6, Loss: 0.07332064898620391\n",
      "all_preds shape: (183,)\n",
      "all_labels shape: (183,)\n",
      "all_probs shape: (183, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 0]), Shape: torch.Size([922])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([179])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.6862422659479338\n",
      "Epoch 2, Loss: 0.639386510026866\n",
      "Epoch 3, Loss: 0.722066915240781\n",
      "Epoch 4, Loss: 0.6576614451819452\n",
      "Epoch 5, Loss: 0.6809939680428341\n",
      "Epoch 6, Loss: 0.6646228319612043\n",
      "all_preds shape: (179,)\n",
      "all_labels shape: (179,)\n",
      "all_probs shape: (179, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 1]), Shape: torch.Size([922])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([177])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.6873614890822048\n",
      "Epoch 2, Loss: 0.3041320699588235\n",
      "Epoch 3, Loss: 0.10662219367503863\n",
      "Epoch 4, Loss: 0.04068230314517458\n",
      "Epoch 5, Loss: 0.041897044880395945\n",
      "Epoch 6, Loss: 0.0013125960490313069\n",
      "all_preds shape: (177,)\n",
      "all_labels shape: (177,)\n",
      "all_probs shape: (177, 2)\n",
      "Train labels sample: tensor([1, 1, 1, 0, 1]), Shape: torch.Size([922])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([179])\n",
      "##########################distilbert-base-multilingual-cased---bert####################\n",
      "Epoch 1, Loss: 0.7284504390996078\n",
      "Epoch 2, Loss: 0.6833090443035652\n",
      "Epoch 3, Loss: 0.6892475835208235\n",
      "Epoch 4, Loss: 0.6711278182679209\n",
      "Epoch 5, Loss: 0.668056810210491\n",
      "Epoch 6, Loss: 0.6920608194737599\n",
      "all_preds shape: (179,)\n",
      "all_labels shape: (179,)\n",
      "all_probs shape: (179, 2)\n",
      "train_Изъяты лексемы с частотой выше 100.csv\n",
      "test_Изъяты лексемы с частотой выше 100.csv\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([887])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([200])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6762579684810979\n",
      "Epoch 2, Loss: 0.6108261618231025\n",
      "Epoch 3, Loss: 0.46521943741078886\n",
      "Epoch 4, Loss: 0.3010507701630039\n",
      "Epoch 5, Loss: 0.24623302589835866\n",
      "Epoch 6, Loss: 0.23898842305477178\n",
      "all_preds shape: (200,)\n",
      "all_labels shape: (200,)\n",
      "all_probs shape: (200, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([887])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([203])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7282153069972992\n",
      "Epoch 2, Loss: 0.6741627114159721\n",
      "Epoch 3, Loss: 0.686475827225617\n",
      "Epoch 4, Loss: 0.6719846283750874\n",
      "Epoch 5, Loss: 0.5593229220913989\n",
      "Epoch 6, Loss: 0.32737998983689715\n",
      "all_preds shape: (203,)\n",
      "all_labels shape: (203,)\n",
      "all_probs shape: (203, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([887])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([199])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.721129479685\n",
      "Epoch 2, Loss: 0.6821234588112149\n",
      "Epoch 3, Loss: 0.6732061404202666\n",
      "Epoch 4, Loss: 0.6875183369432177\n",
      "Epoch 5, Loss: 0.6653891205787659\n",
      "Epoch 6, Loss: 0.6668190360069275\n",
      "all_preds shape: (199,)\n",
      "all_labels shape: (199,)\n",
      "all_probs shape: (199, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([887])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([194])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7439470397574561\n",
      "Epoch 2, Loss: 0.6842520471130099\n",
      "Epoch 3, Loss: 0.6738334447145462\n",
      "Epoch 4, Loss: 0.6681309235947472\n",
      "Epoch 5, Loss: 0.6670097990759781\n",
      "Epoch 6, Loss: 0.6715081589562553\n",
      "all_preds shape: (194,)\n",
      "all_labels shape: (194,)\n",
      "all_probs shape: (194, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([887])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([183])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7048387692442962\n",
      "Epoch 2, Loss: 0.6896583730620998\n",
      "Epoch 3, Loss: 0.6601116220865931\n",
      "Epoch 4, Loss: 0.6654186589377267\n",
      "Epoch 5, Loss: 0.6765749944107873\n",
      "Epoch 6, Loss: 0.6729426969374929\n",
      "all_preds shape: (183,)\n",
      "all_labels shape: (183,)\n",
      "all_probs shape: (183, 2)\n",
      "Train labels sample: tensor([1, 1, 1, 0, 0]), Shape: torch.Size([887])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([189])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7629016520721572\n",
      "Epoch 2, Loss: 0.6748930178582668\n",
      "Epoch 3, Loss: 0.6785384868936879\n",
      "Epoch 4, Loss: 0.6726416264261518\n",
      "Epoch 5, Loss: 0.6681152933410236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.6741568180067199\n",
      "all_preds shape: (189,)\n",
      "all_labels shape: (189,)\n",
      "all_probs shape: (189, 2)\n",
      "train_Изъяты лексемы с частотой выше 49.csv\n",
      "test_Изъяты лексемы с частотой выше 49.csv\n",
      "Train labels sample: tensor([1, 1, 1, 0, 0]), Shape: torch.Size([916])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([195])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6890136394007452\n",
      "Epoch 2, Loss: 0.6459978335890276\n",
      "Epoch 3, Loss: 0.6602924219493208\n",
      "Epoch 4, Loss: 0.6527151917589122\n",
      "Epoch 5, Loss: 0.6430563556736913\n",
      "Epoch 6, Loss: 0.6430481204698826\n",
      "all_preds shape: (195,)\n",
      "all_labels shape: (195,)\n",
      "all_probs shape: (195, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([916])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([187])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.703741814555793\n",
      "Epoch 2, Loss: 0.6714392566475375\n",
      "Epoch 3, Loss: 0.661474816244224\n",
      "Epoch 4, Loss: 0.6636020361349501\n",
      "Epoch 5, Loss: 0.6737046421601854\n",
      "Epoch 6, Loss: 0.6699214567398203\n",
      "all_preds shape: (187,)\n",
      "all_labels shape: (187,)\n",
      "all_probs shape: (187, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 1]), Shape: torch.Size([916])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([190])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6939837382785206\n",
      "Epoch 2, Loss: 0.6651540598992643\n",
      "Epoch 3, Loss: 0.6681468199039328\n",
      "Epoch 4, Loss: 0.6610540197841053\n",
      "Epoch 5, Loss: 0.6667102770558719\n",
      "Epoch 6, Loss: 0.6628469958387572\n",
      "all_preds shape: (190,)\n",
      "all_labels shape: (190,)\n",
      "all_probs shape: (190, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 0]), Shape: torch.Size([916])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([179])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7232835909415936\n",
      "Epoch 2, Loss: 0.656648384086017\n",
      "Epoch 3, Loss: 0.6524224507397619\n",
      "Epoch 4, Loss: 0.655607841138182\n",
      "Epoch 5, Loss: 0.6632921135631101\n",
      "Epoch 6, Loss: 0.6544231561751201\n",
      "all_preds shape: (179,)\n",
      "all_labels shape: (179,)\n",
      "all_probs shape: (179, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([916])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([195])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7053543211057268\n",
      "Epoch 2, Loss: 0.6599664513407082\n",
      "Epoch 3, Loss: 0.6631476719831598\n",
      "Epoch 4, Loss: 0.6597722409100368\n",
      "Epoch 5, Loss: 0.6748685451417133\n",
      "Epoch 6, Loss: 0.6612413931509544\n",
      "all_preds shape: (195,)\n",
      "all_labels shape: (195,)\n",
      "all_probs shape: (195, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 1, 0]), Shape: torch.Size([916])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([186])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6987863388554804\n",
      "Epoch 2, Loss: 0.6634773632575726\n",
      "Epoch 3, Loss: 0.6613901298621605\n",
      "Epoch 4, Loss: 0.6662870460543139\n",
      "Epoch 5, Loss: 0.6700501930097054\n",
      "Epoch 6, Loss: 0.6620608136571687\n",
      "all_preds shape: (186,)\n",
      "all_labels shape: (186,)\n",
      "all_probs shape: (186, 2)\n",
      "train_Изъяты лексемы с частотой выше 29.csv\n",
      "test_Изъяты лексемы с частотой выше 29.csv\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([904])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([191])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7098190209321809\n",
      "Epoch 2, Loss: 0.6456066145185839\n",
      "Epoch 3, Loss: 0.637957902853949\n",
      "Epoch 4, Loss: 0.6526088913281759\n",
      "Epoch 5, Loss: 0.6522625339658636\n",
      "Epoch 6, Loss: 0.6556414580136015\n",
      "all_preds shape: (191,)\n",
      "all_labels shape: (191,)\n",
      "all_probs shape: (191, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 1]), Shape: torch.Size([904])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([205])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6641137992080889\n",
      "Epoch 2, Loss: 0.6621167607474745\n",
      "Epoch 3, Loss: 0.6415917659014986\n",
      "Epoch 4, Loss: 0.6580177069756022\n",
      "Epoch 5, Loss: 0.6638611157735189\n",
      "Epoch 6, Loss: 0.6567835839171159\n",
      "all_preds shape: (205,)\n",
      "all_labels shape: (205,)\n",
      "all_probs shape: (205, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([904])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([186])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7071460041037777\n",
      "Epoch 2, Loss: 0.6578018539830258\n",
      "Epoch 3, Loss: 0.6503550535754153\n",
      "Epoch 4, Loss: 0.6518772291509729\n",
      "Epoch 5, Loss: 0.6430220206578573\n",
      "Epoch 6, Loss: 0.6397084018640351\n",
      "all_preds shape: (186,)\n",
      "all_labels shape: (186,)\n",
      "all_probs shape: (186, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([904])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([200])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6907335901469515\n",
      "Epoch 2, Loss: 0.6591973853738684\n",
      "Epoch 3, Loss: 0.6574691079164806\n",
      "Epoch 4, Loss: 0.6574477501082838\n",
      "Epoch 5, Loss: 0.6505757246101112\n",
      "Epoch 6, Loss: 0.6578570965089297\n",
      "all_preds shape: (200,)\n",
      "all_labels shape: (200,)\n",
      "all_probs shape: (200, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 0]), Shape: torch.Size([904])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([202])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6486983158086476\n",
      "Epoch 2, Loss: 0.5494242464764076\n",
      "Epoch 3, Loss: 0.3617465514362904\n",
      "Epoch 4, Loss: 0.287871854739231\n",
      "Epoch 5, Loss: 0.26720216579473854\n",
      "Epoch 6, Loss: 0.22320490069033808\n",
      "all_preds shape: (202,)\n",
      "all_labels shape: (202,)\n",
      "all_probs shape: (202, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 0]), Shape: torch.Size([904])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([193])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7029294497088382\n",
      "Epoch 2, Loss: 0.6550775703630949\n",
      "Epoch 3, Loss: 0.675410809224112\n",
      "Epoch 4, Loss: 0.654047326560606\n",
      "Epoch 5, Loss: 0.6496295118541048\n",
      "Epoch 6, Loss: 0.6539373983416641\n",
      "all_preds shape: (193,)\n",
      "all_labels shape: (193,)\n",
      "all_probs shape: (193, 2)\n",
      "train_Изъяты лексемы с частотой выше 9.csv\n",
      "test_Изъяты лексемы с частотой выше 9.csv\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([773])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([336])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.725448522567749\n",
      "Epoch 2, Loss: 0.6643876171112061\n",
      "Epoch 3, Loss: 0.6544701600074768\n",
      "Epoch 4, Loss: 0.6469722926616669\n",
      "all_preds shape: (336,)\n",
      "all_labels shape: (336,)\n",
      "all_probs shape: (336, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 1, 1]), Shape: torch.Size([773])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([345])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6875729036331176\n",
      "Epoch 2, Loss: 0.6717945003509521\n",
      "Epoch 3, Loss: 0.5731833636760711\n",
      "Epoch 4, Loss: 0.585618063211441\n",
      "all_preds shape: (345,)\n",
      "all_labels shape: (345,)\n",
      "all_probs shape: (345, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 1, 0]), Shape: torch.Size([773])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([281])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7232162857055664\n",
      "Epoch 2, Loss: 0.6513752794265747\n",
      "Epoch 3, Loss: 0.6521044826507568\n",
      "Epoch 4, Loss: 0.5656740057468415\n",
      "all_preds shape: (281,)\n",
      "all_labels shape: (281,)\n",
      "all_probs shape: (281, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 1]), Shape: torch.Size([773])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([289])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7153648471832276\n",
      "Epoch 2, Loss: 0.6493148493766785\n",
      "Epoch 3, Loss: 0.6274801445007324\n",
      "Epoch 4, Loss: 0.5950633192062378\n",
      "all_preds shape: (289,)\n",
      "all_labels shape: (289,)\n",
      "all_probs shape: (289, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([773])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([278])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7672846758365631\n",
      "Epoch 2, Loss: 0.6742107367515564\n",
      "Epoch 3, Loss: 0.6556241130828857\n",
      "Epoch 4, Loss: 0.6730904960632325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_preds shape: (278,)\n",
      "all_labels shape: (278,)\n",
      "all_probs shape: (278, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 0]), Shape: torch.Size([773])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([273])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6872239923477172\n",
      "Epoch 2, Loss: 0.6207172501087189\n",
      "Epoch 3, Loss: 0.536007103919983\n",
      "Epoch 4, Loss: 0.5014565712213517\n",
      "all_preds shape: (273,)\n",
      "all_labels shape: (273,)\n",
      "all_probs shape: (273, 2)\n",
      "train_Изъяты лексемы с частотой выше 5.csv\n",
      "test_Изъяты лексемы с частотой выше 5.csv\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([99])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([524])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7275110099996839\n",
      "Epoch 2, Loss: 0.6462322728974479\n",
      "Epoch 3, Loss: 0.5641281008720398\n",
      "Epoch 4, Loss: 0.5587720572948456\n",
      "Epoch 5, Loss: 0.5853194040911538\n",
      "Epoch 6, Loss: 0.5865731239318848\n",
      "all_preds shape: (524,)\n",
      "all_labels shape: (524,)\n",
      "all_probs shape: (524, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([99])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([530])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7789669462612697\n",
      "Epoch 2, Loss: 0.5426913116659436\n",
      "Epoch 3, Loss: 0.5017390591757638\n",
      "Epoch 4, Loss: 0.5489639682429177\n",
      "Epoch 5, Loss: 0.5822723933628627\n",
      "Epoch 6, Loss: 0.5398741492203304\n",
      "all_preds shape: (530,)\n",
      "all_labels shape: (530,)\n",
      "all_probs shape: (530, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([99])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([573])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7191416195460728\n",
      "Epoch 2, Loss: 0.5798342823982239\n",
      "Epoch 3, Loss: 0.5691991065229688\n",
      "Epoch 4, Loss: 0.5662211733204978\n",
      "Epoch 5, Loss: 0.5209518883909497\n",
      "Epoch 6, Loss: 0.5655730579580579\n",
      "all_preds shape: (573,)\n",
      "all_labels shape: (573,)\n",
      "all_probs shape: (573, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([99])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([475])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6348800616604942\n",
      "Epoch 2, Loss: 0.6118049834455762\n",
      "Epoch 3, Loss: 0.5579142315047128\n",
      "Epoch 4, Loss: 0.5538805893489293\n",
      "Epoch 5, Loss: 0.5528231518609183\n",
      "Epoch 6, Loss: 0.6026099026203156\n",
      "all_preds shape: (475,)\n",
      "all_labels shape: (475,)\n",
      "all_probs shape: (475, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([99])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([519])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.8799694776535034\n",
      "Epoch 2, Loss: 0.5697654017380306\n",
      "Epoch 3, Loss: 0.6352033657687051\n",
      "Epoch 4, Loss: 0.6206802342619214\n",
      "Epoch 5, Loss: 0.716384755713599\n",
      "Epoch 6, Loss: 0.6216365822723934\n",
      "all_preds shape: (519,)\n",
      "all_labels shape: (519,)\n",
      "all_probs shape: (519, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([99])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([468])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6464223691395351\n",
      "Epoch 2, Loss: 0.5482358059712819\n",
      "Epoch 3, Loss: 0.5833382776805333\n",
      "Epoch 4, Loss: 0.5958727981363025\n",
      "Epoch 5, Loss: 0.531392114503043\n",
      "Epoch 6, Loss: 0.5715662155832563\n",
      "all_preds shape: (468,)\n",
      "all_labels shape: (468,)\n",
      "all_probs shape: (468, 2)\n",
      "train_Изъяты лексемы с частотой выше 3.csv\n",
      "test_Изъяты лексемы с частотой выше 3.csv\n",
      "Train labels sample: tensor([0, 1, 1, 1, 0]), Shape: torch.Size([92])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([722])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7135010759035746\n",
      "Epoch 2, Loss: 0.6226361145575842\n",
      "Epoch 3, Loss: 0.5678048481543859\n",
      "Epoch 4, Loss: 0.5770373394091924\n",
      "Epoch 5, Loss: 0.554595281680425\n",
      "Epoch 6, Loss: 0.5481231460968653\n",
      "all_preds shape: (722,)\n",
      "all_labels shape: (722,)\n",
      "all_probs shape: (722, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 0]), Shape: torch.Size([92])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([735])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.743571271498998\n",
      "Epoch 2, Loss: 0.5050896356503168\n",
      "Epoch 3, Loss: 0.5325314551591873\n",
      "Epoch 4, Loss: 0.4775907099246979\n",
      "Epoch 5, Loss: 0.5148121317227682\n",
      "Epoch 6, Loss: 0.49100030461947125\n",
      "all_preds shape: (735,)\n",
      "all_labels shape: (735,)\n",
      "all_probs shape: (735, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([92])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([714])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7319246580203375\n",
      "Epoch 2, Loss: 0.6119351585706075\n",
      "Epoch 3, Loss: 0.5578232308228811\n",
      "Epoch 4, Loss: 0.5825157562891642\n",
      "Epoch 5, Loss: 0.555489718914032\n",
      "Epoch 6, Loss: 0.5592234482367834\n",
      "all_preds shape: (714,)\n",
      "all_labels shape: (714,)\n",
      "all_probs shape: (714, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([92])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([736])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6805335531632105\n",
      "Epoch 2, Loss: 0.5397161468863487\n",
      "Epoch 3, Loss: 0.5102270444234213\n",
      "Epoch 4, Loss: 0.4830380578835805\n",
      "Epoch 5, Loss: 0.48833635449409485\n",
      "Epoch 6, Loss: 0.4781245291233063\n",
      "all_preds shape: (736,)\n",
      "all_labels shape: (736,)\n",
      "all_probs shape: (736, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 0]), Shape: torch.Size([92])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([699])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.8850419024626414\n",
      "Epoch 2, Loss: 0.5448187738656998\n",
      "Epoch 3, Loss: 0.5517351378997167\n",
      "Epoch 4, Loss: 0.5513099779685339\n",
      "Epoch 5, Loss: 0.5607373615105947\n",
      "Epoch 6, Loss: 0.5414688636859258\n",
      "all_preds shape: (699,)\n",
      "all_labels shape: (699,)\n",
      "all_probs shape: (699, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([92])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([706])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.8673244565725327\n",
      "Epoch 2, Loss: 0.6040227115154266\n",
      "Epoch 3, Loss: 0.5614612946907679\n",
      "Epoch 4, Loss: 0.5858051180839539\n",
      "Epoch 5, Loss: 0.5553762863079706\n",
      "Epoch 6, Loss: 0.5560352057218552\n",
      "all_preds shape: (706,)\n",
      "all_labels shape: (706,)\n",
      "all_probs shape: (706, 2)\n",
      "train_Без прилагательных первый-второй жанр.csv\n",
      "test_Без прилагательных первый-второй жанр.csv\n",
      "Train labels sample: tensor([0, 0, 0, 1, 1]), Shape: torch.Size([872])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([159])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7826369021620069\n",
      "Epoch 2, Loss: 0.66830504153456\n",
      "Epoch 3, Loss: 0.6551121154001781\n",
      "Epoch 4, Loss: 0.6501832625695637\n",
      "all_preds shape: (159,)\n",
      "all_labels shape: (159,)\n",
      "all_probs shape: (159, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 0]), Shape: torch.Size([872])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([159])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6483932222638812\n",
      "Epoch 2, Loss: 0.5445782957332475\n",
      "Epoch 3, Loss: 0.5530631190964154\n",
      "Epoch 4, Loss: 0.3726898101823671\n",
      "all_preds shape: (159,)\n",
      "all_labels shape: (159,)\n",
      "all_probs shape: (159, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 0]), Shape: torch.Size([872])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([161])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6927461389984403\n",
      "Epoch 2, Loss: 0.6493896501404899\n",
      "Epoch 3, Loss: 0.5946019494107792\n",
      "Epoch 4, Loss: 0.4155569097825459\n",
      "all_preds shape: (161,)\n",
      "all_labels shape: (161,)\n",
      "all_probs shape: (161, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 1, 1]), Shape: torch.Size([872])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([153])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7435275422675269\n",
      "Epoch 2, Loss: 0.6515703839915139\n",
      "Epoch 3, Loss: 0.6141283810138702\n",
      "Epoch 4, Loss: 0.5214139169880322\n",
      "all_preds shape: (153,)\n",
      "all_labels shape: (153,)\n",
      "all_probs shape: (153, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([872])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([174])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7068558569465365\n",
      "Epoch 2, Loss: 0.6084821884121213\n",
      "Epoch 3, Loss: 0.4905065042631967\n",
      "Epoch 4, Loss: 0.424970170216901\n",
      "all_preds shape: (174,)\n",
      "all_labels shape: (174,)\n",
      "all_probs shape: (174, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([872])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([162])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.722018009849957\n",
      "Epoch 2, Loss: 0.6602207911866051\n",
      "Epoch 3, Loss: 0.5363602393439838\n",
      "Epoch 4, Loss: 0.4091427544397967\n",
      "all_preds shape: (162,)\n",
      "all_labels shape: (162,)\n",
      "all_probs shape: (162, 2)\n",
      "train_1.Первый жанр исходная выборка.csv\n",
      "test_1.Первый жанр исходная выборка.csv\n",
      "Train labels sample: tensor([1, 0, 0, 1, 1]), Shape: torch.Size([912])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([169])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7075379156229789\n",
      "Epoch 2, Loss: 0.6720434557973293\n",
      "Epoch 3, Loss: 0.6600655714670817\n",
      "Epoch 4, Loss: 0.6771059078082704\n",
      "Epoch 5, Loss: 0.6595145809023004\n",
      "Epoch 6, Loss: 0.6620217566950279\n",
      "all_preds shape: (169,)\n",
      "all_labels shape: (169,)\n",
      "all_probs shape: (169, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 1]), Shape: torch.Size([912])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([169])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7126206776552033\n",
      "Epoch 2, Loss: 0.6747517428900066\n",
      "Epoch 3, Loss: 0.6855788596889429\n",
      "Epoch 4, Loss: 0.6672782605154473\n",
      "Epoch 5, Loss: 0.6702949723653626\n",
      "Epoch 6, Loss: 0.6793055231111091\n",
      "all_preds shape: (169,)\n",
      "all_labels shape: (169,)\n",
      "all_probs shape: (169, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([912])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([168])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6848085276913225\n",
      "Epoch 2, Loss: 0.673475510195682\n",
      "Epoch 3, Loss: 0.681674249339522\n",
      "Epoch 4, Loss: 0.6744710729833234\n",
      "Epoch 5, Loss: 0.6753603021303812\n",
      "Epoch 6, Loss: 0.676160656046449\n",
      "all_preds shape: (168,)\n",
      "all_labels shape: (168,)\n",
      "all_probs shape: (168, 2)\n",
      "Train labels sample: tensor([1, 1, 1, 1, 0]), Shape: torch.Size([912])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([167])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7210657502475538\n",
      "Epoch 2, Loss: 0.6637589345898545\n",
      "Epoch 3, Loss: 0.6750631348082894\n",
      "Epoch 4, Loss: 0.6677084863185883\n",
      "Epoch 5, Loss: 0.6676581219622963\n",
      "Epoch 6, Loss: 0.6644117853097748\n",
      "all_preds shape: (167,)\n",
      "all_labels shape: (167,)\n",
      "all_probs shape: (167, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([912])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([168])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7133801077541552\n",
      "Epoch 2, Loss: 0.6812472343444824\n",
      "Epoch 3, Loss: 0.662434250639196\n",
      "Epoch 4, Loss: 0.6781073871411776\n",
      "Epoch 5, Loss: 0.6726979989754526\n",
      "Epoch 6, Loss: 0.6351766026856607\n",
      "all_preds shape: (168,)\n",
      "all_labels shape: (168,)\n",
      "all_probs shape: (168, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 1]), Shape: torch.Size([912])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([152])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.715040221548917\n",
      "Epoch 2, Loss: 0.6693352157609505\n",
      "Epoch 3, Loss: 0.6777100573506272\n",
      "Epoch 4, Loss: 0.6875937863400108\n",
      "Epoch 5, Loss: 0.6670271315072712\n",
      "Epoch 6, Loss: 0.6644898349778694\n",
      "all_preds shape: (152,)\n",
      "all_labels shape: (152,)\n",
      "all_probs shape: (152, 2)\n",
      "train_2.Первый жанр без клауз, включающих наречия.csv\n",
      "test_2.Первый жанр без клауз, включающих наречия.csv\n",
      "Train labels sample: tensor([0, 1, 1, 0, 1]), Shape: torch.Size([900])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([167])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7082634761668088\n",
      "Epoch 2, Loss: 0.673904652135414\n",
      "Epoch 3, Loss: 0.6487843540676853\n",
      "Epoch 4, Loss: 0.4711865432429732\n",
      "Epoch 5, Loss: 0.32427651369780824\n",
      "Epoch 6, Loss: 0.38858009512095076\n",
      "all_preds shape: (167,)\n",
      "all_labels shape: (167,)\n",
      "all_probs shape: (167, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 1]), Shape: torch.Size([900])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([168])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7350584048973886\n",
      "Epoch 2, Loss: 0.6734004752677784\n",
      "Epoch 3, Loss: 0.6724363554987991\n",
      "Epoch 4, Loss: 0.6668693235045985\n",
      "Epoch 5, Loss: 0.6746076057877457\n",
      "Epoch 6, Loss: 0.6707042968064024\n",
      "all_preds shape: (168,)\n",
      "all_labels shape: (168,)\n",
      "all_probs shape: (168, 2)\n",
      "Train labels sample: tensor([1, 1, 1, 0, 0]), Shape: torch.Size([900])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([163])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.710883895556132\n",
      "Epoch 2, Loss: 0.6783644515171385\n",
      "Epoch 3, Loss: 0.6717280008290943\n",
      "Epoch 4, Loss: 0.6582532305466501\n",
      "Epoch 5, Loss: 0.6633078963087317\n",
      "Epoch 6, Loss: 0.5799918843988787\n",
      "all_preds shape: (163,)\n",
      "all_labels shape: (163,)\n",
      "all_probs shape: (163, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 0]), Shape: torch.Size([900])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([170])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7095688455983212\n",
      "Epoch 2, Loss: 0.6785993973414103\n",
      "Epoch 3, Loss: 0.6806572931900359\n",
      "Epoch 4, Loss: 0.6130289801380091\n",
      "Epoch 5, Loss: 0.46803192232261626\n",
      "Epoch 6, Loss: 0.2877621501684189\n",
      "all_preds shape: (170,)\n",
      "all_labels shape: (170,)\n",
      "all_probs shape: (170, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([900])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([170])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6916708428608743\n",
      "Epoch 2, Loss: 0.6679855480528715\n",
      "Epoch 3, Loss: 0.5853927104096663\n",
      "Epoch 4, Loss: 0.44789577968287886\n",
      "Epoch 5, Loss: 0.26320802897476314\n",
      "Epoch 6, Loss: 0.21717195317410587\n",
      "all_preds shape: (170,)\n",
      "all_labels shape: (170,)\n",
      "all_probs shape: (170, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 0]), Shape: torch.Size([900])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([169])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6539399822552999\n",
      "Epoch 2, Loss: 0.5186590029482256\n",
      "Epoch 3, Loss: 0.3688202075529517\n",
      "Epoch 4, Loss: 0.28212763499795346\n",
      "Epoch 5, Loss: 0.22914582634704156\n",
      "Epoch 6, Loss: 0.20911932658208043\n",
      "all_preds shape: (169,)\n",
      "all_labels shape: (169,)\n",
      "all_probs shape: (169, 2)\n",
      "train_3.Первый жанр без клауз, включающих глаголы.csv\n",
      "test_3.Первый жанр без клауз, включающих глаголы.csv\n",
      "Train labels sample: tensor([0, 0, 0, 1, 1]), Shape: torch.Size([841])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([226])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7232896100799993\n",
      "Epoch 2, Loss: 0.6778995338475929\n",
      "Epoch 3, Loss: 0.684288593958009\n",
      "Epoch 4, Loss: 0.683630072283295\n",
      "Epoch 5, Loss: 0.48229394065883924\n",
      "Epoch 6, Loss: 0.3518445170572344\n",
      "all_preds shape: (226,)\n",
      "all_labels shape: (226,)\n",
      "all_probs shape: (226, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 1, 0]), Shape: torch.Size([841])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([217])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7480106331267447\n",
      "Epoch 2, Loss: 0.6961848207239835\n",
      "Epoch 3, Loss: 0.6973176339887223\n",
      "Epoch 4, Loss: 0.7049016097806534\n",
      "Epoch 5, Loss: 0.6861290920455501\n",
      "Epoch 6, Loss: 0.6511798624722462\n",
      "all_preds shape: (217,)\n",
      "all_labels shape: (217,)\n",
      "all_probs shape: (217, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 1]), Shape: torch.Size([841])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([216])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7315497724515088\n",
      "Epoch 2, Loss: 0.6820957424505701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.6834380480478395\n",
      "Epoch 4, Loss: 0.6902315695330782\n",
      "Epoch 5, Loss: 0.6513827100115003\n",
      "Epoch 6, Loss: 0.4621679387846083\n",
      "all_preds shape: (216,)\n",
      "all_labels shape: (216,)\n",
      "all_probs shape: (216, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 0]), Shape: torch.Size([841])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([225])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7115134934209427\n",
      "Epoch 2, Loss: 0.6903714385797393\n",
      "Epoch 3, Loss: 0.6627031716535676\n",
      "Epoch 4, Loss: 0.6900993664309664\n",
      "Epoch 5, Loss: 0.6870301377098516\n",
      "Epoch 6, Loss: 0.6844705320754141\n",
      "all_preds shape: (225,)\n",
      "all_labels shape: (225,)\n",
      "all_probs shape: (225, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 1, 1]), Shape: torch.Size([841])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([203])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7055419784671856\n",
      "Epoch 2, Loss: 0.6464172264315048\n",
      "Epoch 3, Loss: 0.5198846900800489\n",
      "Epoch 4, Loss: 0.3259234088912325\n",
      "Epoch 5, Loss: 0.2263625015346509\n",
      "Epoch 6, Loss: 0.16649276856333017\n",
      "all_preds shape: (203,)\n",
      "all_labels shape: (203,)\n",
      "all_probs shape: (203, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([841])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([217])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7207706216371285\n",
      "Epoch 2, Loss: 0.6893274171172448\n",
      "Epoch 3, Loss: 0.6884972610563602\n",
      "Epoch 4, Loss: 0.6138211623677667\n",
      "Epoch 5, Loss: 0.6551194444017591\n",
      "Epoch 6, Loss: 0.6991006883810151\n",
      "all_preds shape: (217,)\n",
      "all_labels shape: (217,)\n",
      "all_probs shape: (217, 2)\n",
      "train_4.Первый жанр без клауз, включающих глаголы и наречия.csv\n",
      "test_4.Первый жанр без клауз, включающих глаголы и наречия.csv\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([822])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([239])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7529382602526591\n",
      "Epoch 2, Loss: 0.6929823721830661\n",
      "Epoch 3, Loss: 0.6921872393443034\n",
      "Epoch 4, Loss: 0.6825596873576825\n",
      "Epoch 5, Loss: 0.6877759935764166\n",
      "Epoch 6, Loss: 0.6384542825130316\n",
      "all_preds shape: (239,)\n",
      "all_labels shape: (239,)\n",
      "all_probs shape: (239, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 1]), Shape: torch.Size([822])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([228])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7490727855609014\n",
      "Epoch 2, Loss: 0.6924971651572448\n",
      "Epoch 3, Loss: 0.682810651568266\n",
      "Epoch 4, Loss: 0.678790671321062\n",
      "Epoch 5, Loss: 0.7014374881982803\n",
      "Epoch 6, Loss: 0.652403188439516\n",
      "all_preds shape: (228,)\n",
      "all_labels shape: (228,)\n",
      "all_probs shape: (228, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 1, 0]), Shape: torch.Size([822])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([233])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7245274186134338\n",
      "Epoch 2, Loss: 0.6959688055973786\n",
      "Epoch 3, Loss: 0.6910500606665244\n",
      "Epoch 4, Loss: 0.6736269123279132\n",
      "Epoch 5, Loss: 0.5715227195849786\n",
      "Epoch 6, Loss: 0.4862715353329594\n",
      "all_preds shape: (233,)\n",
      "all_labels shape: (233,)\n",
      "all_probs shape: (233, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 1]), Shape: torch.Size([822])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([236])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6994149111784421\n",
      "Epoch 2, Loss: 0.5662538655675374\n",
      "Epoch 3, Loss: 0.36328196733330304\n",
      "Epoch 4, Loss: 0.2860221711632151\n",
      "Epoch 5, Loss: 0.2010359305780954\n",
      "Epoch 6, Loss: 0.18023963652264613\n",
      "all_preds shape: (236,)\n",
      "all_labels shape: (236,)\n",
      "all_probs shape: (236, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([822])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([232])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7253577147538846\n",
      "Epoch 2, Loss: 0.688427804181209\n",
      "Epoch 3, Loss: 0.7106160212021607\n",
      "Epoch 4, Loss: 0.6762370822521356\n",
      "Epoch 5, Loss: 0.695259268467243\n",
      "Epoch 6, Loss: 0.6908831688073965\n",
      "all_preds shape: (232,)\n",
      "all_labels shape: (232,)\n",
      "all_probs shape: (232, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 0]), Shape: torch.Size([822])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([230])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7266684369398997\n",
      "Epoch 2, Loss: 0.6929251998662949\n",
      "Epoch 3, Loss: 0.6927601603361276\n",
      "Epoch 4, Loss: 0.6810836207408172\n",
      "Epoch 5, Loss: 0.6851948568454156\n",
      "Epoch 6, Loss: 0.6837875487712713\n",
      "all_preds shape: (230,)\n",
      "all_labels shape: (230,)\n",
      "all_probs shape: (230, 2)\n",
      "train_5.без клауз, включающих местоимения.txt.csv\n",
      "test_5.без клауз, включающих местоимения.txt.csv\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([876])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([186])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6647191985086961\n",
      "Epoch 2, Loss: 0.5261314969171177\n",
      "Epoch 3, Loss: 0.29638065800748087\n",
      "Epoch 4, Loss: 0.26579688509756866\n",
      "Epoch 5, Loss: 0.17631633484905415\n",
      "Epoch 6, Loss: 0.19964801613241434\n",
      "all_preds shape: (186,)\n",
      "all_labels shape: (186,)\n",
      "all_probs shape: (186, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([876])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([200])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7007494915615429\n",
      "Epoch 2, Loss: 0.6776656578887593\n",
      "Epoch 3, Loss: 0.6497636247764934\n",
      "Epoch 4, Loss: 0.6533020079135895\n",
      "Epoch 5, Loss: 0.48823116279461165\n",
      "Epoch 6, Loss: 0.28413362218575045\n",
      "all_preds shape: (200,)\n",
      "all_labels shape: (200,)\n",
      "all_probs shape: (200, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([876])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([192])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.715244535966353\n",
      "Epoch 2, Loss: 0.6752742377194492\n",
      "Epoch 3, Loss: 0.6665725366635756\n",
      "Epoch 4, Loss: 0.5006529062986373\n",
      "Epoch 5, Loss: 0.3292303824966604\n",
      "Epoch 6, Loss: 0.2665736716240644\n",
      "all_preds shape: (192,)\n",
      "all_labels shape: (192,)\n",
      "all_probs shape: (192, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([876])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([181])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7492790455167944\n",
      "Epoch 2, Loss: 0.6817749381065369\n",
      "Epoch 3, Loss: 0.6881617887453599\n",
      "Epoch 4, Loss: 0.6529663741588593\n",
      "Epoch 5, Loss: 0.5750855448571118\n",
      "Epoch 6, Loss: 0.41590417244217615\n",
      "all_preds shape: (181,)\n",
      "all_labels shape: (181,)\n",
      "all_probs shape: (181, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 1]), Shape: torch.Size([876])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([175])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7278295706618916\n",
      "Epoch 2, Loss: 0.6965496095744046\n",
      "Epoch 3, Loss: 0.6749940308657559\n",
      "Epoch 4, Loss: 0.6726174408739264\n",
      "Epoch 5, Loss: 0.6733804182572798\n",
      "Epoch 6, Loss: 0.6785811305046081\n",
      "all_preds shape: (175,)\n",
      "all_labels shape: (175,)\n",
      "all_probs shape: (175, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 1, 0]), Shape: torch.Size([876])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([185])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7081947147846221\n",
      "Epoch 2, Loss: 0.669309080730785\n",
      "Epoch 3, Loss: 0.5283480953086507\n",
      "Epoch 4, Loss: 0.3666024301539768\n",
      "Epoch 5, Loss: 0.24861465042287653\n",
      "Epoch 6, Loss: 0.227098165537146\n",
      "all_preds shape: (185,)\n",
      "all_labels shape: (185,)\n",
      "all_probs shape: (185, 2)\n",
      "train_6.без слов функциональных.txt.csv\n",
      "test_6.без слов функциональных.txt.csv\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([944])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([172])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6872934613187435\n",
      "Epoch 2, Loss: 0.6591270439705607\n",
      "Epoch 3, Loss: 0.6283417808807502\n",
      "Epoch 4, Loss: 0.4306447358202126\n",
      "Epoch 5, Loss: 0.2645096943666369\n",
      "Epoch 6, Loss: 0.2414813005696919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_preds shape: (172,)\n",
      "all_labels shape: (172,)\n",
      "all_probs shape: (172, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([944])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([188])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7084104822853864\n",
      "Epoch 2, Loss: 0.6606982532194106\n",
      "Epoch 3, Loss: 0.6640634587255575\n",
      "Epoch 4, Loss: 0.6396793697850179\n",
      "Epoch 5, Loss: 0.629078172020993\n",
      "Epoch 6, Loss: 0.6580885375960398\n",
      "all_preds shape: (188,)\n",
      "all_labels shape: (188,)\n",
      "all_probs shape: (188, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([944])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([173])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7004188662868435\n",
      "Epoch 2, Loss: 0.6721041530875836\n",
      "Epoch 3, Loss: 0.6693164630461548\n",
      "Epoch 4, Loss: 0.635230144706823\n",
      "Epoch 5, Loss: 0.46005385747905503\n",
      "Epoch 6, Loss: 0.26784015933083277\n",
      "all_preds shape: (173,)\n",
      "all_labels shape: (173,)\n",
      "all_probs shape: (173, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([944])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([171])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6940263696646286\n",
      "Epoch 2, Loss: 0.6800156175079992\n",
      "Epoch 3, Loss: 0.6696448290752153\n",
      "Epoch 4, Loss: 0.6814405185691381\n",
      "Epoch 5, Loss: 0.6763416700443979\n",
      "Epoch 6, Loss: 0.670567409972013\n",
      "all_preds shape: (171,)\n",
      "all_labels shape: (171,)\n",
      "all_probs shape: (171, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 1, 0]), Shape: torch.Size([944])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([160])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6513299578327244\n",
      "Epoch 2, Loss: 0.4285770722870099\n",
      "Epoch 3, Loss: 0.17717344585364148\n",
      "Epoch 4, Loss: 0.13271614794729877\n",
      "Epoch 5, Loss: 0.09924985084414356\n",
      "Epoch 6, Loss: 0.07556382194556044\n",
      "all_preds shape: (160,)\n",
      "all_labels shape: (160,)\n",
      "all_probs shape: (160, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 0]), Shape: torch.Size([944])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([182])\n",
      "##########################roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6918399333953857\n",
      "Epoch 2, Loss: 0.6600266573792797\n",
      "Epoch 3, Loss: 0.41404138906401095\n",
      "Epoch 4, Loss: 0.18362258178955418\n",
      "Epoch 5, Loss: 0.13634447833144311\n",
      "Epoch 6, Loss: 0.0857997591039008\n",
      "all_preds shape: (182,)\n",
      "all_labels shape: (182,)\n",
      "all_probs shape: (182, 2)\n",
      "train_Изъяты лексемы с частотой выше 100.csv\n",
      "test_Изъяты лексемы с частотой выше 100.csv\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([895])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([188])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.688482398965529\n",
      "Epoch 2, Loss: 0.6060620924191815\n",
      "Epoch 3, Loss: 0.5955943060772759\n",
      "Epoch 4, Loss: 0.587399957967656\n",
      "Epoch 5, Loss: 0.5707861967384815\n",
      "Epoch 6, Loss: 0.5635816172829696\n",
      "all_preds shape: (188,)\n",
      "all_labels shape: (188,)\n",
      "all_probs shape: (188, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 1]), Shape: torch.Size([895])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([198])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.7016396618315152\n",
      "Epoch 2, Loss: 0.6156000345945358\n",
      "Epoch 3, Loss: 0.6125967007662568\n",
      "Epoch 4, Loss: 0.6168025625603539\n",
      "Epoch 5, Loss: 0.641467918242727\n",
      "Epoch 6, Loss: 0.6240573679762227\n",
      "all_preds shape: (198,)\n",
      "all_labels shape: (198,)\n",
      "all_probs shape: (198, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([895])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([189])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6421428024768829\n",
      "Epoch 2, Loss: 0.6141635679772922\n",
      "Epoch 3, Loss: 0.6038423217833042\n",
      "Epoch 4, Loss: 0.6018760316073895\n",
      "Epoch 5, Loss: 0.5450249676193509\n",
      "Epoch 6, Loss: 0.5496662216527122\n",
      "all_preds shape: (189,)\n",
      "all_labels shape: (189,)\n",
      "all_probs shape: (189, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([895])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([195])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.7164326448525701\n",
      "Epoch 2, Loss: 0.6207164781434196\n",
      "Epoch 3, Loss: 0.5969056447169611\n",
      "Epoch 4, Loss: 0.6016418944512095\n",
      "Epoch 5, Loss: 0.5889847970434597\n",
      "Epoch 6, Loss: 0.5499345037553992\n",
      "all_preds shape: (195,)\n",
      "all_labels shape: (195,)\n",
      "all_probs shape: (195, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 1]), Shape: torch.Size([895])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([187])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.7467093398528439\n",
      "Epoch 2, Loss: 0.6490948689835412\n",
      "Epoch 3, Loss: 0.6055191962846688\n",
      "Epoch 4, Loss: 0.6230315566062927\n",
      "Epoch 5, Loss: 0.6155359196875777\n",
      "Epoch 6, Loss: 0.598962025982993\n",
      "all_preds shape: (187,)\n",
      "all_labels shape: (187,)\n",
      "all_probs shape: (187, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 1]), Shape: torch.Size([895])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([190])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6861789354256221\n",
      "Epoch 2, Loss: 0.6366298076297555\n",
      "Epoch 3, Loss: 0.5989113833223071\n",
      "Epoch 4, Loss: 0.5825904235243797\n",
      "Epoch 5, Loss: 0.5593769108610493\n",
      "Epoch 6, Loss: 0.5579746312328747\n",
      "all_preds shape: (190,)\n",
      "all_labels shape: (190,)\n",
      "all_probs shape: (190, 2)\n",
      "train_Изъяты лексемы с частотой выше 49.csv\n",
      "test_Изъяты лексемы с частотой выше 49.csv\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([926])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([181])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6470949310680916\n",
      "Epoch 2, Loss: 0.5938341003553621\n",
      "Epoch 3, Loss: 0.5683085990363154\n",
      "Epoch 4, Loss: 0.6121753608358318\n",
      "Epoch 5, Loss: 0.5694485168004858\n",
      "Epoch 6, Loss: 0.5524161329557156\n",
      "all_preds shape: (181,)\n",
      "all_labels shape: (181,)\n",
      "all_probs shape: (181, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 1]), Shape: torch.Size([926])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([197])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6629404441036028\n",
      "Epoch 2, Loss: 0.5859190328367825\n",
      "Epoch 3, Loss: 0.5939160744691717\n",
      "Epoch 4, Loss: 0.5606633512110546\n",
      "Epoch 5, Loss: 0.5527588139320242\n",
      "Epoch 6, Loss: 0.5407974170199756\n",
      "all_preds shape: (197,)\n",
      "all_labels shape: (197,)\n",
      "all_probs shape: (197, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 1, 0]), Shape: torch.Size([926])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([184])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6796945189607555\n",
      "Epoch 2, Loss: 0.5743223608567797\n",
      "Epoch 3, Loss: 0.5528908952556807\n",
      "Epoch 4, Loss: 0.5673153914254287\n",
      "Epoch 5, Loss: 0.5375732774364537\n",
      "Epoch 6, Loss: 0.5554959172832554\n",
      "all_preds shape: (184,)\n",
      "all_labels shape: (184,)\n",
      "all_probs shape: (184, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([926])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([173])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.614621596603558\n",
      "Epoch 2, Loss: 0.5994011045529924\n",
      "Epoch 3, Loss: 0.5826129851670101\n",
      "Epoch 4, Loss: 0.5486035485719812\n",
      "Epoch 5, Loss: 0.541969674928435\n",
      "Epoch 6, Loss: 0.5384108778731577\n",
      "all_preds shape: (173,)\n",
      "all_labels shape: (173,)\n",
      "all_probs shape: (173, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 0]), Shape: torch.Size([926])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([194])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.7000431906560372\n",
      "Epoch 2, Loss: 0.5899001483259529\n",
      "Epoch 3, Loss: 0.5829603975189144\n",
      "Epoch 4, Loss: 0.5712714477859694\n",
      "Epoch 5, Loss: 0.5646784187390886\n",
      "Epoch 6, Loss: 0.5874916541165319\n",
      "all_preds shape: (194,)\n",
      "all_labels shape: (194,)\n",
      "all_probs shape: (194, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 1, 0]), Shape: torch.Size([926])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([186])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6122685804449278\n",
      "Epoch 2, Loss: 0.579480593060625\n",
      "Epoch 3, Loss: 0.5457719179063008\n",
      "Epoch 4, Loss: 0.534875036313616\n",
      "Epoch 5, Loss: 0.5020107832448236\n",
      "Epoch 6, Loss: 0.47724319454924813\n",
      "all_preds shape: (186,)\n",
      "all_labels shape: (186,)\n",
      "all_probs shape: (186, 2)\n",
      "train_Изъяты лексемы с частотой выше 29.csv\n",
      "test_Изъяты лексемы с частотой выше 29.csv\n",
      "Train labels sample: tensor([1, 0, 0, 0, 0]), Shape: torch.Size([914])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([185])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6040162865457863\n",
      "Epoch 2, Loss: 0.5494267534593056\n",
      "Epoch 3, Loss: 0.5933094692641291\n",
      "Epoch 4, Loss: 0.5580242014136808\n",
      "Epoch 5, Loss: 0.548582555918858\n",
      "Epoch 6, Loss: 0.5371978981741543\n",
      "all_preds shape: (185,)\n",
      "all_labels shape: (185,)\n",
      "all_probs shape: (185, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 1, 1]), Shape: torch.Size([914])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([183])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6182123843965859\n",
      "Epoch 2, Loss: 0.5787499983762873\n",
      "Epoch 3, Loss: 0.6108090728521347\n",
      "Epoch 4, Loss: 0.6387045203611769\n",
      "Epoch 5, Loss: 0.553265453412615\n",
      "Epoch 6, Loss: 0.5463066666290678\n",
      "all_preds shape: (183,)\n",
      "all_labels shape: (183,)\n",
      "all_probs shape: (183, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 1, 0]), Shape: torch.Size([914])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([195])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6613962547532444\n",
      "Epoch 2, Loss: 0.5699063154130146\n",
      "Epoch 3, Loss: 0.5512224836596127\n",
      "Epoch 4, Loss: 0.5845367404921301\n",
      "Epoch 5, Loss: 0.5494227260351181\n",
      "Epoch 6, Loss: 0.5470154264877582\n",
      "all_preds shape: (195,)\n",
      "all_labels shape: (195,)\n",
      "all_probs shape: (195, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 1, 0]), Shape: torch.Size([914])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([195])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6230899054428627\n",
      "Epoch 2, Loss: 0.563870565644626\n",
      "Epoch 3, Loss: 0.5566445928195427\n",
      "Epoch 4, Loss: 0.5276634389984196\n",
      "Epoch 5, Loss: 0.5187766007308302\n",
      "Epoch 6, Loss: 0.5157469937513615\n",
      "all_preds shape: (195,)\n",
      "all_labels shape: (195,)\n",
      "all_probs shape: (195, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 0]), Shape: torch.Size([914])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([191])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.576929367821792\n",
      "Epoch 2, Loss: 0.577551031934804\n",
      "Epoch 3, Loss: 0.5473712482329073\n",
      "Epoch 4, Loss: 0.5886173304812662\n",
      "Epoch 5, Loss: 0.5516629958974903\n",
      "Epoch 6, Loss: 0.533490329467017\n",
      "all_preds shape: (191,)\n",
      "all_labels shape: (191,)\n",
      "all_probs shape: (191, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([914])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([196])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.644574295857857\n",
      "Epoch 2, Loss: 0.5941369723657082\n",
      "Epoch 3, Loss: 0.571230916113689\n",
      "Epoch 4, Loss: 0.5708541710828913\n",
      "Epoch 5, Loss: 0.5591090768575668\n",
      "Epoch 6, Loss: 0.5479353106227415\n",
      "all_preds shape: (196,)\n",
      "all_labels shape: (196,)\n",
      "all_probs shape: (196, 2)\n",
      "train_Изъяты лексемы с частотой выше 9.csv\n",
      "test_Изъяты лексемы с частотой выше 9.csv\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([784])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([265])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6145999431610107\n",
      "Epoch 2, Loss: 0.5913048577308655\n",
      "Epoch 3, Loss: 0.5873128151893616\n",
      "Epoch 4, Loss: 0.5674604058265686\n",
      "all_preds shape: (265,)\n",
      "all_labels shape: (265,)\n",
      "all_probs shape: (265, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([784])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([264])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.7090999126434326\n",
      "Epoch 2, Loss: 0.602193523645401\n",
      "Epoch 3, Loss: 0.5731655120849609\n",
      "Epoch 4, Loss: 0.5847753155231475\n",
      "all_preds shape: (264,)\n",
      "all_labels shape: (264,)\n",
      "all_probs shape: (264, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 1]), Shape: torch.Size([784])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([337])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.7018519067764282\n",
      "Epoch 2, Loss: 0.5847427916526794\n",
      "Epoch 3, Loss: 0.5851860070228576\n",
      "Epoch 4, Loss: 0.5848724830150605\n",
      "all_preds shape: (337,)\n",
      "all_labels shape: (337,)\n",
      "all_probs shape: (337, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 0]), Shape: torch.Size([784])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([268])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6711473846435547\n",
      "Epoch 2, Loss: 0.597453989982605\n",
      "Epoch 3, Loss: 0.6026830911636353\n",
      "Epoch 4, Loss: 0.5788800644874573\n",
      "all_preds shape: (268,)\n",
      "all_labels shape: (268,)\n",
      "all_probs shape: (268, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([784])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([268])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6245969378948212\n",
      "Epoch 2, Loss: 0.5820532190799713\n",
      "Epoch 3, Loss: 0.5810454082489014\n",
      "Epoch 4, Loss: 0.5652959895133972\n",
      "all_preds shape: (268,)\n",
      "all_labels shape: (268,)\n",
      "all_probs shape: (268, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([784])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([272])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6460919618606568\n",
      "Epoch 2, Loss: 0.5999720966815949\n",
      "Epoch 3, Loss: 0.5978852224349975\n",
      "Epoch 4, Loss: 0.5887519431114197\n",
      "all_preds shape: (272,)\n",
      "all_labels shape: (272,)\n",
      "all_probs shape: (272, 2)\n",
      "train_Изъяты лексемы с частотой выше 5.csv\n",
      "test_Изъяты лексемы с частотой выше 5.csv\n",
      "Train labels sample: tensor([1, 0, 0, 0, 0]), Shape: torch.Size([97])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([550])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.541637837354626\n",
      "Epoch 2, Loss: 0.7291768661567143\n",
      "Epoch 3, Loss: 0.797326100724084\n",
      "Epoch 4, Loss: 0.5512386858463287\n",
      "Epoch 5, Loss: 0.5215467129434858\n",
      "Epoch 6, Loss: 0.5231998945985522\n",
      "all_preds shape: (550,)\n",
      "all_labels shape: (550,)\n",
      "all_probs shape: (550, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([97])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([503])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6737408850874219\n",
      "Epoch 2, Loss: 0.7677808233669826\n",
      "Epoch 3, Loss: 0.5528530393327985\n",
      "Epoch 4, Loss: 0.4886207559279033\n",
      "Epoch 5, Loss: 0.5049423341240201\n",
      "Epoch 6, Loss: 0.49705809354782104\n",
      "all_preds shape: (503,)\n",
      "all_labels shape: (503,)\n",
      "all_probs shape: (503, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 0]), Shape: torch.Size([97])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([489])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6153756635529655\n",
      "Epoch 2, Loss: 0.4847366639545986\n",
      "Epoch 3, Loss: 0.5508831441402435\n",
      "Epoch 4, Loss: 0.5239599389689309\n",
      "Epoch 5, Loss: 0.5230178449835096\n",
      "Epoch 6, Loss: 0.6719248678003039\n",
      "all_preds shape: (489,)\n",
      "all_labels shape: (489,)\n",
      "all_probs shape: (489, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([97])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([487])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.7158674001693726\n",
      "Epoch 2, Loss: 0.5684642153126853\n",
      "Epoch 3, Loss: 0.5245141237974167\n",
      "Epoch 4, Loss: 0.5222420692443848\n",
      "Epoch 5, Loss: 0.5210653820208141\n",
      "Epoch 6, Loss: 0.5130062465156827\n",
      "all_preds shape: (487,)\n",
      "all_labels shape: (487,)\n",
      "all_probs shape: (487, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([97])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([499])\n",
      "##########################xlnet-base-cased---bert####################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.8096514897687095\n",
      "Epoch 2, Loss: 0.601523722921099\n",
      "Epoch 3, Loss: 0.61415034532547\n",
      "Epoch 4, Loss: 0.5549931568758828\n",
      "Epoch 5, Loss: 0.7232975704329354\n",
      "Epoch 6, Loss: 0.5449147054127285\n",
      "all_preds shape: (499,)\n",
      "all_labels shape: (499,)\n",
      "all_probs shape: (499, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([97])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([474])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.7894937992095947\n",
      "Epoch 2, Loss: 0.7471079954079219\n",
      "Epoch 3, Loss: 0.6286846484456744\n",
      "Epoch 4, Loss: 0.5449071420090539\n",
      "Epoch 5, Loss: 0.5871113751615796\n",
      "Epoch 6, Loss: 0.6882726081780025\n",
      "all_preds shape: (474,)\n",
      "all_labels shape: (474,)\n",
      "all_probs shape: (474, 2)\n",
      "train_Изъяты лексемы с частотой выше 3.csv\n",
      "test_Изъяты лексемы с частотой выше 3.csv\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([91])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([708])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.5441897859176\n",
      "Epoch 2, Loss: 0.5500387201706568\n",
      "Epoch 3, Loss: 0.5392665565013885\n",
      "Epoch 4, Loss: 0.5032783895730972\n",
      "Epoch 5, Loss: 0.49736450612545013\n",
      "Epoch 6, Loss: 0.49086499214172363\n",
      "all_preds shape: (708,)\n",
      "all_labels shape: (708,)\n",
      "all_probs shape: (708, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([91])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([736])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.5462115009625753\n",
      "Epoch 2, Loss: 0.47797240565220517\n",
      "Epoch 3, Loss: 0.4823194742202759\n",
      "Epoch 4, Loss: 0.532199556628863\n",
      "Epoch 5, Loss: 0.519441694021225\n",
      "Epoch 6, Loss: 0.48442578315734863\n",
      "all_preds shape: (736,)\n",
      "all_labels shape: (736,)\n",
      "all_probs shape: (736, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([91])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([745])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.8652290254831314\n",
      "Epoch 2, Loss: 0.6345894287029902\n",
      "Epoch 3, Loss: 0.593997726837794\n",
      "Epoch 4, Loss: 0.5809455662965775\n",
      "Epoch 5, Loss: 0.6086790213982264\n",
      "Epoch 6, Loss: 0.5662186741828918\n",
      "all_preds shape: (745,)\n",
      "all_labels shape: (745,)\n",
      "all_probs shape: (745, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([91])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([740])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.587006558974584\n",
      "Epoch 2, Loss: 0.5063045273224512\n",
      "Epoch 3, Loss: 0.5080898553133011\n",
      "Epoch 4, Loss: 0.5367473339041074\n",
      "Epoch 5, Loss: 0.512094184756279\n",
      "Epoch 6, Loss: 0.5259680350621542\n",
      "all_preds shape: (740,)\n",
      "all_labels shape: (740,)\n",
      "all_probs shape: (740, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([91])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([731])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.971704105536143\n",
      "Epoch 2, Loss: 0.5994201997915903\n",
      "Epoch 3, Loss: 0.5817987074454626\n",
      "Epoch 4, Loss: 0.5552477290232977\n",
      "Epoch 5, Loss: 0.5764413873354594\n",
      "Epoch 6, Loss: 0.5847931702931722\n",
      "all_preds shape: (731,)\n",
      "all_labels shape: (731,)\n",
      "all_probs shape: (731, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([91])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([698])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.7940548832217852\n",
      "Epoch 2, Loss: 0.524948055545489\n",
      "Epoch 3, Loss: 0.5667489171028137\n",
      "Epoch 4, Loss: 0.5518971880276998\n",
      "Epoch 5, Loss: 0.5337336510419846\n",
      "Epoch 6, Loss: 0.5348951468865076\n",
      "all_preds shape: (698,)\n",
      "all_labels shape: (698,)\n",
      "all_probs shape: (698, 2)\n",
      "train_Без прилагательных первый-второй жанр.csv\n",
      "test_Без прилагательных первый-второй жанр.csv\n",
      "Train labels sample: tensor([1, 0, 0, 0, 1]), Shape: torch.Size([874])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([168])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.5980400538870266\n",
      "Epoch 2, Loss: 0.5310575099928039\n",
      "Epoch 3, Loss: 0.5648258775472641\n",
      "Epoch 4, Loss: 0.5061604338032859\n",
      "all_preds shape: (168,)\n",
      "all_labels shape: (168,)\n",
      "all_probs shape: (168, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 0]), Shape: torch.Size([874])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([159])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6142811136586326\n",
      "Epoch 2, Loss: 0.5322691170232636\n",
      "Epoch 3, Loss: 0.51841808536223\n",
      "Epoch 4, Loss: 0.537581958941051\n",
      "all_preds shape: (159,)\n",
      "all_labels shape: (159,)\n",
      "all_probs shape: (159, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 1, 0]), Shape: torch.Size([874])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([164])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6657171323895454\n",
      "Epoch 2, Loss: 0.5755092554858753\n",
      "Epoch 3, Loss: 0.5518636628985405\n",
      "Epoch 4, Loss: 0.5045953009809766\n",
      "all_preds shape: (164,)\n",
      "all_labels shape: (164,)\n",
      "all_probs shape: (164, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([874])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([173])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6561041197606495\n",
      "Epoch 2, Loss: 0.581371220094817\n",
      "Epoch 3, Loss: 0.5498313659003803\n",
      "Epoch 4, Loss: 0.5658856119428363\n",
      "all_preds shape: (173,)\n",
      "all_labels shape: (173,)\n",
      "all_probs shape: (173, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 1, 0]), Shape: torch.Size([874])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([169])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.61461605238063\n",
      "Epoch 2, Loss: 0.5623270305139678\n",
      "Epoch 3, Loss: 0.5582815217120307\n",
      "Epoch 4, Loss: 0.5309898427554539\n",
      "all_preds shape: (169,)\n",
      "all_labels shape: (169,)\n",
      "all_probs shape: (169, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 1]), Shape: torch.Size([874])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([161])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6930423038346427\n",
      "Epoch 2, Loss: 0.569511700953756\n",
      "Epoch 3, Loss: 0.5580862260290554\n",
      "Epoch 4, Loss: 0.5245309982981\n",
      "all_preds shape: (161,)\n",
      "all_labels shape: (161,)\n",
      "all_probs shape: (161, 2)\n",
      "train_1.Первый жанр исходная выборка.csv\n",
      "test_1.Первый жанр исходная выборка.csv\n",
      "Train labels sample: tensor([1, 0, 0, 0, 0]), Shape: torch.Size([914])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([177])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6656114053109596\n",
      "Epoch 2, Loss: 0.5939834993974916\n",
      "Epoch 3, Loss: 0.5711898269324467\n",
      "Epoch 4, Loss: 0.5825873839444128\n",
      "Epoch 5, Loss: 0.5516188843496914\n",
      "Epoch 6, Loss: 0.49369592887574226\n",
      "all_preds shape: (177,)\n",
      "all_labels shape: (177,)\n",
      "all_probs shape: (177, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 1, 1]), Shape: torch.Size([914])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([164])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6855673425156494\n",
      "Epoch 2, Loss: 0.6145421944815537\n",
      "Epoch 3, Loss: 0.6348410495396318\n",
      "Epoch 4, Loss: 0.6857564079350439\n",
      "Epoch 5, Loss: 0.6703772041304358\n",
      "Epoch 6, Loss: 0.66588921639426\n",
      "all_preds shape: (164,)\n",
      "all_labels shape: (164,)\n",
      "all_probs shape: (164, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 1, 0]), Shape: torch.Size([914])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([154])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6657781791070412\n",
      "Epoch 2, Loss: 0.609571099281311\n",
      "Epoch 3, Loss: 0.5994036449440594\n",
      "Epoch 4, Loss: 0.582390216403994\n",
      "Epoch 5, Loss: 0.5710229467729042\n",
      "Epoch 6, Loss: 0.5618807205866123\n",
      "all_preds shape: (154,)\n",
      "all_labels shape: (154,)\n",
      "all_probs shape: (154, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 1, 0]), Shape: torch.Size([914])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([156])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6773114564089939\n",
      "Epoch 2, Loss: 0.606613473645572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.6348751043451244\n",
      "Epoch 4, Loss: 0.5898286083648945\n",
      "Epoch 5, Loss: 0.5819157192419315\n",
      "Epoch 6, Loss: 0.5837641959560329\n",
      "all_preds shape: (156,)\n",
      "all_labels shape: (156,)\n",
      "all_probs shape: (156, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 0]), Shape: torch.Size([914])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([155])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.686334401882928\n",
      "Epoch 2, Loss: 0.6075666500576611\n",
      "Epoch 3, Loss: 0.609864766227788\n",
      "Epoch 4, Loss: 0.620306674262573\n",
      "Epoch 5, Loss: 0.597754617703372\n",
      "Epoch 6, Loss: 0.5696718484677118\n",
      "all_preds shape: (155,)\n",
      "all_labels shape: (155,)\n",
      "all_probs shape: (155, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([914])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([163])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6349198448246923\n",
      "Epoch 2, Loss: 0.623415704431205\n",
      "Epoch 3, Loss: 0.5977097539038494\n",
      "Epoch 4, Loss: 0.5846069460285122\n",
      "Epoch 5, Loss: 0.5659806980141278\n",
      "Epoch 6, Loss: 0.5614554486398039\n",
      "all_preds shape: (163,)\n",
      "all_labels shape: (163,)\n",
      "all_probs shape: (163, 2)\n",
      "train_2.Первый жанр без клауз, включающих наречия.csv\n",
      "test_2.Первый жанр без клауз, включающих наречия.csv\n",
      "Train labels sample: tensor([0, 0, 1, 1, 0]), Shape: torch.Size([902])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([167])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6550057106896451\n",
      "Epoch 2, Loss: 0.6182693263940644\n",
      "Epoch 3, Loss: 0.5784277220567068\n",
      "Epoch 4, Loss: 0.5747283812154803\n",
      "Epoch 5, Loss: 0.5677372507358852\n",
      "Epoch 6, Loss: 0.5659222508731642\n",
      "all_preds shape: (167,)\n",
      "all_labels shape: (167,)\n",
      "all_probs shape: (167, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 1]), Shape: torch.Size([902])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([173])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6615802232633557\n",
      "Epoch 2, Loss: 0.5979124103721819\n",
      "Epoch 3, Loss: 0.5961370588394633\n",
      "Epoch 4, Loss: 0.5895017699191445\n",
      "Epoch 5, Loss: 0.6054813736363461\n",
      "Epoch 6, Loss: 0.5681038221769166\n",
      "all_preds shape: (173,)\n",
      "all_labels shape: (173,)\n",
      "all_probs shape: (173, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 1]), Shape: torch.Size([902])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([164])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6688773307883948\n",
      "Epoch 2, Loss: 0.5895006233140042\n",
      "Epoch 3, Loss: 0.5802064517088104\n",
      "Epoch 4, Loss: 0.5906007684636534\n",
      "Epoch 5, Loss: 0.5728747619871508\n",
      "Epoch 6, Loss: 0.5293413536590442\n",
      "all_preds shape: (164,)\n",
      "all_labels shape: (164,)\n",
      "all_probs shape: (164, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([902])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([184])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6777931932817426\n",
      "Epoch 2, Loss: 0.5835730130212349\n",
      "Epoch 3, Loss: 0.582571835110062\n",
      "Epoch 4, Loss: 0.5856166021865711\n",
      "Epoch 5, Loss: 0.5432567999028323\n",
      "Epoch 6, Loss: 0.5504511113752398\n",
      "all_preds shape: (184,)\n",
      "all_labels shape: (184,)\n",
      "all_probs shape: (184, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([902])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([175])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6411767486940351\n",
      "Epoch 2, Loss: 0.6001461838421068\n",
      "Epoch 3, Loss: 0.5911439981376916\n",
      "Epoch 4, Loss: 0.5657301294176202\n",
      "Epoch 5, Loss: 0.5489495422756463\n",
      "Epoch 6, Loss: 0.5395786526956057\n",
      "all_preds shape: (175,)\n",
      "all_labels shape: (175,)\n",
      "all_probs shape: (175, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([902])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([158])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6477978093582287\n",
      "Epoch 2, Loss: 0.60553055583385\n",
      "Epoch 3, Loss: 0.6221375062800291\n",
      "Epoch 4, Loss: 0.6032384686302721\n",
      "Epoch 5, Loss: 0.6096879039940081\n",
      "Epoch 6, Loss: 0.586953050734704\n",
      "all_preds shape: (158,)\n",
      "all_labels shape: (158,)\n",
      "all_probs shape: (158, 2)\n",
      "train_3.Первый жанр без клауз, включающих глаголы.csv\n",
      "test_3.Первый жанр без клауз, включающих глаголы.csv\n",
      "Train labels sample: tensor([1, 1, 0, 0, 1]), Shape: torch.Size([844])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([220])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6568620182433218\n",
      "Epoch 2, Loss: 0.5796504313091062\n",
      "Epoch 3, Loss: 0.5751220938169731\n",
      "Epoch 4, Loss: 0.5586823135052087\n",
      "Epoch 5, Loss: 0.6096983709425297\n",
      "Epoch 6, Loss: 0.5645384006905105\n",
      "all_preds shape: (220,)\n",
      "all_labels shape: (220,)\n",
      "all_probs shape: (220, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 1, 1]), Shape: torch.Size([844])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([220])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6877714637315498\n",
      "Epoch 2, Loss: 0.6252118056675173\n",
      "Epoch 3, Loss: 0.5921740503805988\n",
      "Epoch 4, Loss: 0.5892406578333873\n",
      "Epoch 5, Loss: 0.5689772530546728\n",
      "Epoch 6, Loss: 0.5347918086456802\n",
      "all_preds shape: (220,)\n",
      "all_labels shape: (220,)\n",
      "all_probs shape: (220, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 1]), Shape: torch.Size([844])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([212])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.657673217777936\n",
      "Epoch 2, Loss: 0.6221579538201386\n",
      "Epoch 3, Loss: 0.604172224143766\n",
      "Epoch 4, Loss: 0.596897338921169\n",
      "Epoch 5, Loss: 0.5338745606395433\n",
      "Epoch 6, Loss: 0.5277101060129562\n",
      "all_preds shape: (212,)\n",
      "all_labels shape: (212,)\n",
      "all_probs shape: (212, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([844])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([219])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.646248643690685\n",
      "Epoch 2, Loss: 0.6079590416179513\n",
      "Epoch 3, Loss: 0.5861787115627864\n",
      "Epoch 4, Loss: 0.5464169270587418\n",
      "Epoch 5, Loss: 0.5509612340972109\n",
      "Epoch 6, Loss: 0.5027742402733497\n",
      "all_preds shape: (219,)\n",
      "all_labels shape: (219,)\n",
      "all_probs shape: (219, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 1, 1]), Shape: torch.Size([844])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([212])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6329376163347712\n",
      "Epoch 2, Loss: 0.6025259483535335\n",
      "Epoch 3, Loss: 0.6175632398083525\n",
      "Epoch 4, Loss: 0.6011909915591186\n",
      "Epoch 5, Loss: 0.5984252393245697\n",
      "Epoch 6, Loss: 0.5796734038388954\n",
      "all_preds shape: (212,)\n",
      "all_labels shape: (212,)\n",
      "all_probs shape: (212, 2)\n",
      "Train labels sample: tensor([1, 1, 1, 0, 0]), Shape: torch.Size([844])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([218])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6519992565209011\n",
      "Epoch 2, Loss: 0.5866158149152432\n",
      "Epoch 3, Loss: 0.6004082178169826\n",
      "Epoch 4, Loss: 0.5666482797208822\n",
      "Epoch 5, Loss: 0.5776947309386056\n",
      "Epoch 6, Loss: 0.5434952597573118\n",
      "all_preds shape: (218,)\n",
      "all_labels shape: (218,)\n",
      "all_probs shape: (218, 2)\n",
      "train_4.Первый жанр без клауз, включающих глаголы и наречия.csv\n",
      "test_4.Первый жанр без клауз, включающих глаголы и наречия.csv\n",
      "Train labels sample: tensor([1, 1, 1, 0, 1]), Shape: torch.Size([824])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([245])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.687083651813177\n",
      "Epoch 2, Loss: 0.592321009016954\n",
      "Epoch 3, Loss: 0.5789497420191765\n",
      "Epoch 4, Loss: 0.5429180075342839\n",
      "Epoch 5, Loss: 0.5463544210562339\n",
      "Epoch 6, Loss: 0.558697848365857\n",
      "all_preds shape: (245,)\n",
      "all_labels shape: (245,)\n",
      "all_probs shape: (245, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 1, 0]), Shape: torch.Size([824])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([236])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6005235996383887\n",
      "Epoch 2, Loss: 0.6001039170301877\n",
      "Epoch 3, Loss: 0.5841427978414756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.5586163808520024\n",
      "Epoch 5, Loss: 0.5520028598033465\n",
      "Epoch 6, Loss: 0.5218573877444634\n",
      "all_preds shape: (236,)\n",
      "all_labels shape: (236,)\n",
      "all_probs shape: (236, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([824])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([235])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6697217117135341\n",
      "Epoch 2, Loss: 0.5872710685317333\n",
      "Epoch 3, Loss: 0.5832298850783935\n",
      "Epoch 4, Loss: 0.5717903662186402\n",
      "Epoch 5, Loss: 0.5388784660742834\n",
      "Epoch 6, Loss: 0.5322133362866365\n",
      "all_preds shape: (235,)\n",
      "all_labels shape: (235,)\n",
      "all_probs shape: (235, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 1]), Shape: torch.Size([824])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([236])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6869259488124114\n",
      "Epoch 2, Loss: 0.5746472724355184\n",
      "Epoch 3, Loss: 0.5591050214492358\n",
      "Epoch 4, Loss: 0.5497727113274428\n",
      "Epoch 5, Loss: 0.5838354694155546\n",
      "Epoch 6, Loss: 0.5343466100211327\n",
      "all_preds shape: (236,)\n",
      "all_labels shape: (236,)\n",
      "all_probs shape: (236, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 0]), Shape: torch.Size([824])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([246])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6163072677759024\n",
      "Epoch 2, Loss: 0.5686365913313168\n",
      "Epoch 3, Loss: 0.5241646285240467\n",
      "Epoch 4, Loss: 0.5380240667324799\n",
      "Epoch 5, Loss: 0.4822352316517096\n",
      "Epoch 6, Loss: 0.512113188035213\n",
      "all_preds shape: (246,)\n",
      "all_labels shape: (246,)\n",
      "all_probs shape: (246, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 1]), Shape: torch.Size([824])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([248])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6571029957670432\n",
      "Epoch 2, Loss: 0.5904642045497894\n",
      "Epoch 3, Loss: 0.5878846536462123\n",
      "Epoch 4, Loss: 0.5442953499463888\n",
      "Epoch 5, Loss: 0.529255113349511\n",
      "Epoch 6, Loss: 0.46190145511466724\n",
      "all_preds shape: (248,)\n",
      "all_labels shape: (248,)\n",
      "all_probs shape: (248, 2)\n",
      "train_5.без клауз, включающих местоимения.txt.csv\n",
      "test_5.без клауз, включающих местоимения.txt.csv\n",
      "Train labels sample: tensor([0, 1, 0, 1, 0]), Shape: torch.Size([879])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([169])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6888052951205861\n",
      "Epoch 2, Loss: 0.6029165728525682\n",
      "Epoch 3, Loss: 0.6172724572095004\n",
      "Epoch 4, Loss: 0.5949341074986891\n",
      "Epoch 5, Loss: 0.5953022729266774\n",
      "Epoch 6, Loss: 0.5621903874657371\n",
      "all_preds shape: (169,)\n",
      "all_labels shape: (169,)\n",
      "all_probs shape: (169, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([879])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([187])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6953922174193642\n",
      "Epoch 2, Loss: 0.620153458551927\n",
      "Epoch 3, Loss: 0.6067058481953361\n",
      "Epoch 4, Loss: 0.5779453082518144\n",
      "Epoch 5, Loss: 0.5444217427210374\n",
      "Epoch 6, Loss: 0.4886128628795797\n",
      "all_preds shape: (187,)\n",
      "all_labels shape: (187,)\n",
      "all_probs shape: (187, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 1, 1]), Shape: torch.Size([879])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([179])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6625620094212619\n",
      "Epoch 2, Loss: 0.603681227835742\n",
      "Epoch 3, Loss: 0.6066697646271099\n",
      "Epoch 4, Loss: 0.5828235534104433\n",
      "Epoch 5, Loss: 0.5812976636669852\n",
      "Epoch 6, Loss: 0.574402590231462\n",
      "all_preds shape: (179,)\n",
      "all_labels shape: (179,)\n",
      "all_probs shape: (179, 2)\n",
      "Train labels sample: tensor([1, 1, 1, 1, 0]), Shape: torch.Size([879])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([166])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6554761317643252\n",
      "Epoch 2, Loss: 0.6081810544837605\n",
      "Epoch 3, Loss: 0.589497924934734\n",
      "Epoch 4, Loss: 0.5879518953236667\n",
      "Epoch 5, Loss: 0.555202679200606\n",
      "Epoch 6, Loss: 0.5085189960219644\n",
      "all_preds shape: (166,)\n",
      "all_labels shape: (166,)\n",
      "all_probs shape: (166, 2)\n",
      "Train labels sample: tensor([1, 1, 1, 1, 0]), Shape: torch.Size([879])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([169])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.7302455392750826\n",
      "Epoch 2, Loss: 0.6282427646897056\n",
      "Epoch 3, Loss: 0.605967459895394\n",
      "Epoch 4, Loss: 0.5952976855364713\n",
      "Epoch 5, Loss: 0.5242754394357855\n",
      "Epoch 6, Loss: 0.49757427627390083\n",
      "all_preds shape: (169,)\n",
      "all_labels shape: (169,)\n",
      "all_probs shape: (169, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 1]), Shape: torch.Size([879])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([169])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.67360609553077\n",
      "Epoch 2, Loss: 0.6274630183523352\n",
      "Epoch 3, Loss: 0.6202976725318216\n",
      "Epoch 4, Loss: 0.63587320338596\n",
      "Epoch 5, Loss: 0.5829207544977014\n",
      "Epoch 6, Loss: 0.5715664245865562\n",
      "all_preds shape: (169,)\n",
      "all_labels shape: (169,)\n",
      "all_probs shape: (169, 2)\n",
      "train_6.без слов функциональных.txt.csv\n",
      "test_6.без слов функциональных.txt.csv\n",
      "Train labels sample: tensor([0, 0, 1, 0, 1]), Shape: torch.Size([947])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([168])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6125536565979322\n",
      "Epoch 2, Loss: 0.5700813355545203\n",
      "Epoch 3, Loss: 0.5339991932113966\n",
      "Epoch 4, Loss: 0.4386932671070099\n",
      "Epoch 5, Loss: 0.45770053565502167\n",
      "Epoch 6, Loss: 0.39566209018230436\n",
      "all_preds shape: (168,)\n",
      "all_labels shape: (168,)\n",
      "all_probs shape: (168, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([947])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([168])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6980085864663124\n",
      "Epoch 2, Loss: 0.6069311420122783\n",
      "Epoch 3, Loss: 0.6051521191994349\n",
      "Epoch 4, Loss: 0.5504856035113335\n",
      "Epoch 5, Loss: 0.5072307522098224\n",
      "Epoch 6, Loss: 0.49593308170636496\n",
      "all_preds shape: (168,)\n",
      "all_labels shape: (168,)\n",
      "all_probs shape: (168, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 1, 0]), Shape: torch.Size([947])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([178])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6566008637348811\n",
      "Epoch 2, Loss: 0.5918504297733307\n",
      "Epoch 3, Loss: 0.556287881731987\n",
      "Epoch 4, Loss: 0.499507558097442\n",
      "Epoch 5, Loss: 0.4966660633683205\n",
      "Epoch 6, Loss: 0.4611112751066685\n",
      "all_preds shape: (178,)\n",
      "all_labels shape: (178,)\n",
      "all_probs shape: (178, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([947])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([179])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6298414011796315\n",
      "Epoch 2, Loss: 0.5834120015303293\n",
      "Epoch 3, Loss: 0.5043108979860942\n",
      "Epoch 4, Loss: 0.48487369194626806\n",
      "Epoch 5, Loss: 0.4312512251238028\n",
      "Epoch 6, Loss: 0.3579826897631089\n",
      "all_preds shape: (179,)\n",
      "all_labels shape: (179,)\n",
      "all_probs shape: (179, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 1]), Shape: torch.Size([947])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([168])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6670212964216868\n",
      "Epoch 2, Loss: 0.5773528282841046\n",
      "Epoch 3, Loss: 0.5625557233889897\n",
      "Epoch 4, Loss: 0.4803802728652954\n",
      "Epoch 5, Loss: 0.46759766042232515\n",
      "Epoch 6, Loss: 0.5231424023707708\n",
      "all_preds shape: (168,)\n",
      "all_labels shape: (168,)\n",
      "all_probs shape: (168, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 1, 0]), Shape: torch.Size([947])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([162])\n",
      "##########################xlnet-base-cased---bert####################\n",
      "Epoch 1, Loss: 0.6341428791483243\n",
      "Epoch 2, Loss: 0.5776366839806238\n",
      "Epoch 3, Loss: 0.5632641921440761\n",
      "Epoch 4, Loss: 0.47837188839912415\n",
      "Epoch 5, Loss: 0.5100474014878273\n",
      "Epoch 6, Loss: 0.4566289732853572\n",
      "all_preds shape: (162,)\n",
      "all_labels shape: (162,)\n",
      "all_probs shape: (162, 2)\n",
      "train_Изъяты лексемы с частотой выше 100.csv\n",
      "test_Изъяты лексемы с частотой выше 100.csv\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([864])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([197])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7142783061221794\n",
      "Epoch 2, Loss: 0.6852405060220648\n",
      "Epoch 3, Loss: 0.679744749157517\n",
      "Epoch 4, Loss: 0.6739353217460491\n",
      "Epoch 5, Loss: 0.6631104195559466\n",
      "Epoch 6, Loss: 0.6357113834884431\n",
      "all_preds shape: (197,)\n",
      "all_labels shape: (197,)\n",
      "all_probs shape: (197, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 0]), Shape: torch.Size([864])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([202])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7165801425774893\n",
      "Epoch 2, Loss: 0.6861058412878601\n",
      "Epoch 3, Loss: 0.6817884500379916\n",
      "Epoch 4, Loss: 0.6429222093688117\n",
      "Epoch 5, Loss: 0.4781847062210242\n",
      "Epoch 6, Loss: 0.156782703957072\n",
      "all_preds shape: (202,)\n",
      "all_labels shape: (202,)\n",
      "all_probs shape: (202, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 1]), Shape: torch.Size([864])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([195])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7340951953773145\n",
      "Epoch 2, Loss: 0.6814800793374026\n",
      "Epoch 3, Loss: 0.671769897143046\n",
      "Epoch 4, Loss: 0.6693569907435665\n",
      "Epoch 5, Loss: 0.6702280165972533\n",
      "Epoch 6, Loss: 0.670003299911817\n",
      "all_preds shape: (195,)\n",
      "all_labels shape: (195,)\n",
      "all_probs shape: (195, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 1, 0]), Shape: torch.Size([864])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([200])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7005754863774335\n",
      "Epoch 2, Loss: 0.6813825457184403\n",
      "Epoch 3, Loss: 0.6586902274025811\n",
      "Epoch 4, Loss: 0.6880043242816571\n",
      "Epoch 5, Loss: 0.6804150762381377\n",
      "Epoch 6, Loss: 0.6669236233940831\n",
      "all_preds shape: (200,)\n",
      "all_labels shape: (200,)\n",
      "all_probs shape: (200, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([864])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([191])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7199179099665748\n",
      "Epoch 2, Loss: 0.678572130424005\n",
      "Epoch 3, Loss: 0.6792740590042539\n",
      "Epoch 4, Loss: 0.6688654764934823\n",
      "Epoch 5, Loss: 0.6715213772323396\n",
      "Epoch 6, Loss: 0.6856354552286642\n",
      "all_preds shape: (191,)\n",
      "all_labels shape: (191,)\n",
      "all_probs shape: (191, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 0]), Shape: torch.Size([864])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([194])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6944325632519193\n",
      "Epoch 2, Loss: 0.5585070513188839\n",
      "Epoch 3, Loss: 0.20395435063444353\n",
      "Epoch 4, Loss: 0.07418106114319353\n",
      "Epoch 5, Loss: 0.002651284485451739\n",
      "Epoch 6, Loss: 0.0007923412054811639\n",
      "all_preds shape: (194,)\n",
      "all_labels shape: (194,)\n",
      "all_probs shape: (194, 2)\n",
      "train_Изъяты лексемы с частотой выше 49.csv\n",
      "test_Изъяты лексемы с частотой выше 49.csv\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([895])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([173])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6882315852812358\n",
      "Epoch 2, Loss: 0.6580970308610371\n",
      "Epoch 3, Loss: 0.5884386638977698\n",
      "Epoch 4, Loss: 0.6711260963763509\n",
      "Epoch 5, Loss: 0.66802949990545\n",
      "Epoch 6, Loss: 0.6623478531837463\n",
      "all_preds shape: (173,)\n",
      "all_labels shape: (173,)\n",
      "all_probs shape: (173, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 1]), Shape: torch.Size([895])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([180])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7078228513044971\n",
      "Epoch 2, Loss: 0.67189400855984\n",
      "Epoch 3, Loss: 0.6406264416873455\n",
      "Epoch 4, Loss: 0.6644475640995162\n",
      "Epoch 5, Loss: 0.6731452909963471\n",
      "Epoch 6, Loss: 0.6616034513073308\n",
      "all_preds shape: (180,)\n",
      "all_labels shape: (180,)\n",
      "all_probs shape: (180, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([895])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([184])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6931822938578469\n",
      "Epoch 2, Loss: 0.6541706323623657\n",
      "Epoch 3, Loss: 0.6576956106083733\n",
      "Epoch 4, Loss: 0.5154222830065659\n",
      "Epoch 5, Loss: 0.19943661660155548\n",
      "Epoch 6, Loss: 0.048637647348056944\n",
      "all_preds shape: (184,)\n",
      "all_labels shape: (184,)\n",
      "all_probs shape: (184, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([895])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([187])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.746472144233329\n",
      "Epoch 2, Loss: 0.6765729880758694\n",
      "Epoch 3, Loss: 0.681508236697742\n",
      "Epoch 4, Loss: 0.6715088146073478\n",
      "Epoch 5, Loss: 0.6753881706723145\n",
      "Epoch 6, Loss: 0.6702789536544255\n",
      "all_preds shape: (187,)\n",
      "all_labels shape: (187,)\n",
      "all_probs shape: (187, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 1]), Shape: torch.Size([895])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([182])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6924576652901513\n",
      "Epoch 2, Loss: 0.6859963041331086\n",
      "Epoch 3, Loss: 0.6601648224251611\n",
      "Epoch 4, Loss: 0.6513050943613052\n",
      "Epoch 5, Loss: 0.6211706157773733\n",
      "Epoch 6, Loss: 0.5586330060447965\n",
      "all_preds shape: (182,)\n",
      "all_labels shape: (182,)\n",
      "all_probs shape: (182, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 1]), Shape: torch.Size([895])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([178])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6957762310547488\n",
      "Epoch 2, Loss: 0.608570982036846\n",
      "Epoch 3, Loss: 0.34113126836850177\n",
      "Epoch 4, Loss: 0.10855535577034711\n",
      "Epoch 5, Loss: 0.03251349644519256\n",
      "Epoch 6, Loss: 0.030555128186408962\n",
      "all_preds shape: (178,)\n",
      "all_labels shape: (178,)\n",
      "all_probs shape: (178, 2)\n",
      "train_Изъяты лексемы с частотой выше 29.csv\n",
      "test_Изъяты лексемы с частотой выше 29.csv\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([881])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([192])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6429120951465198\n",
      "Epoch 2, Loss: 0.523864875680634\n",
      "Epoch 3, Loss: 0.1993204737373162\n",
      "Epoch 4, Loss: 0.08440566282037512\n",
      "Epoch 5, Loss: 0.01281643336655439\n",
      "Epoch 6, Loss: 0.0008627963221182913\n",
      "all_preds shape: (192,)\n",
      "all_labels shape: (192,)\n",
      "all_probs shape: (192, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 1, 0]), Shape: torch.Size([881])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([182])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7003777527383396\n",
      "Epoch 2, Loss: 0.659402320427554\n",
      "Epoch 3, Loss: 0.6571946080241885\n",
      "Epoch 4, Loss: 0.6543054378458432\n",
      "Epoch 5, Loss: 0.6358789333275386\n",
      "Epoch 6, Loss: 0.657880827252354\n",
      "all_preds shape: (182,)\n",
      "all_labels shape: (182,)\n",
      "all_probs shape: (182, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([881])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([195])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.664767752800669\n",
      "Epoch 2, Loss: 0.5595539234844702\n",
      "Epoch 3, Loss: 0.31824163598607164\n",
      "Epoch 4, Loss: 0.19210567734470324\n",
      "Epoch 5, Loss: 0.04650915317220746\n",
      "Epoch 6, Loss: 0.0802476423726018\n",
      "all_preds shape: (195,)\n",
      "all_labels shape: (195,)\n",
      "all_probs shape: (195, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 1, 0]), Shape: torch.Size([881])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([197])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.682852107499327\n",
      "Epoch 2, Loss: 0.6737481914460659\n",
      "Epoch 3, Loss: 0.6456972102501563\n",
      "Epoch 4, Loss: 0.6633449558700834\n",
      "Epoch 5, Loss: 0.6718614734709263\n",
      "Epoch 6, Loss: 0.6562790817448071\n",
      "all_preds shape: (197,)\n",
      "all_labels shape: (197,)\n",
      "all_probs shape: (197, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 1, 0]), Shape: torch.Size([881])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([183])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7455167504293578\n",
      "Epoch 2, Loss: 0.6592568558241639\n",
      "Epoch 3, Loss: 0.6709565903459277\n",
      "Epoch 4, Loss: 0.6791516675480774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.6457141228020191\n",
      "Epoch 6, Loss: 0.6761357680495296\n",
      "all_preds shape: (183,)\n",
      "all_labels shape: (183,)\n",
      "all_probs shape: (183, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([881])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([183])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6578858249953815\n",
      "Epoch 2, Loss: 0.6099619157612324\n",
      "Epoch 3, Loss: 0.23927479446865618\n",
      "Epoch 4, Loss: 0.06146304661940251\n",
      "Epoch 5, Loss: 0.020348980122694878\n",
      "Epoch 6, Loss: 0.025865837307979485\n",
      "all_preds shape: (183,)\n",
      "all_labels shape: (183,)\n",
      "all_probs shape: (183, 2)\n",
      "train_Изъяты лексемы с частотой выше 9.csv\n",
      "test_Изъяты лексемы с частотой выше 9.csv\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([758])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([269])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6375305727124214\n",
      "Epoch 2, Loss: 0.637656236688296\n",
      "Epoch 3, Loss: 0.45068345094720524\n",
      "Epoch 4, Loss: 0.3841247024635474\n",
      "all_preds shape: (269,)\n",
      "all_labels shape: (269,)\n",
      "all_probs shape: (269, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 1, 0]), Shape: torch.Size([758])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([272])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7166634971896807\n",
      "Epoch 2, Loss: 0.6660478065411249\n",
      "Epoch 3, Loss: 0.6255660193661848\n",
      "Epoch 4, Loss: 0.5423244858781496\n",
      "all_preds shape: (272,)\n",
      "all_labels shape: (272,)\n",
      "all_probs shape: (272, 2)\n",
      "Train labels sample: tensor([1, 1, 1, 0, 1]), Shape: torch.Size([758])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([260])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6755266661445299\n",
      "Epoch 2, Loss: 0.6243557694057623\n",
      "Epoch 3, Loss: 0.5681486378113428\n",
      "Epoch 4, Loss: 0.3114137292529146\n",
      "all_preds shape: (260,)\n",
      "all_labels shape: (260,)\n",
      "all_probs shape: (260, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 1]), Shape: torch.Size([758])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([252])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7527712633212408\n",
      "Epoch 2, Loss: 0.671191876133283\n",
      "Epoch 3, Loss: 0.6545715282360712\n",
      "Epoch 4, Loss: 0.6574176127711931\n",
      "all_preds shape: (252,)\n",
      "all_labels shape: (252,)\n",
      "all_probs shape: (252, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 0]), Shape: torch.Size([758])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([265])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6802517275015513\n",
      "Epoch 2, Loss: 0.6284842553238074\n",
      "Epoch 3, Loss: 0.5606359628339609\n",
      "Epoch 4, Loss: 0.34312872992207605\n",
      "all_preds shape: (265,)\n",
      "all_labels shape: (265,)\n",
      "all_probs shape: (265, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([758])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([257])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7112311025460561\n",
      "Epoch 2, Loss: 0.6446544354160627\n",
      "Epoch 3, Loss: 0.6253525440891584\n",
      "Epoch 4, Loss: 0.5612494399150213\n",
      "all_preds shape: (257,)\n",
      "all_labels shape: (257,)\n",
      "all_probs shape: (257, 2)\n",
      "train_Изъяты лексемы с частотой выше 5.csv\n",
      "test_Изъяты лексемы с частотой выше 5.csv\n",
      "Train labels sample: tensor([1, 0, 0, 1, 0]), Shape: torch.Size([95])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([533])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7939942280451456\n",
      "Epoch 2, Loss: 0.5993863741556803\n",
      "Epoch 3, Loss: 0.5892315804958344\n",
      "Epoch 4, Loss: 0.5724513381719589\n",
      "Epoch 5, Loss: 0.5809919933478037\n",
      "Epoch 6, Loss: 0.5924281775951385\n",
      "all_preds shape: (533,)\n",
      "all_labels shape: (533,)\n",
      "all_probs shape: (533, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([95])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([548])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7931550939877828\n",
      "Epoch 2, Loss: 0.5658343533674876\n",
      "Epoch 3, Loss: 0.5365098963181177\n",
      "Epoch 4, Loss: 0.5130056291818619\n",
      "Epoch 5, Loss: 0.5090000828107198\n",
      "Epoch 6, Loss: 0.5055994838476181\n",
      "all_preds shape: (548,)\n",
      "all_labels shape: (548,)\n",
      "all_probs shape: (548, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([95])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([438])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.8171204030513763\n",
      "Epoch 2, Loss: 0.6405917406082153\n",
      "Epoch 3, Loss: 0.5777135143677393\n",
      "Epoch 4, Loss: 0.5855892101923624\n",
      "Epoch 5, Loss: 0.5986072520414988\n",
      "Epoch 6, Loss: 0.5791802704334259\n",
      "all_preds shape: (438,)\n",
      "all_labels shape: (438,)\n",
      "all_probs shape: (438, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([95])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([514])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7394316792488098\n",
      "Epoch 2, Loss: 0.5332700163125992\n",
      "Epoch 3, Loss: 0.5765956342220306\n",
      "Epoch 4, Loss: 0.5394121160109838\n",
      "Epoch 5, Loss: 0.554456447561582\n",
      "Epoch 6, Loss: 0.5726177344719569\n",
      "all_preds shape: (514,)\n",
      "all_labels shape: (514,)\n",
      "all_probs shape: (514, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([95])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([478])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7616844773292542\n",
      "Epoch 2, Loss: 0.6419923553864161\n",
      "Epoch 3, Loss: 0.6401148239771525\n",
      "Epoch 4, Loss: 0.5994542588790258\n",
      "Epoch 5, Loss: 0.5787886083126068\n",
      "Epoch 6, Loss: 0.5833148807287216\n",
      "all_preds shape: (478,)\n",
      "all_labels shape: (478,)\n",
      "all_probs shape: (478, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 0]), Shape: torch.Size([95])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([428])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.8212997317314148\n",
      "Epoch 2, Loss: 0.601655845840772\n",
      "Epoch 3, Loss: 0.5960892339547476\n",
      "Epoch 4, Loss: 0.5897271633148193\n",
      "Epoch 5, Loss: 0.596077581246694\n",
      "Epoch 6, Loss: 0.5940434535344442\n",
      "all_preds shape: (428,)\n",
      "all_labels shape: (428,)\n",
      "all_probs shape: (428, 2)\n",
      "train_Изъяты лексемы с частотой выше 3.csv\n",
      "test_Изъяты лексемы с частотой выше 3.csv\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([89])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([765])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.8098801076412201\n",
      "Epoch 2, Loss: 0.557584618528684\n",
      "Epoch 3, Loss: 0.538188248872757\n",
      "Epoch 4, Loss: 0.5062384208043417\n",
      "Epoch 5, Loss: 0.5071801394224167\n",
      "Epoch 6, Loss: 0.5233208934466044\n",
      "all_preds shape: (765,)\n",
      "all_labels shape: (765,)\n",
      "all_probs shape: (765, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([89])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([733])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6363156189521154\n",
      "Epoch 2, Loss: 0.6107103526592255\n",
      "Epoch 3, Loss: 0.5431857158740362\n",
      "Epoch 4, Loss: 0.5003540019194285\n",
      "Epoch 5, Loss: 0.5324170837799708\n",
      "Epoch 6, Loss: 0.5200249999761581\n",
      "all_preds shape: (733,)\n",
      "all_labels shape: (733,)\n",
      "all_probs shape: (733, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([89])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([744])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.8634299139181772\n",
      "Epoch 2, Loss: 0.7082874129215876\n",
      "Epoch 3, Loss: 0.6287581225236257\n",
      "Epoch 4, Loss: 0.6000240991512934\n",
      "Epoch 5, Loss: 0.6351842880249023\n",
      "Epoch 6, Loss: 0.6260806620121002\n",
      "all_preds shape: (744,)\n",
      "all_labels shape: (744,)\n",
      "all_probs shape: (744, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([89])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([749])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7453615367412567\n",
      "Epoch 2, Loss: 0.5149628967046738\n",
      "Epoch 3, Loss: 0.5237471908330917\n",
      "Epoch 4, Loss: 0.48820723593235016\n",
      "Epoch 5, Loss: 0.5443011869986852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.5260248432556788\n",
      "all_preds shape: (749,)\n",
      "all_labels shape: (749,)\n",
      "all_probs shape: (749, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([89])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([738])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6212900429964066\n",
      "Epoch 2, Loss: 0.6653060615062714\n",
      "Epoch 3, Loss: 0.5696903566519419\n",
      "Epoch 4, Loss: 0.6071095565954844\n",
      "Epoch 5, Loss: 0.5801364878813425\n",
      "Epoch 6, Loss: 0.571234275897344\n",
      "all_preds shape: (738,)\n",
      "all_labels shape: (738,)\n",
      "all_probs shape: (738, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([89])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([696])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.9736333588759104\n",
      "Epoch 2, Loss: 0.5895921116073927\n",
      "Epoch 3, Loss: 0.5566759904225668\n",
      "Epoch 4, Loss: 0.5552020271619161\n",
      "Epoch 5, Loss: 0.5933085729678472\n",
      "Epoch 6, Loss: 0.5391415655612946\n",
      "all_preds shape: (696,)\n",
      "all_labels shape: (696,)\n",
      "all_probs shape: (696, 2)\n",
      "train_Без прилагательных первый-второй жанр.csv\n",
      "test_Без прилагательных первый-второй жанр.csv\n",
      "Train labels sample: tensor([0, 0, 1, 0, 1]), Shape: torch.Size([833])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([175])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7358749553009316\n",
      "Epoch 2, Loss: 0.6565970226570412\n",
      "Epoch 3, Loss: 0.5538605664063383\n",
      "Epoch 4, Loss: 0.4041194082755182\n",
      "all_preds shape: (175,)\n",
      "all_labels shape: (175,)\n",
      "all_probs shape: (175, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 1]), Shape: torch.Size([833])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([190])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6489312554951068\n",
      "Epoch 2, Loss: 0.463671881015654\n",
      "Epoch 3, Loss: 0.32343168639474446\n",
      "Epoch 4, Loss: 0.8614481599242599\n",
      "all_preds shape: (190,)\n",
      "all_labels shape: (190,)\n",
      "all_probs shape: (190, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([833])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([188])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7463656376909327\n",
      "Epoch 2, Loss: 0.6801518025221648\n",
      "Epoch 3, Loss: 0.616134083657353\n",
      "Epoch 4, Loss: 0.3798680401372689\n",
      "all_preds shape: (188,)\n",
      "all_labels shape: (188,)\n",
      "all_probs shape: (188, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 0]), Shape: torch.Size([833])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([169])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7239129410849677\n",
      "Epoch 2, Loss: 0.6563155165425053\n",
      "Epoch 3, Loss: 0.5163440279386662\n",
      "Epoch 4, Loss: 0.23403487040626783\n",
      "all_preds shape: (169,)\n",
      "all_labels shape: (169,)\n",
      "all_probs shape: (169, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 1, 1]), Shape: torch.Size([833])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([170])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7245880100462172\n",
      "Epoch 2, Loss: 0.6020716892348396\n",
      "Epoch 3, Loss: 0.465798893460521\n",
      "Epoch 4, Loss: 0.1972565966554814\n",
      "all_preds shape: (170,)\n",
      "all_labels shape: (170,)\n",
      "all_probs shape: (170, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([833])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([175])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7679707496254532\n",
      "Epoch 2, Loss: 0.6456939125502551\n",
      "Epoch 3, Loss: 0.5069376373732531\n",
      "Epoch 4, Loss: 0.28471455236689913\n",
      "all_preds shape: (175,)\n",
      "all_labels shape: (175,)\n",
      "all_probs shape: (175, 2)\n",
      "train_1.Первый жанр исходная выборка.csv\n",
      "test_1.Первый жанр исходная выборка.csv\n",
      "Train labels sample: tensor([0, 0, 1, 1, 1]), Shape: torch.Size([884])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([167])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7235386946371624\n",
      "Epoch 2, Loss: 0.6739764362573624\n",
      "Epoch 3, Loss: 0.6599457253302846\n",
      "Epoch 4, Loss: 0.66705685960395\n",
      "Epoch 5, Loss: 0.6758565886744431\n",
      "Epoch 6, Loss: 0.6645885030073779\n",
      "all_preds shape: (167,)\n",
      "all_labels shape: (167,)\n",
      "all_probs shape: (167, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([884])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([164])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6757361314126423\n",
      "Epoch 2, Loss: 0.5533564906301243\n",
      "Epoch 3, Loss: 0.20407201213363027\n",
      "Epoch 4, Loss: 0.08881302903007184\n",
      "Epoch 5, Loss: 0.031093645109129802\n",
      "Epoch 6, Loss: 0.009867217154741021\n",
      "all_preds shape: (164,)\n",
      "all_labels shape: (164,)\n",
      "all_probs shape: (164, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([884])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([166])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7034994193485805\n",
      "Epoch 2, Loss: 0.6615962934281144\n",
      "Epoch 3, Loss: 0.6706788358943803\n",
      "Epoch 4, Loss: 0.6676633240921157\n",
      "Epoch 5, Loss: 0.6746315541011947\n",
      "Epoch 6, Loss: 0.6701974283371653\n",
      "all_preds shape: (166,)\n",
      "all_labels shape: (166,)\n",
      "all_probs shape: (166, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 1, 1]), Shape: torch.Size([884])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([162])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7020064517855644\n",
      "Epoch 2, Loss: 0.7047989602599826\n",
      "Epoch 3, Loss: 0.6675472025360379\n",
      "Epoch 4, Loss: 0.4103824845348884\n",
      "Epoch 5, Loss: 0.1382601888035424\n",
      "Epoch 6, Loss: 0.027168671874928156\n",
      "all_preds shape: (162,)\n",
      "all_labels shape: (162,)\n",
      "all_probs shape: (162, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 0]), Shape: torch.Size([884])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([157])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7138091421553067\n",
      "Epoch 2, Loss: 0.6827795106385436\n",
      "Epoch 3, Loss: 0.49971436782340917\n",
      "Epoch 4, Loss: 0.14850648709606112\n",
      "Epoch 5, Loss: 0.05450106994456811\n",
      "Epoch 6, Loss: 0.027259186708501408\n",
      "all_preds shape: (157,)\n",
      "all_labels shape: (157,)\n",
      "all_probs shape: (157, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 1]), Shape: torch.Size([884])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([161])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.731212032692773\n",
      "Epoch 2, Loss: 0.6784108089549201\n",
      "Epoch 3, Loss: 0.6803119629621506\n",
      "Epoch 4, Loss: 0.6782462974744183\n",
      "Epoch 5, Loss: 0.6963522487453052\n",
      "Epoch 6, Loss: 0.6836433932185173\n",
      "all_preds shape: (161,)\n",
      "all_labels shape: (161,)\n",
      "all_probs shape: (161, 2)\n",
      "train_2.Первый жанр без клауз, включающих наречия.csv\n",
      "test_2.Первый жанр без клауз, включающих наречия.csv\n",
      "Train labels sample: tensor([1, 1, 1, 0, 0]), Shape: torch.Size([871])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([179])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7094603516838768\n",
      "Epoch 2, Loss: 0.6947125662456859\n",
      "Epoch 3, Loss: 0.6667828787456859\n",
      "Epoch 4, Loss: 0.6666362285614014\n",
      "Epoch 5, Loss: 0.6907972839745609\n",
      "Epoch 6, Loss: 0.6646130366758867\n",
      "all_preds shape: (179,)\n",
      "all_labels shape: (179,)\n",
      "all_probs shape: (179, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([871])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([171])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7051973364569925\n",
      "Epoch 2, Loss: 0.6734299356287176\n",
      "Epoch 3, Loss: 0.6667443985288793\n",
      "Epoch 4, Loss: 0.6871030699123036\n",
      "Epoch 5, Loss: 0.6771721568974581\n",
      "Epoch 6, Loss: 0.6795760935003107\n",
      "all_preds shape: (171,)\n",
      "all_labels shape: (171,)\n",
      "all_probs shape: (171, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 1]), Shape: torch.Size([871])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([168])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6997936205430464\n",
      "Epoch 2, Loss: 0.5160031538118016\n",
      "Epoch 3, Loss: 0.16490419214930047\n",
      "Epoch 4, Loss: 0.030579028992955996\n",
      "Epoch 5, Loss: 0.03267705928830599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.023819439444394613\n",
      "all_preds shape: (168,)\n",
      "all_labels shape: (168,)\n",
      "all_probs shape: (168, 2)\n",
      "Train labels sample: tensor([1, 1, 1, 0, 1]), Shape: torch.Size([871])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([178])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7517277798869393\n",
      "Epoch 2, Loss: 0.6814377443356947\n",
      "Epoch 3, Loss: 0.6766655412587252\n",
      "Epoch 4, Loss: 0.6815132363276049\n",
      "Epoch 5, Loss: 0.6791785305196588\n",
      "Epoch 6, Loss: 0.6844781788912686\n",
      "all_preds shape: (178,)\n",
      "all_labels shape: (178,)\n",
      "all_probs shape: (178, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([871])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([166])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7327381003986705\n",
      "Epoch 2, Loss: 0.6946128904819489\n",
      "Epoch 3, Loss: 0.6815823446620595\n",
      "Epoch 4, Loss: 0.6732229666276411\n",
      "Epoch 5, Loss: 0.677393021366813\n",
      "Epoch 6, Loss: 0.6904483031142842\n",
      "all_preds shape: (166,)\n",
      "all_labels shape: (166,)\n",
      "all_probs shape: (166, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([871])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([170])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6979804039001465\n",
      "Epoch 2, Loss: 0.6432743473486466\n",
      "Epoch 3, Loss: 0.6309906479309906\n",
      "Epoch 4, Loss: 0.6851589116183194\n",
      "Epoch 5, Loss: 0.6815723126584833\n",
      "Epoch 6, Loss: 0.6733750354159962\n",
      "all_preds shape: (170,)\n",
      "all_labels shape: (170,)\n",
      "all_probs shape: (170, 2)\n",
      "train_3.Первый жанр без клауз, включающих глаголы.csv\n",
      "test_3.Первый жанр без клауз, включающих глаголы.csv\n",
      "Train labels sample: tensor([1, 0, 0, 0, 0]), Shape: torch.Size([816])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([215])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.772694949425903\n",
      "Epoch 2, Loss: 0.6853771005191055\n",
      "Epoch 3, Loss: 0.6799441459132176\n",
      "Epoch 4, Loss: 0.6757017064328287\n",
      "Epoch 5, Loss: 0.6829157489187577\n",
      "Epoch 6, Loss: 0.682098925113678\n",
      "all_preds shape: (215,)\n",
      "all_labels shape: (215,)\n",
      "all_probs shape: (215, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([816])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([211])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7619188509735406\n",
      "Epoch 2, Loss: 0.6866904824387794\n",
      "Epoch 3, Loss: 0.6871866373454824\n",
      "Epoch 4, Loss: 0.6933890113643572\n",
      "Epoch 5, Loss: 0.6919096883605508\n",
      "Epoch 6, Loss: 0.6878748512735554\n",
      "all_preds shape: (211,)\n",
      "all_labels shape: (211,)\n",
      "all_probs shape: (211, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 1]), Shape: torch.Size([816])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([206])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.733600981095258\n",
      "Epoch 2, Loss: 0.6806157640382355\n",
      "Epoch 3, Loss: 0.6818036139011383\n",
      "Epoch 4, Loss: 0.6906511427140704\n",
      "Epoch 5, Loss: 0.6882116093355066\n",
      "Epoch 6, Loss: 0.6936426980822694\n",
      "all_preds shape: (206,)\n",
      "all_labels shape: (206,)\n",
      "all_probs shape: (206, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([816])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([225])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7253384531713\n",
      "Epoch 2, Loss: 0.6945671894971062\n",
      "Epoch 3, Loss: 0.6983805783823425\n",
      "Epoch 4, Loss: 0.6845200599408617\n",
      "Epoch 5, Loss: 0.6183051531221352\n",
      "Epoch 6, Loss: 0.6155115374747444\n",
      "all_preds shape: (225,)\n",
      "all_labels shape: (225,)\n",
      "all_probs shape: (225, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([816])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([211])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.704460042364457\n",
      "Epoch 2, Loss: 0.672396251968309\n",
      "Epoch 3, Loss: 0.7046220127274009\n",
      "Epoch 4, Loss: 0.6870069457035438\n",
      "Epoch 5, Loss: 0.6808921461011849\n",
      "Epoch 6, Loss: 0.6914771643339419\n",
      "all_preds shape: (211,)\n",
      "all_labels shape: (211,)\n",
      "all_probs shape: (211, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 1]), Shape: torch.Size([816])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([212])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.719719385399538\n",
      "Epoch 2, Loss: 0.6847526138904048\n",
      "Epoch 3, Loss: 0.6361249545041252\n",
      "Epoch 4, Loss: 0.2540673592000031\n",
      "Epoch 5, Loss: 0.03859835415713343\n",
      "Epoch 6, Loss: 0.09431527070609816\n",
      "all_preds shape: (212,)\n",
      "all_labels shape: (212,)\n",
      "all_probs shape: (212, 2)\n",
      "train_4.Первый жанр без клауз, включающих глаголы и наречия.csv\n",
      "test_4.Первый жанр без клауз, включающих глаголы и наречия.csv\n",
      "Train labels sample: tensor([1, 0, 1, 0, 1]), Shape: torch.Size([797])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([239])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7350989949703216\n",
      "Epoch 2, Loss: 0.6922190511226654\n",
      "Epoch 3, Loss: 0.6988270354270935\n",
      "Epoch 4, Loss: 0.6730114412307739\n",
      "Epoch 5, Loss: 0.6897203874588013\n",
      "Epoch 6, Loss: 0.68956827044487\n",
      "all_preds shape: (239,)\n",
      "all_labels shape: (239,)\n",
      "all_probs shape: (239, 2)\n",
      "Train labels sample: tensor([1, 1, 1, 0, 1]), Shape: torch.Size([797])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([226])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7169250679016114\n",
      "Epoch 2, Loss: 0.680228453874588\n",
      "Epoch 3, Loss: 0.6925086426734924\n",
      "Epoch 4, Loss: 0.6937189519405365\n",
      "Epoch 5, Loss: 0.6392976868152619\n",
      "Epoch 6, Loss: 0.14393326542340218\n",
      "all_preds shape: (226,)\n",
      "all_labels shape: (226,)\n",
      "all_probs shape: (226, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([797])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([235])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7357905197143555\n",
      "Epoch 2, Loss: 0.693448976278305\n",
      "Epoch 3, Loss: 0.5357786035537719\n",
      "Epoch 4, Loss: 0.14203866261057554\n",
      "Epoch 5, Loss: 0.03924629634711892\n",
      "Epoch 6, Loss: 0.2894388921931386\n",
      "all_preds shape: (235,)\n",
      "all_labels shape: (235,)\n",
      "all_probs shape: (235, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([797])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([239])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7620555818080902\n",
      "Epoch 2, Loss: 0.7049761116504669\n",
      "Epoch 3, Loss: 0.7213913011550903\n",
      "Epoch 4, Loss: 0.7029318487644196\n",
      "Epoch 5, Loss: 0.6729853975772858\n",
      "Epoch 6, Loss: 0.676717324256897\n",
      "all_preds shape: (239,)\n",
      "all_labels shape: (239,)\n",
      "all_probs shape: (239, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 1, 0]), Shape: torch.Size([797])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([237])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.718636872768402\n",
      "Epoch 2, Loss: 0.48633121713995936\n",
      "Epoch 3, Loss: 0.11490339545998722\n",
      "Epoch 4, Loss: 0.13610539629124105\n",
      "Epoch 5, Loss: 0.04313764472492039\n",
      "Epoch 6, Loss: 0.011801700452342629\n",
      "all_preds shape: (237,)\n",
      "all_labels shape: (237,)\n",
      "all_probs shape: (237, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 1, 1]), Shape: torch.Size([797])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([242])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7485455310344696\n",
      "Epoch 2, Loss: 0.692473417520523\n",
      "Epoch 3, Loss: 0.6998752772808075\n",
      "Epoch 4, Loss: 0.4298561516404152\n",
      "Epoch 5, Loss: 0.0683295197179541\n",
      "Epoch 6, Loss: 0.021150322270113976\n",
      "all_preds shape: (242,)\n",
      "all_labels shape: (242,)\n",
      "all_probs shape: (242, 2)\n",
      "train_5.без клауз, включающих местоимения.txt.csv\n",
      "test_5.без клауз, включающих местоимения.txt.csv\n",
      "Train labels sample: tensor([1, 0, 0, 0, 1]), Shape: torch.Size([849])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([201])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7290725476211972\n",
      "Epoch 2, Loss: 0.7032987376054128\n",
      "Epoch 3, Loss: 0.669492573649795\n",
      "Epoch 4, Loss: 0.6719644770578102\n",
      "Epoch 5, Loss: 0.6842240735336587\n",
      "Epoch 6, Loss: 0.7072990039984385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_preds shape: (201,)\n",
      "all_labels shape: (201,)\n",
      "all_probs shape: (201, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 1]), Shape: torch.Size([849])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([185])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7214102750575101\n",
      "Epoch 2, Loss: 0.7109976907571157\n",
      "Epoch 3, Loss: 0.6567222641574012\n",
      "Epoch 4, Loss: 0.4448435585255976\n",
      "Epoch 5, Loss: 0.1324352768887938\n",
      "Epoch 6, Loss: 0.27596781635656953\n",
      "all_preds shape: (185,)\n",
      "all_labels shape: (185,)\n",
      "all_probs shape: (185, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 0]), Shape: torch.Size([849])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([196])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6885814628115406\n",
      "Epoch 2, Loss: 0.6024486941319925\n",
      "Epoch 3, Loss: 0.3065876512857223\n",
      "Epoch 4, Loss: 0.48792478690544766\n",
      "Epoch 5, Loss: 0.06720333811999471\n",
      "Epoch 6, Loss: 0.03784529442243554\n",
      "all_preds shape: (196,)\n",
      "all_labels shape: (196,)\n",
      "all_probs shape: (196, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 1, 0]), Shape: torch.Size([849])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([187])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7521952986717224\n",
      "Epoch 2, Loss: 0.7000958213099727\n",
      "Epoch 3, Loss: 0.6844261316237626\n",
      "Epoch 4, Loss: 0.6905141903294457\n",
      "Epoch 5, Loss: 0.6887185363857834\n",
      "Epoch 6, Loss: 0.6899949566081718\n",
      "all_preds shape: (187,)\n",
      "all_labels shape: (187,)\n",
      "all_probs shape: (187, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 1]), Shape: torch.Size([849])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([181])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7210860213747731\n",
      "Epoch 2, Loss: 0.665753463352168\n",
      "Epoch 3, Loss: 0.6041620655192269\n",
      "Epoch 4, Loss: 0.6897129780716367\n",
      "Epoch 5, Loss: 0.7017026378048791\n",
      "Epoch 6, Loss: 0.683973941538069\n",
      "all_preds shape: (181,)\n",
      "all_labels shape: (181,)\n",
      "all_probs shape: (181, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([849])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([190])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7215791212187873\n",
      "Epoch 2, Loss: 0.6824311724415532\n",
      "Epoch 3, Loss: 0.4921005751799654\n",
      "Epoch 4, Loss: 0.20889895719786486\n",
      "Epoch 5, Loss: 0.10865403704897121\n",
      "Epoch 6, Loss: 0.048149515766891894\n",
      "all_preds shape: (190,)\n",
      "all_labels shape: (190,)\n",
      "all_probs shape: (190, 2)\n",
      "train_6.без слов функциональных.txt.csv\n",
      "test_6.без слов функциональных.txt.csv\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([927])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([169])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6160386143059566\n",
      "Epoch 2, Loss: 0.20379996241937423\n",
      "Epoch 3, Loss: 0.07720316333519615\n",
      "Epoch 4, Loss: 0.07166478957916642\n",
      "Epoch 5, Loss: 0.08265121830306177\n",
      "Epoch 6, Loss: 0.03508939234547895\n",
      "all_preds shape: (169,)\n",
      "all_labels shape: (169,)\n",
      "all_probs shape: (169, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 1]), Shape: torch.Size([927])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([178])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6304001268641702\n",
      "Epoch 2, Loss: 0.42491962245248\n",
      "Epoch 3, Loss: 0.2852806204384\n",
      "Epoch 4, Loss: 0.1255295218675044\n",
      "Epoch 5, Loss: 0.0850540501113724\n",
      "Epoch 6, Loss: 0.08710315614408845\n",
      "all_preds shape: (178,)\n",
      "all_labels shape: (178,)\n",
      "all_probs shape: (178, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 1, 0]), Shape: torch.Size([927])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([174])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.5639907093718648\n",
      "Epoch 2, Loss: 0.13578023845008735\n",
      "Epoch 3, Loss: 0.11693389790839162\n",
      "Epoch 4, Loss: 0.1253817879940094\n",
      "Epoch 5, Loss: 0.02493944660969207\n",
      "Epoch 6, Loss: 0.01929007502320897\n",
      "all_preds shape: (174,)\n",
      "all_labels shape: (174,)\n",
      "all_probs shape: (174, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([927])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([185])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7034956802581919\n",
      "Epoch 2, Loss: 0.5067040112254948\n",
      "Epoch 3, Loss: 0.15416073566302657\n",
      "Epoch 4, Loss: 0.07843273687818698\n",
      "Epoch 5, Loss: 0.058590724883633184\n",
      "Epoch 6, Loss: 0.008668969268910587\n",
      "all_preds shape: (185,)\n",
      "all_labels shape: (185,)\n",
      "all_probs shape: (185, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 1]), Shape: torch.Size([927])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([182])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.6789420329291245\n",
      "Epoch 2, Loss: 0.2423342614241972\n",
      "Epoch 3, Loss: 0.08681658737297202\n",
      "Epoch 4, Loss: 0.05592456009993651\n",
      "Epoch 5, Loss: 0.1264180403026142\n",
      "Epoch 6, Loss: 0.10790549557463362\n",
      "all_preds shape: (182,)\n",
      "all_labels shape: (182,)\n",
      "all_probs shape: (182, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([927])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([175])\n",
      "##########################xlm-roberta-base---bert####################\n",
      "Epoch 1, Loss: 0.7162313831263575\n",
      "Epoch 2, Loss: 0.6869888434122349\n",
      "Epoch 3, Loss: 0.36047993047997867\n",
      "Epoch 4, Loss: 0.11757701221082745\n",
      "Epoch 5, Loss: 0.17434523338130836\n",
      "Epoch 6, Loss: 0.09212983613191493\n",
      "all_preds shape: (175,)\n",
      "all_labels shape: (175,)\n",
      "all_probs shape: (175, 2)\n",
      "train_Изъяты лексемы с частотой выше 100.csv\n",
      "test_Изъяты лексемы с частотой выше 100.csv\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([876])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([205])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.6089116511019793\n",
      "Epoch 2, Loss: 0.3904097451405092\n",
      "Epoch 3, Loss: 0.2871728889644146\n",
      "Epoch 4, Loss: 0.2030788351866332\n",
      "Epoch 5, Loss: 0.21721954237331043\n",
      "Epoch 6, Loss: 0.13336565127088265\n",
      "all_preds shape: (205,)\n",
      "all_labels shape: (205,)\n",
      "all_probs shape: (205, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([876])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([210])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.681169485503977\n",
      "Epoch 2, Loss: 0.36661965413527053\n",
      "Epoch 3, Loss: 0.28970129801468414\n",
      "Epoch 4, Loss: 0.19150633281096816\n",
      "Epoch 5, Loss: 0.22304241264408284\n",
      "Epoch 6, Loss: 0.13982920095493848\n",
      "all_preds shape: (210,)\n",
      "all_labels shape: (210,)\n",
      "all_probs shape: (210, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([876])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([190])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.5750255351716822\n",
      "Epoch 2, Loss: 0.3665978849611499\n",
      "Epoch 3, Loss: 0.2551528921858831\n",
      "Epoch 4, Loss: 0.1854779107326811\n",
      "Epoch 5, Loss: 0.1312711140140891\n",
      "Epoch 6, Loss: 0.10666052067483013\n",
      "all_preds shape: (190,)\n",
      "all_labels shape: (190,)\n",
      "all_probs shape: (190, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([876])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([194])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.5585683421655134\n",
      "Epoch 2, Loss: 0.3432002465833317\n",
      "Epoch 3, Loss: 0.22443806840614838\n",
      "Epoch 4, Loss: 0.18159018206325445\n",
      "Epoch 5, Loss: 0.15359510418704964\n",
      "Epoch 6, Loss: 0.10918404177169908\n",
      "all_preds shape: (194,)\n",
      "all_labels shape: (194,)\n",
      "all_probs shape: (194, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 1]), Shape: torch.Size([876])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([190])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.6189159073612907\n",
      "Epoch 2, Loss: 0.423792350563136\n",
      "Epoch 3, Loss: 0.25277805734764447\n",
      "Epoch 4, Loss: 0.19462319409305398\n",
      "Epoch 5, Loss: 0.1688294999470765\n",
      "Epoch 6, Loss: 0.15857730148868127\n",
      "all_preds shape: (190,)\n",
      "all_labels shape: (190,)\n",
      "all_probs shape: (190, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 1, 0]), Shape: torch.Size([876])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([187])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.581056394089352\n",
      "Epoch 2, Loss: 0.35611923933029177\n",
      "Epoch 3, Loss: 0.220822553912347\n",
      "Epoch 4, Loss: 0.16513973130759868\n",
      "Epoch 5, Loss: 0.1222977440550246\n",
      "Epoch 6, Loss: 0.09145781552757729\n",
      "all_preds shape: (187,)\n",
      "all_labels shape: (187,)\n",
      "all_probs shape: (187, 2)\n",
      "train_Изъяты лексемы с частотой выше 49.csv\n",
      "test_Изъяты лексемы с частотой выше 49.csv\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([905])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([186])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.7435906675823948\n",
      "Epoch 2, Loss: 0.48228677322990016\n",
      "Epoch 3, Loss: 0.378063341766073\n",
      "Epoch 4, Loss: 0.2822676259174682\n",
      "Epoch 5, Loss: 0.2302122295817785\n",
      "Epoch 6, Loss: 0.18113249569739165\n",
      "all_preds shape: (186,)\n",
      "all_labels shape: (186,)\n",
      "all_probs shape: (186, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 1, 1]), Shape: torch.Size([905])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([193])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.5653532053294935\n",
      "Epoch 2, Loss: 0.4173398640072137\n",
      "Epoch 3, Loss: 0.3079133942853986\n",
      "Epoch 4, Loss: 0.2152365302307564\n",
      "Epoch 5, Loss: 0.1641549803643373\n",
      "Epoch 6, Loss: 0.1562156396542202\n",
      "all_preds shape: (193,)\n",
      "all_labels shape: (193,)\n",
      "all_probs shape: (193, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 0]), Shape: torch.Size([905])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([183])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.7804028527778492\n",
      "Epoch 2, Loss: 0.4172901639290023\n",
      "Epoch 3, Loss: 0.29954857509910016\n",
      "Epoch 4, Loss: 0.22674174322501608\n",
      "Epoch 5, Loss: 0.16196091241088875\n",
      "Epoch 6, Loss: 0.13603668312313394\n",
      "all_preds shape: (183,)\n",
      "all_labels shape: (183,)\n",
      "all_probs shape: (183, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 1]), Shape: torch.Size([905])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([196])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.6094965772670612\n",
      "Epoch 2, Loss: 0.43552817351985396\n",
      "Epoch 3, Loss: 0.3171371214353202\n",
      "Epoch 4, Loss: 0.23126522909130967\n",
      "Epoch 5, Loss: 0.19322420736742124\n",
      "Epoch 6, Loss: 0.1349257926799749\n",
      "all_preds shape: (196,)\n",
      "all_labels shape: (196,)\n",
      "all_probs shape: (196, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 1]), Shape: torch.Size([905])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([197])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.6618361075719198\n",
      "Epoch 2, Loss: 0.41671545019275263\n",
      "Epoch 3, Loss: 0.35760278659954403\n",
      "Epoch 4, Loss: 0.24173368304445034\n",
      "Epoch 5, Loss: 0.17683428107646473\n",
      "Epoch 6, Loss: 0.1697098200062388\n",
      "all_preds shape: (197,)\n",
      "all_labels shape: (197,)\n",
      "all_probs shape: (197, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([905])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([191])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.5784473314619901\n",
      "Epoch 2, Loss: 0.4232628118050726\n",
      "Epoch 3, Loss: 0.3346653573476432\n",
      "Epoch 4, Loss: 0.2682894466673596\n",
      "Epoch 5, Loss: 0.20246263789503197\n",
      "Epoch 6, Loss: 0.14990355332561753\n",
      "all_preds shape: (191,)\n",
      "all_labels shape: (191,)\n",
      "all_probs shape: (191, 2)\n",
      "train_Изъяты лексемы с частотой выше 29.csv\n",
      "test_Изъяты лексемы с частотой выше 29.csv\n",
      "Train labels sample: tensor([1, 1, 1, 1, 0]), Shape: torch.Size([892])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([194])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.714003364954676\n",
      "Epoch 2, Loss: 0.5144376081547567\n",
      "Epoch 3, Loss: 0.39955548250249456\n",
      "Epoch 4, Loss: 0.30128264360662016\n",
      "Epoch 5, Loss: 0.21796542213165335\n",
      "Epoch 6, Loss: 0.18255465311397398\n",
      "all_preds shape: (194,)\n",
      "all_labels shape: (194,)\n",
      "all_probs shape: (194, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 1]), Shape: torch.Size([892])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([189])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.5679909884929657\n",
      "Epoch 2, Loss: 0.4063540808856487\n",
      "Epoch 3, Loss: 0.2808783936447331\n",
      "Epoch 4, Loss: 0.23826334475805716\n",
      "Epoch 5, Loss: 0.1882859643415681\n",
      "Epoch 6, Loss: 0.18159602622368506\n",
      "all_preds shape: (189,)\n",
      "all_labels shape: (189,)\n",
      "all_probs shape: (189, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 1]), Shape: torch.Size([892])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([204])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.6018760789717946\n",
      "Epoch 2, Loss: 0.4777656266731875\n",
      "Epoch 3, Loss: 0.36429192099188057\n",
      "Epoch 4, Loss: 0.2461298996183489\n",
      "Epoch 5, Loss: 0.22448570254657949\n",
      "Epoch 6, Loss: 0.17087389641840542\n",
      "all_preds shape: (204,)\n",
      "all_labels shape: (204,)\n",
      "all_probs shape: (204, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([892])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([197])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.5941471058343138\n",
      "Epoch 2, Loss: 0.5028496475092002\n",
      "Epoch 3, Loss: 0.3860023129465325\n",
      "Epoch 4, Loss: 0.31041993560003384\n",
      "Epoch 5, Loss: 0.24031054878806962\n",
      "Epoch 6, Loss: 0.19155864500706749\n",
      "all_preds shape: (197,)\n",
      "all_labels shape: (197,)\n",
      "all_probs shape: (197, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 1]), Shape: torch.Size([892])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([196])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.6157097720674106\n",
      "Epoch 2, Loss: 0.5035391678767545\n",
      "Epoch 3, Loss: 0.3684459073202951\n",
      "Epoch 4, Loss: 0.2877920333828245\n",
      "Epoch 5, Loss: 0.26552867237478495\n",
      "Epoch 6, Loss: 0.2148089138790965\n",
      "all_preds shape: (196,)\n",
      "all_labels shape: (196,)\n",
      "all_probs shape: (196, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 1, 0]), Shape: torch.Size([892])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([207])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.612667614860194\n",
      "Epoch 2, Loss: 0.46473913293864044\n",
      "Epoch 3, Loss: 0.38926212968570845\n",
      "Epoch 4, Loss: 0.2679550894536078\n",
      "Epoch 5, Loss: 0.22790527982371195\n",
      "Epoch 6, Loss: 0.17216589943771915\n",
      "all_preds shape: (207,)\n",
      "all_labels shape: (207,)\n",
      "all_probs shape: (207, 2)\n",
      "train_Изъяты лексемы с частотой выше 9.csv\n",
      "test_Изъяты лексемы с частотой выше 9.csv\n",
      "Train labels sample: tensor([1, 0, 0, 1, 0]), Shape: torch.Size([765])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([335])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.6468884063263735\n",
      "Epoch 2, Loss: 0.5050938899318377\n",
      "Epoch 3, Loss: 0.46034950390458107\n",
      "Epoch 4, Loss: 0.422866765409708\n",
      "all_preds shape: (335,)\n",
      "all_labels shape: (335,)\n",
      "all_probs shape: (335, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 1, 1]), Shape: torch.Size([765])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([348])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 1.004818530132373\n",
      "Epoch 2, Loss: 0.6108026492098967\n",
      "Epoch 3, Loss: 0.5450389621158441\n",
      "Epoch 4, Loss: 0.5014082938432693\n",
      "all_preds shape: (348,)\n",
      "all_labels shape: (348,)\n",
      "all_probs shape: (348, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([765])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([346])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.7433115517099699\n",
      "Epoch 2, Loss: 0.48667702202995616\n",
      "Epoch 3, Loss: 0.4039002290616433\n",
      "Epoch 4, Loss: 0.3817309724787871\n",
      "all_preds shape: (346,)\n",
      "all_labels shape: (346,)\n",
      "all_probs shape: (346, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 1]), Shape: torch.Size([765])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([337])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.7144834746917089\n",
      "Epoch 2, Loss: 0.49904298906524974\n",
      "Epoch 3, Loss: 0.40626272186636925\n",
      "Epoch 4, Loss: 0.3618881280223529\n",
      "all_preds shape: (337,)\n",
      "all_labels shape: (337,)\n",
      "all_probs shape: (337, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 1]), Shape: torch.Size([765])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([346])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.9929381782809893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.5231495959063371\n",
      "Epoch 3, Loss: 0.46190081412593526\n",
      "Epoch 4, Loss: 0.41730905945102376\n",
      "all_preds shape: (346,)\n",
      "all_labels shape: (346,)\n",
      "all_probs shape: (346, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 1]), Shape: torch.Size([765])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([346])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 1.0309131319324176\n",
      "Epoch 2, Loss: 0.573243610560894\n",
      "Epoch 3, Loss: 0.48137328897913295\n",
      "Epoch 4, Loss: 0.4400252625346184\n",
      "all_preds shape: (346,)\n",
      "all_labels shape: (346,)\n",
      "all_probs shape: (346, 2)\n",
      "train_Изъяты лексемы с частотой выше 5.csv\n",
      "test_Изъяты лексемы с частотой выше 5.csv\n",
      "Train labels sample: tensor([1, 0, 0, 1, 0]), Shape: torch.Size([96])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([503])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.6759059031804403\n",
      "Epoch 2, Loss: 0.5345890571673712\n",
      "Epoch 3, Loss: 0.5022849986950556\n",
      "Epoch 4, Loss: 0.4725515792767207\n",
      "Epoch 5, Loss: 0.38918379445870716\n",
      "Epoch 6, Loss: 0.31906913220882416\n",
      "all_preds shape: (503,)\n",
      "all_labels shape: (503,)\n",
      "all_probs shape: (503, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 1]), Shape: torch.Size([96])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([498])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.5484276761611303\n",
      "Epoch 2, Loss: 0.45861123005549115\n",
      "Epoch 3, Loss: 0.3893166755636533\n",
      "Epoch 4, Loss: 0.31768912076950073\n",
      "Epoch 5, Loss: 0.25094863027334213\n",
      "Epoch 6, Loss: 0.16234314565857252\n",
      "all_preds shape: (498,)\n",
      "all_labels shape: (498,)\n",
      "all_probs shape: (498, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([96])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([445])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.9699506511290868\n",
      "Epoch 2, Loss: 0.6798837085564932\n",
      "Epoch 3, Loss: 0.5443068444728851\n",
      "Epoch 4, Loss: 0.5318741748730341\n",
      "Epoch 5, Loss: 0.5027118275562922\n",
      "Epoch 6, Loss: 0.46784045298894245\n",
      "all_preds shape: (445,)\n",
      "all_labels shape: (445,)\n",
      "all_probs shape: (445, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([96])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([498])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.772339661916097\n",
      "Epoch 2, Loss: 0.46179866294066113\n",
      "Epoch 3, Loss: 0.4500209887822469\n",
      "Epoch 4, Loss: 0.38614372660716373\n",
      "Epoch 5, Loss: 0.30614573260148364\n",
      "Epoch 6, Loss: 0.18896864851315817\n",
      "all_preds shape: (498,)\n",
      "all_labels shape: (498,)\n",
      "all_probs shape: (498, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([96])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([463])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 2.167813162008921\n",
      "Epoch 2, Loss: 0.565757135550181\n",
      "Epoch 3, Loss: 0.505588561296463\n",
      "Epoch 4, Loss: 0.4357896496852239\n",
      "Epoch 5, Loss: 0.4422459652026494\n",
      "Epoch 6, Loss: 0.3535284052292506\n",
      "all_preds shape: (463,)\n",
      "all_labels shape: (463,)\n",
      "all_probs shape: (463, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 1]), Shape: torch.Size([96])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([454])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 3.1096617033084235\n",
      "Epoch 2, Loss: 0.6576682726542155\n",
      "Epoch 3, Loss: 0.5266863505045573\n",
      "Epoch 4, Loss: 0.5295962442954382\n",
      "Epoch 5, Loss: 0.4398384888966878\n",
      "Epoch 6, Loss: 0.41034096976121265\n",
      "all_preds shape: (454,)\n",
      "all_labels shape: (454,)\n",
      "all_probs shape: (454, 2)\n",
      "train_Изъяты лексемы с частотой выше 3.csv\n",
      "test_Изъяты лексемы с частотой выше 3.csv\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([89])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([765])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.7578144172827402\n",
      "Epoch 2, Loss: 0.5042337775230408\n",
      "Epoch 3, Loss: 0.40971840421358746\n",
      "Epoch 4, Loss: 0.353627768655618\n",
      "Epoch 5, Loss: 0.26436541974544525\n",
      "Epoch 6, Loss: 0.2235631744066874\n",
      "all_preds shape: (765,)\n",
      "all_labels shape: (765,)\n",
      "all_probs shape: (765, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([89])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([732])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 2.3776961316665015\n",
      "Epoch 2, Loss: 1.7350736899922292\n",
      "Epoch 3, Loss: 0.45085779825846356\n",
      "Epoch 4, Loss: 0.44243790705998737\n",
      "Epoch 5, Loss: 0.39200782775878906\n",
      "Epoch 6, Loss: 0.4101707885662715\n",
      "all_preds shape: (732,)\n",
      "all_labels shape: (732,)\n",
      "all_probs shape: (732, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([89])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([743])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.8719609578450521\n",
      "Epoch 2, Loss: 0.4887864391009013\n",
      "Epoch 3, Loss: 0.5291342933972677\n",
      "Epoch 4, Loss: 0.49527571598688763\n",
      "Epoch 5, Loss: 0.4038907364010811\n",
      "Epoch 6, Loss: 0.33416059364875156\n",
      "all_preds shape: (743,)\n",
      "all_labels shape: (743,)\n",
      "all_probs shape: (743, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([89])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([748])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 1.2217593621462584\n",
      "Epoch 2, Loss: 0.604013471553723\n",
      "Epoch 3, Loss: 0.5988723784685135\n",
      "Epoch 4, Loss: 0.41347934554020566\n",
      "Epoch 5, Loss: 0.4247976392507553\n",
      "Epoch 6, Loss: 0.32055895030498505\n",
      "all_preds shape: (748,)\n",
      "all_labels shape: (748,)\n",
      "all_probs shape: (748, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([89])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([737])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 1.2606881260871887\n",
      "Epoch 2, Loss: 0.5939479768276215\n",
      "Epoch 3, Loss: 0.5336864739656448\n",
      "Epoch 4, Loss: 0.47182557980219525\n",
      "Epoch 5, Loss: 0.42852939168612164\n",
      "Epoch 6, Loss: 0.41148221989472705\n",
      "all_preds shape: (737,)\n",
      "all_labels shape: (737,)\n",
      "all_probs shape: (737, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([89])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([696])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.7346975356340408\n",
      "Epoch 2, Loss: 0.47268377244472504\n",
      "Epoch 3, Loss: 0.4883161435524623\n",
      "Epoch 4, Loss: 0.4459489583969116\n",
      "Epoch 5, Loss: 0.4329349547624588\n",
      "Epoch 6, Loss: 0.3762225757042567\n",
      "all_preds shape: (696,)\n",
      "all_labels shape: (696,)\n",
      "all_probs shape: (696, 2)\n",
      "train_Без прилагательных первый-второй жанр.csv\n",
      "test_Без прилагательных первый-второй жанр.csv\n",
      "Train labels sample: tensor([1, 0, 0, 0, 0]), Shape: torch.Size([860])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([175])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.7407218427569778\n",
      "Epoch 2, Loss: 0.5089441670311822\n",
      "Epoch 3, Loss: 0.4846190170005516\n",
      "Epoch 4, Loss: 0.4232621755864885\n",
      "all_preds shape: (175,)\n",
      "all_labels shape: (175,)\n",
      "all_probs shape: (175, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 0]), Shape: torch.Size([860])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([158])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.7027040035636337\n",
      "Epoch 2, Loss: 0.4971947029784874\n",
      "Epoch 3, Loss: 0.44234893057081437\n",
      "Epoch 4, Loss: 0.3803387328430458\n",
      "all_preds shape: (158,)\n",
      "all_labels shape: (158,)\n",
      "all_probs shape: (158, 2)\n",
      "Train labels sample: tensor([1, 1, 1, 0, 0]), Shape: torch.Size([860])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([175])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.5831912678700907\n",
      "Epoch 2, Loss: 0.49713898808867846\n",
      "Epoch 3, Loss: 0.4558695885870192\n",
      "Epoch 4, Loss: 0.35277125349751226\n",
      "all_preds shape: (175,)\n",
      "all_labels shape: (175,)\n",
      "all_probs shape: (175, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 0]), Shape: torch.Size([860])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([162])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.5591597071400395\n",
      "Epoch 2, Loss: 0.4888412455717723\n",
      "Epoch 3, Loss: 0.40095887471128394\n",
      "Epoch 4, Loss: 0.2917484398241396\n",
      "all_preds shape: (162,)\n",
      "all_labels shape: (162,)\n",
      "all_probs shape: (162, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels sample: tensor([0, 0, 1, 1, 1]), Shape: torch.Size([860])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([167])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.5408877653104288\n",
      "Epoch 2, Loss: 0.48584052478825607\n",
      "Epoch 3, Loss: 0.4378860151326215\n",
      "Epoch 4, Loss: 0.356849307263339\n",
      "all_preds shape: (167,)\n",
      "all_labels shape: (167,)\n",
      "all_probs shape: (167, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 1, 0]), Shape: torch.Size([860])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([158])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.6108109619882371\n",
      "Epoch 2, Loss: 0.5036341007109042\n",
      "Epoch 3, Loss: 0.4561801961174718\n",
      "Epoch 4, Loss: 0.3703445174075939\n",
      "all_preds shape: (158,)\n",
      "all_labels shape: (158,)\n",
      "all_probs shape: (158, 2)\n",
      "train_1.Первый жанр исходная выборка.csv\n",
      "test_1.Первый жанр исходная выборка.csv\n",
      "Train labels sample: tensor([0, 1, 0, 0, 1]), Shape: torch.Size([900])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([167])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.6534442797041776\n",
      "Epoch 2, Loss: 0.44127558197891503\n",
      "Epoch 3, Loss: 0.29115026437661107\n",
      "Epoch 4, Loss: 0.2098437063395977\n",
      "Epoch 5, Loss: 0.15999594311180868\n",
      "Epoch 6, Loss: 0.12090571958310249\n",
      "all_preds shape: (167,)\n",
      "all_labels shape: (167,)\n",
      "all_probs shape: (167, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 1, 1]), Shape: torch.Size([900])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([159])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.8742746778747492\n",
      "Epoch 2, Loss: 0.46904392148319046\n",
      "Epoch 3, Loss: 0.33008768213422673\n",
      "Epoch 4, Loss: 0.2241973062617737\n",
      "Epoch 5, Loss: 0.2089434768024244\n",
      "Epoch 6, Loss: 0.1719225066664972\n",
      "all_preds shape: (159,)\n",
      "all_labels shape: (159,)\n",
      "all_probs shape: (159, 2)\n",
      "Train labels sample: tensor([1, 1, 1, 1, 1]), Shape: torch.Size([900])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([162])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.7494806381955481\n",
      "Epoch 2, Loss: 0.551819362138447\n",
      "Epoch 3, Loss: 0.38198645805057724\n",
      "Epoch 4, Loss: 0.32960548565575953\n",
      "Epoch 5, Loss: 0.29004599284707455\n",
      "Epoch 6, Loss: 0.20071423360914514\n",
      "all_preds shape: (162,)\n",
      "all_labels shape: (162,)\n",
      "all_probs shape: (162, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([900])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([161])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.6092800675776967\n",
      "Epoch 2, Loss: 0.5074225264160257\n",
      "Epoch 3, Loss: 0.31294580091509905\n",
      "Epoch 4, Loss: 0.22184310580824354\n",
      "Epoch 5, Loss: 0.1646221605080523\n",
      "Epoch 6, Loss: 0.13339419494456561\n",
      "all_preds shape: (161,)\n",
      "all_labels shape: (161,)\n",
      "all_probs shape: (161, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 0]), Shape: torch.Size([900])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([158])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.6082184874174887\n",
      "Epoch 2, Loss: 0.3950389207977998\n",
      "Epoch 3, Loss: 0.22243148532875798\n",
      "Epoch 4, Loss: 0.1980622397376257\n",
      "Epoch 5, Loss: 0.15564502102502606\n",
      "Epoch 6, Loss: 0.10624657769017574\n",
      "all_preds shape: (158,)\n",
      "all_labels shape: (158,)\n",
      "all_probs shape: (158, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([900])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([162])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.6181927391311579\n",
      "Epoch 2, Loss: 0.4475754907256679\n",
      "Epoch 3, Loss: 0.32206028882871596\n",
      "Epoch 4, Loss: 0.23063680745269122\n",
      "Epoch 5, Loss: 0.1875669085992533\n",
      "Epoch 6, Loss: 0.13731680970573634\n",
      "all_preds shape: (162,)\n",
      "all_labels shape: (162,)\n",
      "all_probs shape: (162, 2)\n",
      "train_2.Первый жанр без клауз, включающих наречия.csv\n",
      "test_2.Первый жанр без клауз, включающих наречия.csv\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([888])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([169])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.5871843807399273\n",
      "Epoch 2, Loss: 0.41443754439907415\n",
      "Epoch 3, Loss: 0.2870681087619492\n",
      "Epoch 4, Loss: 0.20015481608321092\n",
      "Epoch 5, Loss: 0.1388510791252234\n",
      "Epoch 6, Loss: 0.10885942710696586\n",
      "all_preds shape: (169,)\n",
      "all_labels shape: (169,)\n",
      "all_probs shape: (169, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 1, 0]), Shape: torch.Size([888])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([170])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.6739993973502091\n",
      "Epoch 2, Loss: 0.48281599474804743\n",
      "Epoch 3, Loss: 0.34892414257462534\n",
      "Epoch 4, Loss: 0.24804060706602676\n",
      "Epoch 5, Loss: 0.19085808750241995\n",
      "Epoch 6, Loss: 0.1770381278225354\n",
      "all_preds shape: (170,)\n",
      "all_labels shape: (170,)\n",
      "all_probs shape: (170, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([888])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([164])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.583234505461795\n",
      "Epoch 2, Loss: 0.3813873715698719\n",
      "Epoch 3, Loss: 0.2673574457876384\n",
      "Epoch 4, Loss: 0.19708477166880453\n",
      "Epoch 5, Loss: 0.16971678213615501\n",
      "Epoch 6, Loss: 0.10960779675016445\n",
      "all_preds shape: (164,)\n",
      "all_labels shape: (164,)\n",
      "all_probs shape: (164, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([888])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([174])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.573575112436499\n",
      "Epoch 2, Loss: 0.3721905829651015\n",
      "Epoch 3, Loss: 0.2711860276758671\n",
      "Epoch 4, Loss: 0.24619209560166513\n",
      "Epoch 5, Loss: 0.23060830517871572\n",
      "Epoch 6, Loss: 0.15106925526301243\n",
      "all_preds shape: (174,)\n",
      "all_labels shape: (174,)\n",
      "all_probs shape: (174, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 1, 0]), Shape: torch.Size([888])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([174])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.6611152669148785\n",
      "Epoch 2, Loss: 0.42386569774576593\n",
      "Epoch 3, Loss: 0.2745192137413791\n",
      "Epoch 4, Loss: 0.22149752592667937\n",
      "Epoch 5, Loss: 0.15928524208720773\n",
      "Epoch 6, Loss: 0.12638019427790173\n",
      "all_preds shape: (174,)\n",
      "all_labels shape: (174,)\n",
      "all_probs shape: (174, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 1, 0]), Shape: torch.Size([888])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([158])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.6577805099742753\n",
      "Epoch 2, Loss: 0.48151351804179804\n",
      "Epoch 3, Loss: 0.32847805893314735\n",
      "Epoch 4, Loss: 0.2596290722223265\n",
      "Epoch 5, Loss: 0.22465201360838755\n",
      "Epoch 6, Loss: 0.1626285751283701\n",
      "all_preds shape: (158,)\n",
      "all_labels shape: (158,)\n",
      "all_probs shape: (158, 2)\n",
      "train_3.Первый жанр без клауз, включающих глаголы.csv\n",
      "test_3.Первый жанр без клауз, включающих глаголы.csv\n",
      "Train labels sample: tensor([0, 0, 1, 0, 1]), Shape: torch.Size([833])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([219])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.8435271359839529\n",
      "Epoch 2, Loss: 0.42537788287648615\n",
      "Epoch 3, Loss: 0.29985502461973085\n",
      "Epoch 4, Loss: 0.2246674352044345\n",
      "Epoch 5, Loss: 0.15552509994298783\n",
      "Epoch 6, Loss: 0.12110625978510932\n",
      "all_preds shape: (219,)\n",
      "all_labels shape: (219,)\n",
      "all_probs shape: (219, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 1]), Shape: torch.Size([833])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([217])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.6051680527908622\n",
      "Epoch 2, Loss: 0.48015533619613016\n",
      "Epoch 3, Loss: 0.3147831276621459\n",
      "Epoch 4, Loss: 0.2272066906938013\n",
      "Epoch 5, Loss: 0.18869021938021519\n",
      "Epoch 6, Loss: 0.18916884758773278\n",
      "all_preds shape: (217,)\n",
      "all_labels shape: (217,)\n",
      "all_probs shape: (217, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([833])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([203])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.6538524661424026\n",
      "Epoch 2, Loss: 0.482402393278086\n",
      "Epoch 3, Loss: 0.3070700504588631\n",
      "Epoch 4, Loss: 0.20762856297616689\n",
      "Epoch 5, Loss: 0.21775970757359042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.2163083644513533\n",
      "all_preds shape: (203,)\n",
      "all_labels shape: (203,)\n",
      "all_probs shape: (203, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 0]), Shape: torch.Size([833])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([215])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.6465936595538877\n",
      "Epoch 2, Loss: 0.5300889307597898\n",
      "Epoch 3, Loss: 0.329923669403454\n",
      "Epoch 4, Loss: 0.24834784417070518\n",
      "Epoch 5, Loss: 0.2169314724094463\n",
      "Epoch 6, Loss: 0.24885282693606503\n",
      "all_preds shape: (215,)\n",
      "all_labels shape: (215,)\n",
      "all_probs shape: (215, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 1, 1]), Shape: torch.Size([833])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([216])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.5987108254207755\n",
      "Epoch 2, Loss: 0.4444811701212289\n",
      "Epoch 3, Loss: 0.2731964631733168\n",
      "Epoch 4, Loss: 0.22645781185688837\n",
      "Epoch 5, Loss: 0.17085944615163892\n",
      "Epoch 6, Loss: 0.15815831678655912\n",
      "all_preds shape: (216,)\n",
      "all_labels shape: (216,)\n",
      "all_probs shape: (216, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([833])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([213])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.5887226867225935\n",
      "Epoch 2, Loss: 0.4076052662193828\n",
      "Epoch 3, Loss: 0.2336645364609793\n",
      "Epoch 4, Loss: 0.17902073740765875\n",
      "Epoch 5, Loss: 0.1399883721990265\n",
      "Epoch 6, Loss: 0.14026655724166698\n",
      "all_preds shape: (213,)\n",
      "all_labels shape: (213,)\n",
      "all_probs shape: (213, 2)\n",
      "train_4.Первый жанр без клауз, включающих глаголы и наречия.csv\n",
      "test_4.Первый жанр без клауз, включающих глаголы и наречия.csv\n",
      "Train labels sample: tensor([1, 0, 0, 0, 0]), Shape: torch.Size([814])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([233])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.6113326537842844\n",
      "Epoch 2, Loss: 0.42232818302570607\n",
      "Epoch 3, Loss: 0.29900285864577575\n",
      "Epoch 4, Loss: 0.22409322353846886\n",
      "Epoch 5, Loss: 0.16827078262234435\n",
      "Epoch 6, Loss: 0.1421009646911247\n",
      "all_preds shape: (233,)\n",
      "all_labels shape: (233,)\n",
      "all_probs shape: (233, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([814])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([231])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.5600061276379753\n",
      "Epoch 2, Loss: 0.42495402518440695\n",
      "Epoch 3, Loss: 0.28140344952835755\n",
      "Epoch 4, Loss: 0.1894972136645925\n",
      "Epoch 5, Loss: 0.14619683890658267\n",
      "Epoch 6, Loss: 0.0994999363434081\n",
      "all_preds shape: (231,)\n",
      "all_labels shape: (231,)\n",
      "all_probs shape: (231, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 1]), Shape: torch.Size([814])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([236])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.6100787561313779\n",
      "Epoch 2, Loss: 0.4665578793661267\n",
      "Epoch 3, Loss: 0.3375062632064025\n",
      "Epoch 4, Loss: 0.25011149771949825\n",
      "Epoch 5, Loss: 0.22400317677095824\n",
      "Epoch 6, Loss: 0.15456641881781466\n",
      "all_preds shape: (236,)\n",
      "all_labels shape: (236,)\n",
      "all_probs shape: (236, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 1]), Shape: torch.Size([814])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([237])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.6702586599424774\n",
      "Epoch 2, Loss: 0.49117446763842715\n",
      "Epoch 3, Loss: 0.3923495844298718\n",
      "Epoch 4, Loss: 0.26948875838927194\n",
      "Epoch 5, Loss: 0.19416489828304917\n",
      "Epoch 6, Loss: 0.14089174809701302\n",
      "all_preds shape: (237,)\n",
      "all_labels shape: (237,)\n",
      "all_probs shape: (237, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([814])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([238])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.6052964914078806\n",
      "Epoch 2, Loss: 0.36133645299602957\n",
      "Epoch 3, Loss: 0.26058997199231504\n",
      "Epoch 4, Loss: 0.2073663458520291\n",
      "Epoch 5, Loss: 0.12300606656308267\n",
      "Epoch 6, Loss: 0.08921804386830214\n",
      "all_preds shape: (238,)\n",
      "all_labels shape: (238,)\n",
      "all_probs shape: (238, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 1]), Shape: torch.Size([814])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([241])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.7006889196003184\n",
      "Epoch 2, Loss: 0.5044439841134876\n",
      "Epoch 3, Loss: 0.40458980290328755\n",
      "Epoch 4, Loss: 0.25664326599707793\n",
      "Epoch 5, Loss: 0.3003605209729251\n",
      "Epoch 6, Loss: 0.1849689742221552\n",
      "all_preds shape: (241,)\n",
      "all_labels shape: (241,)\n",
      "all_probs shape: (241, 2)\n",
      "train_5.без клауз, включающих местоимения.txt.csv\n",
      "test_5.без клауз, включающих местоимения.txt.csv\n",
      "Train labels sample: tensor([1, 1, 1, 1, 0]), Shape: torch.Size([867])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([190])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.5949182857166637\n",
      "Epoch 2, Loss: 0.39709316139871426\n",
      "Epoch 3, Loss: 0.25647778605872934\n",
      "Epoch 4, Loss: 0.2037245600399646\n",
      "Epoch 5, Loss: 0.14561795788732443\n",
      "Epoch 6, Loss: 0.1308573321693323\n",
      "all_preds shape: (190,)\n",
      "all_labels shape: (190,)\n",
      "all_probs shape: (190, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 0]), Shape: torch.Size([867])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([182])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.6726829398762096\n",
      "Epoch 2, Loss: 0.5487671526995572\n",
      "Epoch 3, Loss: 0.4169244904409755\n",
      "Epoch 4, Loss: 0.34266108545390045\n",
      "Epoch 5, Loss: 0.23264458721334283\n",
      "Epoch 6, Loss: 0.18522555247287859\n",
      "all_preds shape: (182,)\n",
      "all_labels shape: (182,)\n",
      "all_probs shape: (182, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 1]), Shape: torch.Size([867])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([197])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.6092493702064861\n",
      "Epoch 2, Loss: 0.4667324459010905\n",
      "Epoch 3, Loss: 0.2972846389494159\n",
      "Epoch 4, Loss: 0.22450835840268568\n",
      "Epoch 5, Loss: 0.19276158217002046\n",
      "Epoch 6, Loss: 0.11592562001367862\n",
      "all_preds shape: (197,)\n",
      "all_labels shape: (197,)\n",
      "all_probs shape: (197, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 0]), Shape: torch.Size([867])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([190])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.669235106489875\n",
      "Epoch 2, Loss: 0.4137731446461244\n",
      "Epoch 3, Loss: 0.27278436693278224\n",
      "Epoch 4, Loss: 0.238209516140209\n",
      "Epoch 5, Loss: 0.1695106985216791\n",
      "Epoch 6, Loss: 0.12303229741413485\n",
      "all_preds shape: (190,)\n",
      "all_labels shape: (190,)\n",
      "all_probs shape: (190, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([867])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([185])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.7782438508488915\n",
      "Epoch 2, Loss: 0.49109442802992737\n",
      "Epoch 3, Loss: 0.3075292776931416\n",
      "Epoch 4, Loss: 0.22469703927636148\n",
      "Epoch 5, Loss: 0.18170405731282452\n",
      "Epoch 6, Loss: 0.1424421909011223\n",
      "all_preds shape: (185,)\n",
      "all_labels shape: (185,)\n",
      "all_probs shape: (185, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 0]), Shape: torch.Size([867])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([190])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.7021559257398952\n",
      "Epoch 2, Loss: 0.3530512530525977\n",
      "Epoch 3, Loss: 0.21406923579898748\n",
      "Epoch 4, Loss: 0.20383278314362874\n",
      "Epoch 5, Loss: 0.12811816883358088\n",
      "Epoch 6, Loss: 0.0979393028400161\n",
      "all_preds shape: (190,)\n",
      "all_labels shape: (190,)\n",
      "all_probs shape: (190, 2)\n",
      "train_6.без слов функциональных.txt.csv\n",
      "test_6.без слов функциональных.txt.csv\n",
      "Train labels sample: tensor([0, 1, 1, 1, 1]), Shape: torch.Size([923])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([185])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.4904195003468415\n",
      "Epoch 2, Loss: 0.24511459460160856\n",
      "Epoch 3, Loss: 0.17175750405495538\n",
      "Epoch 4, Loss: 0.13840243681574818\n",
      "Epoch 5, Loss: 0.11362932680841085\n",
      "Epoch 6, Loss: 0.07596181164162458\n",
      "all_preds shape: (185,)\n",
      "all_labels shape: (185,)\n",
      "all_probs shape: (185, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 1, 0]), Shape: torch.Size([923])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([183])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.5923904889616473\n",
      "Epoch 2, Loss: 0.34741901606321335\n",
      "Epoch 3, Loss: 0.16527615271605037\n",
      "Epoch 4, Loss: 0.12716351574736423\n",
      "Epoch 5, Loss: 0.10291339675012719\n",
      "Epoch 6, Loss: 0.08967715272166092\n",
      "all_preds shape: (183,)\n",
      "all_labels shape: (183,)\n",
      "all_probs shape: (183, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([923])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([184])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.5755125320163267\n",
      "Epoch 2, Loss: 0.2776889220393937\n",
      "Epoch 3, Loss: 0.19782869042507534\n",
      "Epoch 4, Loss: 0.11130462225590801\n",
      "Epoch 5, Loss: 0.12734634601029343\n",
      "Epoch 6, Loss: 0.08703013574543955\n",
      "all_preds shape: (184,)\n",
      "all_labels shape: (184,)\n",
      "all_probs shape: (184, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([923])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([179])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.496624990912347\n",
      "Epoch 2, Loss: 0.25292383840885657\n",
      "Epoch 3, Loss: 0.1533031466834504\n",
      "Epoch 4, Loss: 0.135268663433541\n",
      "Epoch 5, Loss: 0.07092190907208314\n",
      "Epoch 6, Loss: 0.06329407937551752\n",
      "all_preds shape: (179,)\n",
      "all_labels shape: (179,)\n",
      "all_probs shape: (179, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 0]), Shape: torch.Size([923])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([189])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.5914690206790792\n",
      "Epoch 2, Loss: 0.33484280443397063\n",
      "Epoch 3, Loss: 0.2033501660515522\n",
      "Epoch 4, Loss: 0.1580249710843481\n",
      "Epoch 5, Loss: 0.11830415180081437\n",
      "Epoch 6, Loss: 0.12194036771061606\n",
      "all_preds shape: (189,)\n",
      "all_labels shape: (189,)\n",
      "all_probs shape: (189, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 1, 1]), Shape: torch.Size([923])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([190])\n",
      "##########################gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.5671788592790735\n",
      "Epoch 2, Loss: 0.29836885322784557\n",
      "Epoch 3, Loss: 0.20658655291230515\n",
      "Epoch 4, Loss: 0.14924838525596365\n",
      "Epoch 5, Loss: 0.10491095152522983\n",
      "Epoch 6, Loss: 0.07653897643442555\n",
      "all_preds shape: (190,)\n",
      "all_labels shape: (190,)\n",
      "all_probs shape: (190, 2)\n",
      "train_Изъяты лексемы с частотой выше 100.csv\n",
      "test_Изъяты лексемы с частотой выше 100.csv\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([880])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([197])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.38177520009604365\n",
      "Epoch 2, Loss: 0.12006554836407304\n",
      "Epoch 3, Loss: 0.09398745548149401\n",
      "Epoch 4, Loss: 0.03566542210173793\n",
      "Epoch 5, Loss: 0.03267535833180194\n",
      "Epoch 6, Loss: 0.02426835102484223\n",
      "all_preds shape: (197,)\n",
      "all_labels shape: (197,)\n",
      "all_probs shape: (197, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([880])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([204])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.4270702810450034\n",
      "Epoch 2, Loss: 0.1527435951950875\n",
      "Epoch 3, Loss: 0.07979980806227435\n",
      "Epoch 4, Loss: 0.05355300597042184\n",
      "Epoch 5, Loss: 0.047523911932313985\n",
      "Epoch 6, Loss: 0.042386098440990526\n",
      "all_preds shape: (204,)\n",
      "all_labels shape: (204,)\n",
      "all_probs shape: (204, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([880])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([191])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.4171328012238849\n",
      "Epoch 2, Loss: 0.1645049665461887\n",
      "Epoch 3, Loss: 0.09617137234996666\n",
      "Epoch 4, Loss: 0.05947880493476987\n",
      "Epoch 5, Loss: 0.050874159703115845\n",
      "Epoch 6, Loss: 0.020420701408346013\n",
      "all_preds shape: (191,)\n",
      "all_labels shape: (191,)\n",
      "all_probs shape: (191, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 0]), Shape: torch.Size([880])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([197])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.405384020913731\n",
      "Epoch 2, Loss: 0.16427236499095504\n",
      "Epoch 3, Loss: 0.08390065861696547\n",
      "Epoch 4, Loss: 0.064959114971994\n",
      "Epoch 5, Loss: 0.01672157324236733\n",
      "Epoch 6, Loss: 0.035238928274272684\n",
      "all_preds shape: (197,)\n",
      "all_labels shape: (197,)\n",
      "all_probs shape: (197, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 0]), Shape: torch.Size([880])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([194])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.4254466260698709\n",
      "Epoch 2, Loss: 0.15385142592713236\n",
      "Epoch 3, Loss: 0.09999469585124064\n",
      "Epoch 4, Loss: 0.06337697752328082\n",
      "Epoch 5, Loss: 0.03249262023971162\n",
      "Epoch 6, Loss: 0.03842541901183061\n",
      "all_preds shape: (194,)\n",
      "all_labels shape: (194,)\n",
      "all_probs shape: (194, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 1]), Shape: torch.Size([880])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([190])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.3553063098002564\n",
      "Epoch 2, Loss: 0.11680352891033347\n",
      "Epoch 3, Loss: 0.08054637614298951\n",
      "Epoch 4, Loss: 0.1079377958593382\n",
      "Epoch 5, Loss: 0.0630975963801823\n",
      "Epoch 6, Loss: 0.05668858154253526\n",
      "all_preds shape: (190,)\n",
      "all_labels shape: (190,)\n",
      "all_probs shape: (190, 2)\n",
      "train_Изъяты лексемы с частотой выше 49.csv\n",
      "test_Изъяты лексемы с частотой выше 49.csv\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([912])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([177])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.3365060979533091\n",
      "Epoch 2, Loss: 0.1354319072050745\n",
      "Epoch 3, Loss: 0.09711602281190847\n",
      "Epoch 4, Loss: 0.064771861720242\n",
      "Epoch 5, Loss: 0.049699091741594634\n",
      "Epoch 6, Loss: 0.12716027826025852\n",
      "all_preds shape: (177,)\n",
      "all_labels shape: (177,)\n",
      "all_probs shape: (177, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 1]), Shape: torch.Size([912])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([191])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.4221752363077381\n",
      "Epoch 2, Loss: 0.1973060958512258\n",
      "Epoch 3, Loss: 0.10640667547194059\n",
      "Epoch 4, Loss: 0.09820972135486572\n",
      "Epoch 5, Loss: 0.07016866722372933\n",
      "Epoch 6, Loss: 0.09224944667514871\n",
      "all_preds shape: (191,)\n",
      "all_labels shape: (191,)\n",
      "all_probs shape: (191, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 0]), Shape: torch.Size([912])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([190])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.46576136036923055\n",
      "Epoch 2, Loss: 0.20772788239022097\n",
      "Epoch 3, Loss: 0.10547872567385957\n",
      "Epoch 4, Loss: 0.0643271193410711\n",
      "Epoch 5, Loss: 0.06805822919858129\n",
      "Epoch 6, Loss: 0.05478466168993659\n",
      "all_preds shape: (190,)\n",
      "all_labels shape: (190,)\n",
      "all_probs shape: (190, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 1, 0]), Shape: torch.Size([912])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([191])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.33684673287758704\n",
      "Epoch 2, Loss: 0.12187703666195535\n",
      "Epoch 3, Loss: 0.11167214283915727\n",
      "Epoch 4, Loss: 0.04783331765180552\n",
      "Epoch 5, Loss: 0.05469832020268465\n",
      "Epoch 6, Loss: 0.09349514715365347\n",
      "all_preds shape: (191,)\n",
      "all_labels shape: (191,)\n",
      "all_probs shape: (191, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 1]), Shape: torch.Size([912])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([183])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.4128229488667689\n",
      "Epoch 2, Loss: 0.18668352194914692\n",
      "Epoch 3, Loss: 0.09647219441014163\n",
      "Epoch 4, Loss: 0.07188165505016386\n",
      "Epoch 5, Loss: 0.07206263196278821\n",
      "Epoch 6, Loss: 0.0572413605302899\n",
      "all_preds shape: (183,)\n",
      "all_labels shape: (183,)\n",
      "all_probs shape: (183, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([912])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([189])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.410066584484619\n",
      "Epoch 2, Loss: 0.14786215912419975\n",
      "Epoch 3, Loss: 0.07577465547314077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.04446636128092283\n",
      "Epoch 5, Loss: 0.06776599023948636\n",
      "Epoch 6, Loss: 0.016698385700134134\n",
      "all_preds shape: (189,)\n",
      "all_labels shape: (189,)\n",
      "all_probs shape: (189, 2)\n",
      "train_Изъяты лексемы с частотой выше 29.csv\n",
      "test_Изъяты лексемы с частотой выше 29.csv\n",
      "Train labels sample: tensor([0, 1, 0, 0, 1]), Shape: torch.Size([900])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([191])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.5757436726176948\n",
      "Epoch 2, Loss: 0.3872586125344561\n",
      "Epoch 3, Loss: 0.23639951058124242\n",
      "Epoch 4, Loss: 0.18154015143712363\n",
      "Epoch 5, Loss: 0.11327322652530775\n",
      "Epoch 6, Loss: 0.09682385235608212\n",
      "all_preds shape: (191,)\n",
      "all_labels shape: (191,)\n",
      "all_probs shape: (191, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 1, 1]), Shape: torch.Size([900])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([190])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.39542211984333236\n",
      "Epoch 2, Loss: 0.14569753019563986\n",
      "Epoch 3, Loss: 0.08179626627207383\n",
      "Epoch 4, Loss: 0.04891446787300274\n",
      "Epoch 5, Loss: 0.03342281959760983\n",
      "Epoch 6, Loss: 0.15408278671256675\n",
      "all_preds shape: (190,)\n",
      "all_labels shape: (190,)\n",
      "all_probs shape: (190, 2)\n",
      "Train labels sample: tensor([1, 1, 1, 1, 1]), Shape: torch.Size([900])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([193])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.41814871654404623\n",
      "Epoch 2, Loss: 0.20587680132634806\n",
      "Epoch 3, Loss: 0.11582557154868386\n",
      "Epoch 4, Loss: 0.07738391348781685\n",
      "Epoch 5, Loss: 0.09673061292095665\n",
      "Epoch 6, Loss: 0.02975280287475407\n",
      "all_preds shape: (193,)\n",
      "all_labels shape: (193,)\n",
      "all_probs shape: (193, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([900])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([195])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.42342794157172503\n",
      "Epoch 2, Loss: 0.1768921690813282\n",
      "Epoch 3, Loss: 0.0845701830919113\n",
      "Epoch 4, Loss: 0.06001592652970239\n",
      "Epoch 5, Loss: 0.05516043828857507\n",
      "Epoch 6, Loss: 0.04522752350915158\n",
      "all_preds shape: (195,)\n",
      "all_labels shape: (195,)\n",
      "all_probs shape: (195, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 0]), Shape: torch.Size([900])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([192])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.3626691434989896\n",
      "Epoch 2, Loss: 0.18759502336513578\n",
      "Epoch 3, Loss: 0.10440862865040176\n",
      "Epoch 4, Loss: 0.054697537866192296\n",
      "Epoch 5, Loss: 0.06689612743885893\n",
      "Epoch 6, Loss: 0.0853741044860758\n",
      "all_preds shape: (192,)\n",
      "all_labels shape: (192,)\n",
      "all_probs shape: (192, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([900])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([188])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.3766367055083576\n",
      "Epoch 2, Loss: 0.16394534649882922\n",
      "Epoch 3, Loss: 0.10914134279939167\n",
      "Epoch 4, Loss: 0.07758944924806424\n",
      "Epoch 5, Loss: 0.10394219652925242\n",
      "Epoch 6, Loss: 0.07214898359144858\n",
      "all_preds shape: (188,)\n",
      "all_labels shape: (188,)\n",
      "all_probs shape: (188, 2)\n",
      "train_Изъяты лексемы с частотой выше 9.csv\n",
      "test_Изъяты лексемы с частотой выше 9.csv\n",
      "Train labels sample: tensor([1, 0, 0, 0, 0]), Shape: torch.Size([772])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([341])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.5642890846729278\n",
      "Epoch 2, Loss: 0.23497793838381767\n",
      "Epoch 3, Loss: 0.1139173660427332\n",
      "Epoch 4, Loss: 0.08444699032232166\n",
      "all_preds shape: (341,)\n",
      "all_labels shape: (341,)\n",
      "all_probs shape: (341, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 1, 1]), Shape: torch.Size([772])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([343])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.4853423634171486\n",
      "Epoch 2, Loss: 0.27603240966796877\n",
      "Epoch 3, Loss: 0.13016996040940285\n",
      "Epoch 4, Loss: 0.07377873610705138\n",
      "all_preds shape: (343,)\n",
      "all_labels shape: (343,)\n",
      "all_probs shape: (343, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 1]), Shape: torch.Size([772])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([352])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.572497376203537\n",
      "Epoch 2, Loss: 0.2685610282421112\n",
      "Epoch 3, Loss: 0.19617696583271027\n",
      "Epoch 4, Loss: 0.10550050787627697\n",
      "all_preds shape: (352,)\n",
      "all_labels shape: (352,)\n",
      "all_probs shape: (352, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 1, 0]), Shape: torch.Size([772])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([346])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.49632911443710326\n",
      "Epoch 2, Loss: 0.26466061115264894\n",
      "Epoch 3, Loss: 0.14073633134365082\n",
      "Epoch 4, Loss: 0.08055384188890458\n",
      "all_preds shape: (346,)\n",
      "all_labels shape: (346,)\n",
      "all_probs shape: (346, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([772])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([348])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.5226460993289948\n",
      "Epoch 2, Loss: 0.2616609688103199\n",
      "Epoch 3, Loss: 0.13845587201416493\n",
      "Epoch 4, Loss: 0.0678918094933033\n",
      "all_preds shape: (348,)\n",
      "all_labels shape: (348,)\n",
      "all_probs shape: (348, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([772])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([349])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.5264781653881073\n",
      "Epoch 2, Loss: 0.2672777420282364\n",
      "Epoch 3, Loss: 0.15238778106868267\n",
      "Epoch 4, Loss: 0.08655895438045264\n",
      "all_preds shape: (349,)\n",
      "all_labels shape: (349,)\n",
      "all_probs shape: (349, 2)\n",
      "train_Изъяты лексемы с частотой выше 5.csv\n",
      "test_Изъяты лексемы с частотой выше 5.csv\n",
      "Train labels sample: tensor([1, 0, 0, 0, 0]), Shape: torch.Size([98])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([541])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.4972101907644953\n",
      "Epoch 2, Loss: 0.31456576686884674\n",
      "Epoch 3, Loss: 0.19255350264055388\n",
      "Epoch 4, Loss: 0.09548165196818965\n",
      "Epoch 5, Loss: 0.0402072202913197\n",
      "Epoch 6, Loss: 0.026953045085455023\n",
      "all_preds shape: (541,)\n",
      "all_labels shape: (541,)\n",
      "all_probs shape: (541, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([98])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([525])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.5108280522482735\n",
      "Epoch 2, Loss: 0.3230694553681782\n",
      "Epoch 3, Loss: 0.17233602383307048\n",
      "Epoch 4, Loss: 0.06468313114185419\n",
      "Epoch 5, Loss: 0.016889553107570725\n",
      "Epoch 6, Loss: 0.011884763296360948\n",
      "all_preds shape: (525,)\n",
      "all_labels shape: (525,)\n",
      "all_probs shape: (525, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([98])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([552])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.5538015706198556\n",
      "Epoch 2, Loss: 0.24034366064838\n",
      "Epoch 3, Loss: 0.16532539682728903\n",
      "Epoch 4, Loss: 0.11608485481701791\n",
      "Epoch 5, Loss: 0.051199102508170266\n",
      "Epoch 6, Loss: 0.013585148179637534\n",
      "all_preds shape: (552,)\n",
      "all_labels shape: (552,)\n",
      "all_probs shape: (552, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([98])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([494])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.6140368069921222\n",
      "Epoch 2, Loss: 0.39349776187113356\n",
      "Epoch 3, Loss: 0.20870649122766086\n",
      "Epoch 4, Loss: 0.14025026559829712\n",
      "Epoch 5, Loss: 0.2663737120372908\n",
      "Epoch 6, Loss: 0.06947339259620224\n",
      "all_preds shape: (494,)\n",
      "all_labels shape: (494,)\n",
      "all_probs shape: (494, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([98])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([528])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.6578379784311567\n",
      "Epoch 2, Loss: 0.3853005085672651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.3128867894411087\n",
      "Epoch 4, Loss: 0.17526632121631078\n",
      "Epoch 5, Loss: 0.09666782830442701\n",
      "Epoch 6, Loss: 0.040624979218201976\n",
      "all_preds shape: (528,)\n",
      "all_labels shape: (528,)\n",
      "all_probs shape: (528, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([98])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([480])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.6860806558813367\n",
      "Epoch 2, Loss: 0.3426115023238318\n",
      "Epoch 3, Loss: 0.16300617264849798\n",
      "Epoch 4, Loss: 0.0888352096496549\n",
      "Epoch 5, Loss: 0.0567643658391067\n",
      "Epoch 6, Loss: 0.02280869897055839\n",
      "all_preds shape: (480,)\n",
      "all_labels shape: (480,)\n",
      "all_probs shape: (480, 2)\n",
      "train_Изъяты лексемы с частотой выше 3.csv\n",
      "test_Изъяты лексемы с частотой выше 3.csv\n",
      "Train labels sample: tensor([0, 0, 0, 1, 1]), Shape: torch.Size([92])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([716])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.8397636016209921\n",
      "Epoch 2, Loss: 0.5533468176921209\n",
      "Epoch 3, Loss: 0.4948852558930715\n",
      "Epoch 4, Loss: 0.3929605384667714\n",
      "Epoch 5, Loss: 0.2792796914776166\n",
      "Epoch 6, Loss: 0.2252712075908979\n",
      "all_preds shape: (716,)\n",
      "all_labels shape: (716,)\n",
      "all_probs shape: (716, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([92])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([720])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.617204986512661\n",
      "Epoch 2, Loss: 0.3476933365066846\n",
      "Epoch 3, Loss: 0.28595995157957077\n",
      "Epoch 4, Loss: 0.21441723654667535\n",
      "Epoch 5, Loss: 0.12183250409240524\n",
      "Epoch 6, Loss: 0.054355548384288944\n",
      "all_preds shape: (720,)\n",
      "all_labels shape: (720,)\n",
      "all_probs shape: (720, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([92])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([701])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.6699990828831991\n",
      "Epoch 2, Loss: 0.44869136313597363\n",
      "Epoch 3, Loss: 0.28848419338464737\n",
      "Epoch 4, Loss: 0.17074963822960854\n",
      "Epoch 5, Loss: 0.10850942445298035\n",
      "Epoch 6, Loss: 0.11981766996905208\n",
      "all_preds shape: (701,)\n",
      "all_labels shape: (701,)\n",
      "all_probs shape: (701, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([92])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([742])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.6026860872904459\n",
      "Epoch 2, Loss: 0.39770708978176117\n",
      "Epoch 3, Loss: 0.2594762531419595\n",
      "Epoch 4, Loss: 0.21767176439364752\n",
      "Epoch 5, Loss: 0.18228859982142845\n",
      "Epoch 6, Loss: 0.09827622833351295\n",
      "all_preds shape: (742,)\n",
      "all_labels shape: (742,)\n",
      "all_probs shape: (742, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 1]), Shape: torch.Size([92])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([695])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.5664854546387991\n",
      "Epoch 2, Loss: 0.4580980862180392\n",
      "Epoch 3, Loss: 0.3156946152448654\n",
      "Epoch 4, Loss: 0.21310044080018997\n",
      "Epoch 5, Loss: 0.09412605874240398\n",
      "Epoch 6, Loss: 0.04164652153849602\n",
      "all_preds shape: (695,)\n",
      "all_labels shape: (695,)\n",
      "all_probs shape: (695, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([92])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([692])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.5969759772221247\n",
      "Epoch 2, Loss: 0.3389913762609164\n",
      "Epoch 3, Loss: 0.20550178239742914\n",
      "Epoch 4, Loss: 0.12329782266169786\n",
      "Epoch 5, Loss: 0.07429025762636836\n",
      "Epoch 6, Loss: 0.029072443799426157\n",
      "all_preds shape: (692,)\n",
      "all_labels shape: (692,)\n",
      "all_probs shape: (692, 2)\n",
      "train_Без прилагательных первый-второй жанр.csv\n",
      "test_Без прилагательных первый-второй жанр.csv\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([865])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([167])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.3648681318653481\n",
      "Epoch 2, Loss: 0.09994923584495805\n",
      "Epoch 3, Loss: 0.035915577942172865\n",
      "Epoch 4, Loss: 0.017571087901111632\n",
      "all_preds shape: (167,)\n",
      "all_labels shape: (167,)\n",
      "all_probs shape: (167, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 1, 0]), Shape: torch.Size([865])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([169])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.43665426703436033\n",
      "Epoch 2, Loss: 0.18248209743095295\n",
      "Epoch 3, Loss: 0.04931763799062797\n",
      "Epoch 4, Loss: 0.026581316138617694\n",
      "all_preds shape: (169,)\n",
      "all_labels shape: (169,)\n",
      "all_probs shape: (169, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 1]), Shape: torch.Size([865])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([173])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.43417254329792093\n",
      "Epoch 2, Loss: 0.16330720324601447\n",
      "Epoch 3, Loss: 0.06674599243810267\n",
      "Epoch 4, Loss: 0.0213067866030201\n",
      "all_preds shape: (173,)\n",
      "all_labels shape: (173,)\n",
      "all_probs shape: (173, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 1, 0]), Shape: torch.Size([865])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([168])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.4587534532350089\n",
      "Epoch 2, Loss: 0.1323540965677239\n",
      "Epoch 3, Loss: 0.09448991216985243\n",
      "Epoch 4, Loss: 0.05251696941974972\n",
      "all_preds shape: (168,)\n",
      "all_labels shape: (168,)\n",
      "all_probs shape: (168, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([865])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([164])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.4343392319445099\n",
      "Epoch 2, Loss: 0.21664297341236047\n",
      "Epoch 3, Loss: 0.06865757615638099\n",
      "Epoch 4, Loss: 0.018018539253846808\n",
      "all_preds shape: (164,)\n",
      "all_labels shape: (164,)\n",
      "all_probs shape: (164, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 0]), Shape: torch.Size([865])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([159])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.3600970567869289\n",
      "Epoch 2, Loss: 0.11521578462062669\n",
      "Epoch 3, Loss: 0.08517518478661909\n",
      "Epoch 4, Loss: 0.02418171472053083\n",
      "all_preds shape: (159,)\n",
      "all_labels shape: (159,)\n",
      "all_probs shape: (159, 2)\n",
      "train_1.Первый жанр исходная выборка.csv\n",
      "test_1.Первый жанр исходная выборка.csv\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([905])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([170])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.301954708764689\n",
      "Epoch 2, Loss: 0.16040679220142856\n",
      "Epoch 3, Loss: 0.0606813798250075\n",
      "Epoch 4, Loss: 0.02257583739660811\n",
      "Epoch 5, Loss: 0.01867135244384898\n",
      "Epoch 6, Loss: 0.04718242204398848\n",
      "all_preds shape: (170,)\n",
      "all_labels shape: (170,)\n",
      "all_probs shape: (170, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 1, 1]), Shape: torch.Size([905])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([169])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.3611973356128785\n",
      "Epoch 2, Loss: 0.09664474582103522\n",
      "Epoch 3, Loss: 0.1115787476709668\n",
      "Epoch 4, Loss: 0.09226277049042676\n",
      "Epoch 5, Loss: 0.04136432528193517\n",
      "Epoch 6, Loss: 0.015980091972762654\n",
      "all_preds shape: (169,)\n",
      "all_labels shape: (169,)\n",
      "all_probs shape: (169, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 0]), Shape: torch.Size([905])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([164])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.36721527566643136\n",
      "Epoch 2, Loss: 0.1008131160123045\n",
      "Epoch 3, Loss: 0.11046903925588387\n",
      "Epoch 4, Loss: 0.024180525996550722\n",
      "Epoch 5, Loss: 0.03933353843259331\n",
      "Epoch 6, Loss: 0.06232308002603404\n",
      "all_preds shape: (164,)\n",
      "all_labels shape: (164,)\n",
      "all_probs shape: (164, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 1]), Shape: torch.Size([905])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([153])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.35899187990447934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.14091079106979204\n",
      "Epoch 3, Loss: 0.06649106881466874\n",
      "Epoch 4, Loss: 0.03815026996502032\n",
      "Epoch 5, Loss: 0.010072161548713777\n",
      "Epoch 6, Loss: 0.005236114941541465\n",
      "all_preds shape: (153,)\n",
      "all_labels shape: (153,)\n",
      "all_probs shape: (153, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 1, 1]), Shape: torch.Size([905])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([165])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.31471430288072216\n",
      "Epoch 2, Loss: 0.14123291595789947\n",
      "Epoch 3, Loss: 0.06354491164491169\n",
      "Epoch 4, Loss: 0.07655081988246948\n",
      "Epoch 5, Loss: 0.05739194741615568\n",
      "Epoch 6, Loss: 0.018207393428862195\n",
      "all_preds shape: (165,)\n",
      "all_labels shape: (165,)\n",
      "all_probs shape: (165, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 1, 0]), Shape: torch.Size([905])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([166])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.4362296471208857\n",
      "Epoch 2, Loss: 0.15756760500044675\n",
      "Epoch 3, Loss: 0.10167949687967305\n",
      "Epoch 4, Loss: 0.06982963586945021\n",
      "Epoch 5, Loss: 0.0416596105318157\n",
      "Epoch 6, Loss: 0.10219015612414009\n",
      "all_preds shape: (166,)\n",
      "all_labels shape: (166,)\n",
      "all_probs shape: (166, 2)\n",
      "train_2.Первый жанр без клауз, включающих наречия.csv\n",
      "test_2.Первый жанр без клауз, включающих наречия.csv\n",
      "Train labels sample: tensor([1, 1, 1, 0, 0]), Shape: torch.Size([892])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([173])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.3270961671535458\n",
      "Epoch 2, Loss: 0.10199672096808042\n",
      "Epoch 3, Loss: 0.0871085617576942\n",
      "Epoch 4, Loss: 0.11292655961525659\n",
      "Epoch 5, Loss: 0.05234656906603569\n",
      "Epoch 6, Loss: 0.011760538748473794\n",
      "all_preds shape: (173,)\n",
      "all_labels shape: (173,)\n",
      "all_probs shape: (173, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 1, 0]), Shape: torch.Size([892])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([173])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.35151428044108407\n",
      "Epoch 2, Loss: 0.23105688087110007\n",
      "Epoch 3, Loss: 0.13542362523730844\n",
      "Epoch 4, Loss: 0.06643760455439665\n",
      "Epoch 5, Loss: 0.07627219435276597\n",
      "Epoch 6, Loss: 0.05611795897129923\n",
      "all_preds shape: (173,)\n",
      "all_labels shape: (173,)\n",
      "all_probs shape: (173, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 1, 0]), Shape: torch.Size([892])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([167])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.36083200567268897\n",
      "Epoch 2, Loss: 0.12075828168807286\n",
      "Epoch 3, Loss: 0.0674372124277787\n",
      "Epoch 4, Loss: 0.04437238472434858\n",
      "Epoch 5, Loss: 0.05612611470964372\n",
      "Epoch 6, Loss: 0.05163776304938698\n",
      "all_preds shape: (167,)\n",
      "all_labels shape: (167,)\n",
      "all_probs shape: (167, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([892])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([182])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.28789670186649474\n",
      "Epoch 2, Loss: 0.08624391543396216\n",
      "Epoch 3, Loss: 0.10761872859438881\n",
      "Epoch 4, Loss: 0.09188386233290657\n",
      "Epoch 5, Loss: 0.07241574475691388\n",
      "Epoch 6, Loss: 0.041033147107685054\n",
      "all_preds shape: (182,)\n",
      "all_labels shape: (182,)\n",
      "all_probs shape: (182, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 1]), Shape: torch.Size([892])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([164])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.35238655570096206\n",
      "Epoch 2, Loss: 0.10786270200540977\n",
      "Epoch 3, Loss: 0.05519095275667496\n",
      "Epoch 4, Loss: 0.13610539735444555\n",
      "Epoch 5, Loss: 0.11253703134467027\n",
      "Epoch 6, Loss: 0.10289782608327057\n",
      "all_preds shape: (164,)\n",
      "all_labels shape: (164,)\n",
      "all_probs shape: (164, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 1, 0]), Shape: torch.Size([892])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([161])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.4366043619811535\n",
      "Epoch 2, Loss: 0.16390794079883822\n",
      "Epoch 3, Loss: 0.09254298689276245\n",
      "Epoch 4, Loss: 0.06599396674677596\n",
      "Epoch 5, Loss: 0.03393044105957545\n",
      "Epoch 6, Loss: 0.019948074327008465\n",
      "all_preds shape: (161,)\n",
      "all_labels shape: (161,)\n",
      "all_probs shape: (161, 2)\n",
      "train_3.Первый жанр без клауз, включающих глаголы.csv\n",
      "test_3.Первый жанр без клауз, включающих глаголы.csv\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([836])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([218])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.412264061634833\n",
      "Epoch 2, Loss: 0.12419118012635494\n",
      "Epoch 3, Loss: 0.0634771835252221\n",
      "Epoch 4, Loss: 0.07166469918634251\n",
      "Epoch 5, Loss: 0.04614858397827396\n",
      "Epoch 6, Loss: 0.012600861417974197\n",
      "all_preds shape: (218,)\n",
      "all_labels shape: (218,)\n",
      "all_probs shape: (218, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 1]), Shape: torch.Size([836])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([224])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.4300588295566586\n",
      "Epoch 2, Loss: 0.14270542068231218\n",
      "Epoch 3, Loss: 0.07850445307053204\n",
      "Epoch 4, Loss: 0.04505369155931304\n",
      "Epoch 5, Loss: 0.043998021413330514\n",
      "Epoch 6, Loss: 0.1536112451961018\n",
      "all_preds shape: (224,)\n",
      "all_labels shape: (224,)\n",
      "all_probs shape: (224, 2)\n",
      "Train labels sample: tensor([1, 1, 1, 0, 0]), Shape: torch.Size([836])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([212])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.36652434810574325\n",
      "Epoch 2, Loss: 0.1140477876003199\n",
      "Epoch 3, Loss: 0.05938917136628111\n",
      "Epoch 4, Loss: 0.0888129477125575\n",
      "Epoch 5, Loss: 0.049254327865299895\n",
      "Epoch 6, Loss: 0.021023880042364152\n",
      "all_preds shape: (212,)\n",
      "all_labels shape: (212,)\n",
      "all_probs shape: (212, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 1, 0]), Shape: torch.Size([836])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([219])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.5001207061533658\n",
      "Epoch 2, Loss: 0.17755201163719286\n",
      "Epoch 3, Loss: 0.08752063085448067\n",
      "Epoch 4, Loss: 0.049589182239219126\n",
      "Epoch 5, Loss: 0.041957126693132354\n",
      "Epoch 6, Loss: 0.051527316512909\n",
      "all_preds shape: (219,)\n",
      "all_labels shape: (219,)\n",
      "all_probs shape: (219, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([836])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([208])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.40712096064158204\n",
      "Epoch 2, Loss: 0.17482997090467867\n",
      "Epoch 3, Loss: 0.05945402014789716\n",
      "Epoch 4, Loss: 0.032272287730110004\n",
      "Epoch 5, Loss: 0.025265770139532902\n",
      "Epoch 6, Loss: 0.06140675477327709\n",
      "all_preds shape: (208,)\n",
      "all_labels shape: (208,)\n",
      "all_probs shape: (208, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 1]), Shape: torch.Size([836])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([222])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.30533124077713714\n",
      "Epoch 2, Loss: 0.1327500136103481\n",
      "Epoch 3, Loss: 0.08496775567742451\n",
      "Epoch 4, Loss: 0.18040672427000487\n",
      "Epoch 5, Loss: 0.4964434650709044\n",
      "Epoch 6, Loss: 0.31402139012473373\n",
      "all_preds shape: (222,)\n",
      "all_labels shape: (222,)\n",
      "all_probs shape: (222, 2)\n",
      "train_4.Первый жанр без клауз, включающих глаголы и наречия.csv\n",
      "test_4.Первый жанр без клауз, включающих глаголы и наречия.csv\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([816])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([241])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.43837732602568236\n",
      "Epoch 2, Loss: 0.18092183794314956\n",
      "Epoch 3, Loss: 0.07660531262666279\n",
      "Epoch 4, Loss: 0.05740764207097099\n",
      "Epoch 5, Loss: 0.09418459100347451\n",
      "Epoch 6, Loss: 0.04294117273507165\n",
      "all_preds shape: (241,)\n",
      "all_labels shape: (241,)\n",
      "all_probs shape: (241, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([816])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([227])\n",
      "##########################facebook/opt-125m---gpt####################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.34889391961270105\n",
      "Epoch 2, Loss: 0.0876784393673434\n",
      "Epoch 3, Loss: 0.055257197679476996\n",
      "Epoch 4, Loss: 0.03504031558073692\n",
      "Epoch 5, Loss: 0.09279499790350468\n",
      "Epoch 6, Loss: 0.06692371523126449\n",
      "all_preds shape: (227,)\n",
      "all_labels shape: (227,)\n",
      "all_probs shape: (227, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 1, 0]), Shape: torch.Size([816])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([252])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.339545851975095\n",
      "Epoch 2, Loss: 0.1116599926323283\n",
      "Epoch 3, Loss: 0.06729970711703394\n",
      "Epoch 4, Loss: 0.032321906832329855\n",
      "Epoch 5, Loss: 0.05731640934177181\n",
      "Epoch 6, Loss: 0.07623396143682447\n",
      "all_preds shape: (252,)\n",
      "all_labels shape: (252,)\n",
      "all_probs shape: (252, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 1, 0]), Shape: torch.Size([816])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([244])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.3451970532244327\n",
      "Epoch 2, Loss: 0.10729057805649206\n",
      "Epoch 3, Loss: 0.08570047572035999\n",
      "Epoch 4, Loss: 0.07609186245712872\n",
      "Epoch 5, Loss: 0.02511929654443235\n",
      "Epoch 6, Loss: 0.04753972297333473\n",
      "all_preds shape: (244,)\n",
      "all_labels shape: (244,)\n",
      "all_probs shape: (244, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 1, 0]), Shape: torch.Size([816])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([242])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.3716084831850786\n",
      "Epoch 2, Loss: 0.11260621524507217\n",
      "Epoch 3, Loss: 0.07234013937390867\n",
      "Epoch 4, Loss: 0.04558611115185069\n",
      "Epoch 5, Loss: 0.0514520688607887\n",
      "Epoch 6, Loss: 0.044912996861681924\n",
      "all_preds shape: (242,)\n",
      "all_labels shape: (242,)\n",
      "all_probs shape: (242, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 1, 0]), Shape: torch.Size([816])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([236])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.3894808228109397\n",
      "Epoch 2, Loss: 0.11504961027051597\n",
      "Epoch 3, Loss: 0.041347226497771984\n",
      "Epoch 4, Loss: 0.08995722889827162\n",
      "Epoch 5, Loss: 0.03853897613418453\n",
      "Epoch 6, Loss: 0.03886256725572999\n",
      "all_preds shape: (236,)\n",
      "all_labels shape: (236,)\n",
      "all_probs shape: (236, 2)\n",
      "train_5.без клауз, включающих местоимения.txt.csv\n",
      "test_5.без клауз, включающих местоимения.txt.csv\n",
      "Train labels sample: tensor([1, 1, 1, 0, 0]), Shape: torch.Size([871])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([196])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.31162617369131607\n",
      "Epoch 2, Loss: 0.12577244991182604\n",
      "Epoch 3, Loss: 0.08280924874442545\n",
      "Epoch 4, Loss: 0.03081116720938801\n",
      "Epoch 5, Loss: 0.042182389112316414\n",
      "Epoch 6, Loss: 0.0690613207912644\n",
      "all_preds shape: (196,)\n",
      "all_labels shape: (196,)\n",
      "all_probs shape: (196, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([871])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([193])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.44680227366360753\n",
      "Epoch 2, Loss: 0.19654960056597537\n",
      "Epoch 3, Loss: 0.1101279210976579\n",
      "Epoch 4, Loss: 0.07187843900597231\n",
      "Epoch 5, Loss: 0.09513756731863726\n",
      "Epoch 6, Loss: 0.10704542183825239\n",
      "all_preds shape: (193,)\n",
      "all_labels shape: (193,)\n",
      "all_probs shape: (193, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 1]), Shape: torch.Size([871])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([188])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.39928630766543477\n",
      "Epoch 2, Loss: 0.16175671995363453\n",
      "Epoch 3, Loss: 0.10502683257853443\n",
      "Epoch 4, Loss: 0.0756973687347702\n",
      "Epoch 5, Loss: 0.04628171547824009\n",
      "Epoch 6, Loss: 0.016275376079879195\n",
      "all_preds shape: (188,)\n",
      "all_labels shape: (188,)\n",
      "all_probs shape: (188, 2)\n",
      "Train labels sample: tensor([1, 1, 1, 0, 1]), Shape: torch.Size([871])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([188])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.3958637861026959\n",
      "Epoch 2, Loss: 0.14424614698880098\n",
      "Epoch 3, Loss: 0.06682300671342421\n",
      "Epoch 4, Loss: 0.04794260053260421\n",
      "Epoch 5, Loss: 0.03974519400825639\n",
      "Epoch 6, Loss: 0.016638957300562075\n",
      "all_preds shape: (188,)\n",
      "all_labels shape: (188,)\n",
      "all_probs shape: (188, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([871])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([183])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.3352403956380757\n",
      "Epoch 2, Loss: 0.11104256799867884\n",
      "Epoch 3, Loss: 0.043739900280806154\n",
      "Epoch 4, Loss: 0.07311086931731552\n",
      "Epoch 5, Loss: 0.060334362875966524\n",
      "Epoch 6, Loss: 0.035199506805193695\n",
      "all_preds shape: (183,)\n",
      "all_labels shape: (183,)\n",
      "all_probs shape: (183, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([871])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([187])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.3920447661118074\n",
      "Epoch 2, Loss: 0.15154252745719118\n",
      "Epoch 3, Loss: 0.09684786435470662\n",
      "Epoch 4, Loss: 0.10622730324552818\n",
      "Epoch 5, Loss: 0.10878515716811472\n",
      "Epoch 6, Loss: 0.05990560717288066\n",
      "all_preds shape: (187,)\n",
      "all_labels shape: (187,)\n",
      "all_probs shape: (187, 2)\n",
      "train_6.без слов функциональных.txt.csv\n",
      "test_6.без слов функциональных.txt.csv\n",
      "Train labels sample: tensor([1, 1, 1, 1, 0]), Shape: torch.Size([936])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([184])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.26364803825647143\n",
      "Epoch 2, Loss: 0.10865124601720816\n",
      "Epoch 3, Loss: 0.05300116569190523\n",
      "Epoch 4, Loss: 0.0812990203345889\n",
      "Epoch 5, Loss: 0.1054918521036536\n",
      "Epoch 6, Loss: 0.052912872872646836\n",
      "all_preds shape: (184,)\n",
      "all_labels shape: (184,)\n",
      "all_probs shape: (184, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 1]), Shape: torch.Size([936])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([185])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.416774388718403\n",
      "Epoch 2, Loss: 0.13332108842303692\n",
      "Epoch 3, Loss: 0.07542916239432643\n",
      "Epoch 4, Loss: 0.0873743503875399\n",
      "Epoch 5, Loss: 0.07403768851200782\n",
      "Epoch 6, Loss: 0.02061863700657988\n",
      "all_preds shape: (185,)\n",
      "all_labels shape: (185,)\n",
      "all_probs shape: (185, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 0]), Shape: torch.Size([936])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([176])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.27436884416867113\n",
      "Epoch 2, Loss: 0.07785785395525775\n",
      "Epoch 3, Loss: 0.1516492683361521\n",
      "Epoch 4, Loss: 0.1334062042464581\n",
      "Epoch 5, Loss: 0.11511527123403247\n",
      "Epoch 6, Loss: 0.08527563177088623\n",
      "all_preds shape: (176,)\n",
      "all_labels shape: (176,)\n",
      "all_probs shape: (176, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([936])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([192])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.28797580705860915\n",
      "Epoch 2, Loss: 0.09811490591441802\n",
      "Epoch 3, Loss: 0.08604580427567332\n",
      "Epoch 4, Loss: 0.028297145466193937\n",
      "Epoch 5, Loss: 0.027445264574006138\n",
      "Epoch 6, Loss: 0.033474151779339115\n",
      "all_preds shape: (192,)\n",
      "all_labels shape: (192,)\n",
      "all_probs shape: (192, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 1]), Shape: torch.Size([936])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([188])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.30282484872613924\n",
      "Epoch 2, Loss: 0.13028321018979205\n",
      "Epoch 3, Loss: 0.08015494479529434\n",
      "Epoch 4, Loss: 0.03595872366232639\n",
      "Epoch 5, Loss: 0.08144037667425129\n",
      "Epoch 6, Loss: 0.02314557528746772\n",
      "all_preds shape: (188,)\n",
      "all_labels shape: (188,)\n",
      "all_probs shape: (188, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([936])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([189])\n",
      "##########################facebook/opt-125m---gpt####################\n",
      "Epoch 1, Loss: 0.3058821139225768\n",
      "Epoch 2, Loss: 0.09458086537039381\n",
      "Epoch 3, Loss: 0.07031404147303458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.07892738439742539\n",
      "Epoch 5, Loss: 0.062379491112936857\n",
      "Epoch 6, Loss: 0.02283562210036276\n",
      "all_preds shape: (189,)\n",
      "all_labels shape: (189,)\n",
      "all_probs shape: (189, 2)\n",
      "train_Изъяты лексемы с частотой выше 100.csv\n",
      "test_Изъяты лексемы с частотой выше 100.csv\n",
      "Train labels sample: tensor([1, 1, 0, 0, 1]), Shape: torch.Size([830])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([184])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.2519529815069221\n",
      "Epoch 2, Loss: 0.07876384236442391\n",
      "Epoch 3, Loss: 0.03640925845593301\n",
      "Epoch 4, Loss: 0.011123203076959517\n",
      "Epoch 5, Loss: 0.0010618444427002226\n",
      "Epoch 6, Loss: 0.00032499776649895187\n",
      "all_preds shape: (184,)\n",
      "all_labels shape: (184,)\n",
      "all_probs shape: (184, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([830])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([203])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.2498983482521278\n",
      "Epoch 2, Loss: 0.07001796541953809\n",
      "Epoch 3, Loss: 0.14220878968626494\n",
      "Epoch 4, Loss: 0.05228156951111137\n",
      "Epoch 5, Loss: 0.023816957521306064\n",
      "Epoch 6, Loss: 0.014980290525132799\n",
      "all_preds shape: (203,)\n",
      "all_labels shape: (203,)\n",
      "all_probs shape: (203, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 0]), Shape: torch.Size([830])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([205])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.20276123315180974\n",
      "Epoch 2, Loss: 0.07604123785194343\n",
      "Epoch 3, Loss: 0.0238277224869727\n",
      "Epoch 4, Loss: 0.03870201013618358\n",
      "Epoch 5, Loss: 0.0898016205386599\n",
      "Epoch 6, Loss: 0.038065205051680095\n",
      "all_preds shape: (205,)\n",
      "all_labels shape: (205,)\n",
      "all_probs shape: (205, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([830])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([195])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.21732208721984464\n",
      "Epoch 2, Loss: 0.06116911294856646\n",
      "Epoch 3, Loss: 0.009455982741835319\n",
      "Epoch 4, Loss: 0.024687941930796323\n",
      "Epoch 5, Loss: 0.007501411879380053\n",
      "Epoch 6, Loss: 0.0007563759980655504\n",
      "all_preds shape: (195,)\n",
      "all_labels shape: (195,)\n",
      "all_probs shape: (195, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 1, 1]), Shape: torch.Size([830])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([196])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.2705159855675167\n",
      "Epoch 2, Loss: 0.054867865250204116\n",
      "Epoch 3, Loss: 0.007985429096134585\n",
      "Epoch 4, Loss: 0.00034313248483587046\n",
      "Epoch 5, Loss: 0.00010565528373700335\n",
      "Epoch 6, Loss: 0.06942779581592386\n",
      "all_preds shape: (196,)\n",
      "all_labels shape: (196,)\n",
      "all_probs shape: (196, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([830])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([200])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.2114443260842773\n",
      "Epoch 2, Loss: 0.04761670835040814\n",
      "Epoch 3, Loss: 0.010983272183483887\n",
      "Epoch 4, Loss: 0.02573218024477123\n",
      "Epoch 5, Loss: 0.0653771216577271\n",
      "Epoch 6, Loss: 0.04477982967422087\n",
      "all_preds shape: (200,)\n",
      "all_labels shape: (200,)\n",
      "all_probs shape: (200, 2)\n",
      "train_Изъяты лексемы с частотой выше 49.csv\n",
      "test_Изъяты лексемы с частотой выше 49.csv\n",
      "Train labels sample: tensor([0, 1, 1, 0, 0]), Shape: torch.Size([844])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([203])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.23874802973543136\n",
      "Epoch 2, Loss: 0.029513660949580196\n",
      "Epoch 3, Loss: 0.03688641133557778\n",
      "Epoch 4, Loss: 0.035416372764508576\n",
      "Epoch 5, Loss: 0.02698687709239751\n",
      "Epoch 6, Loss: 0.011806416070217927\n",
      "all_preds shape: (203,)\n",
      "all_labels shape: (203,)\n",
      "all_probs shape: (203, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 1, 0]), Shape: torch.Size([844])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([211])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.19811385228158027\n",
      "Epoch 2, Loss: 0.05198759131709044\n",
      "Epoch 3, Loss: 0.016295003112023584\n",
      "Epoch 4, Loss: 0.031218145419531863\n",
      "Epoch 5, Loss: 0.025321638529073064\n",
      "Epoch 6, Loss: 0.01644805011357058\n",
      "all_preds shape: (211,)\n",
      "all_labels shape: (211,)\n",
      "all_probs shape: (211, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([844])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([207])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.23537951043064548\n",
      "Epoch 2, Loss: 0.07251915657115257\n",
      "Epoch 3, Loss: 0.04128333772170217\n",
      "Epoch 4, Loss: 0.09866022803474685\n",
      "Epoch 5, Loss: 0.011656099439209837\n",
      "Epoch 6, Loss: 0.0023366364797732116\n",
      "all_preds shape: (207,)\n",
      "all_labels shape: (207,)\n",
      "all_probs shape: (207, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([844])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([211])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.2576189236012551\n",
      "Epoch 2, Loss: 0.13367725686468887\n",
      "Epoch 3, Loss: 0.09361023745249149\n",
      "Epoch 4, Loss: 0.03960928165151755\n",
      "Epoch 5, Loss: 0.010720328848072072\n",
      "Epoch 6, Loss: 0.18041610515337078\n",
      "all_preds shape: (211,)\n",
      "all_labels shape: (211,)\n",
      "all_probs shape: (211, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 1]), Shape: torch.Size([844])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([209])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.2230854276040534\n",
      "Epoch 2, Loss: 0.09062519083172961\n",
      "Epoch 3, Loss: 0.029763551461312675\n",
      "Epoch 4, Loss: 0.010974666822942212\n",
      "Epoch 5, Loss: 0.004385326173264129\n",
      "Epoch 6, Loss: 0.0046341412738533936\n",
      "all_preds shape: (209,)\n",
      "all_labels shape: (209,)\n",
      "all_probs shape: (209, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 1]), Shape: torch.Size([844])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([204])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.21648682158370064\n",
      "Epoch 2, Loss: 0.02620347648134723\n",
      "Epoch 3, Loss: 0.009608865963906832\n",
      "Epoch 4, Loss: 0.02038147572971557\n",
      "Epoch 5, Loss: 0.07182005963086169\n",
      "Epoch 6, Loss: 0.06184816142160584\n",
      "all_preds shape: (204,)\n",
      "all_labels shape: (204,)\n",
      "all_probs shape: (204, 2)\n",
      "train_Изъяты лексемы с частотой выше 29.csv\n",
      "test_Изъяты лексемы с частотой выше 29.csv\n",
      "Train labels sample: tensor([0, 1, 1, 0, 1]), Shape: torch.Size([825])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([211])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.2155815394966899\n",
      "Epoch 2, Loss: 0.053428719416283105\n",
      "Epoch 3, Loss: 0.03729971133221765\n",
      "Epoch 4, Loss: 0.017033923973240935\n",
      "Epoch 5, Loss: 0.0012005156860281064\n",
      "Epoch 6, Loss: 0.001395768288707936\n",
      "all_preds shape: (211,)\n",
      "all_labels shape: (211,)\n",
      "all_probs shape: (211, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([825])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([222])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.3900093167590407\n",
      "Epoch 2, Loss: 0.36877471000815815\n",
      "Epoch 3, Loss: 0.10559415800246195\n",
      "Epoch 4, Loss: 0.06036421275054566\n",
      "Epoch 5, Loss: 0.04861348470456925\n",
      "Epoch 6, Loss: 0.007697816935643249\n",
      "all_preds shape: (222,)\n",
      "all_labels shape: (222,)\n",
      "all_probs shape: (222, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([825])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([218])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.22455830403711075\n",
      "Epoch 2, Loss: 0.07437039962217376\n",
      "Epoch 3, Loss: 0.2827826923435518\n",
      "Epoch 4, Loss: 0.09916487931667899\n",
      "Epoch 5, Loss: 0.06419002534620141\n",
      "Epoch 6, Loss: 0.053981637467726475\n",
      "all_preds shape: (218,)\n",
      "all_labels shape: (218,)\n",
      "all_probs shape: (218, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([825])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([226])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.3070651833236647\n",
      "Epoch 2, Loss: 0.07523917413009958\n",
      "Epoch 3, Loss: 0.0623335456397259\n",
      "Epoch 4, Loss: 0.07517447957154498\n",
      "Epoch 5, Loss: 0.059964946082730494\n",
      "Epoch 6, Loss: 0.08194735824005105\n",
      "all_preds shape: (226,)\n",
      "all_labels shape: (226,)\n",
      "all_probs shape: (226, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 0]), Shape: torch.Size([825])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([218])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.2534935151054994\n",
      "Epoch 2, Loss: 0.07085079572481426\n",
      "Epoch 3, Loss: 0.029526322146571495\n",
      "Epoch 4, Loss: 0.080867921616328\n",
      "Epoch 5, Loss: 0.3642029954991393\n",
      "Epoch 6, Loss: 0.12782829154121617\n",
      "all_preds shape: (218,)\n",
      "all_labels shape: (218,)\n",
      "all_probs shape: (218, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 0]), Shape: torch.Size([825])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([216])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.2257406951393932\n",
      "Epoch 2, Loss: 0.05463460821266171\n",
      "Epoch 3, Loss: 0.06590022544528787\n",
      "Epoch 4, Loss: 0.014535097509416608\n",
      "Epoch 5, Loss: 0.00371597259108442\n",
      "Epoch 6, Loss: 0.00019725815778971222\n",
      "all_preds shape: (216,)\n",
      "all_labels shape: (216,)\n",
      "all_probs shape: (216, 2)\n",
      "train_Изъяты лексемы с частотой выше 9.csv\n",
      "test_Изъяты лексемы с частотой выше 9.csv\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([701])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([348])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.5285529250448401\n",
      "Epoch 2, Loss: 0.1645688136870211\n",
      "Epoch 3, Loss: 0.14338114036416466\n",
      "Epoch 4, Loss: 0.10264291891574183\n",
      "all_preds shape: (348,)\n",
      "all_labels shape: (348,)\n",
      "all_probs shape: (348, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 0]), Shape: torch.Size([701])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([363])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.4560450552539392\n",
      "Epoch 2, Loss: 0.14068946860391984\n",
      "Epoch 3, Loss: 0.12555305888368326\n",
      "Epoch 4, Loss: 0.03926623860289427\n",
      "all_preds shape: (363,)\n",
      "all_labels shape: (363,)\n",
      "all_probs shape: (363, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([701])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([357])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.6397451514547522\n",
      "Epoch 2, Loss: 0.20805439894849603\n",
      "Epoch 3, Loss: 0.10577230934392322\n",
      "Epoch 4, Loss: 0.05511065569325266\n",
      "all_preds shape: (357,)\n",
      "all_labels shape: (357,)\n",
      "all_probs shape: (357, 2)\n",
      "Train labels sample: tensor([1, 1, 1, 1, 0]), Shape: torch.Size([701])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([360])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.31004774824462156\n",
      "Epoch 2, Loss: 0.1282189472324469\n",
      "Epoch 3, Loss: 0.034362874776971614\n",
      "Epoch 4, Loss: 0.05046649231702428\n",
      "all_preds shape: (360,)\n",
      "all_labels shape: (360,)\n",
      "all_probs shape: (360, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 0]), Shape: torch.Size([701])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([362])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.3776708245277405\n",
      "Epoch 2, Loss: 0.13065999470100823\n",
      "Epoch 3, Loss: 0.055974328043785965\n",
      "Epoch 4, Loss: 0.05111714888533408\n",
      "all_preds shape: (362,)\n",
      "all_labels shape: (362,)\n",
      "all_probs shape: (362, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 1]), Shape: torch.Size([701])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([365])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.4162568219683387\n",
      "Epoch 2, Loss: 0.13451516357335178\n",
      "Epoch 3, Loss: 0.08491528942249715\n",
      "Epoch 4, Loss: 0.06382739455015822\n",
      "all_preds shape: (365,)\n",
      "all_labels shape: (365,)\n",
      "all_probs shape: (365, 2)\n",
      "train_Изъяты лексемы с частотой выше 5.csv\n",
      "test_Изъяты лексемы с частотой выше 5.csv\n",
      "Train labels sample: tensor([1, 0, 0, 0, 0]), Shape: torch.Size([93])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([504])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.4666535407304764\n",
      "Epoch 2, Loss: 0.10343019850552082\n",
      "Epoch 3, Loss: 0.007362151363243659\n",
      "Epoch 4, Loss: 0.0030817478133637146\n",
      "Epoch 5, Loss: 0.00015095865152640423\n",
      "Epoch 6, Loss: 5.326012349845163e-05\n",
      "all_preds shape: (504,)\n",
      "all_labels shape: (504,)\n",
      "all_probs shape: (504, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 1]), Shape: torch.Size([93])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([466])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.282134547829628\n",
      "Epoch 2, Loss: 0.03492935856532616\n",
      "Epoch 3, Loss: 0.004008158923776743\n",
      "Epoch 4, Loss: 0.00859417676110752\n",
      "Epoch 5, Loss: 4.584815784861954e-05\n",
      "Epoch 6, Loss: 9.204265370499343e-05\n",
      "all_preds shape: (466,)\n",
      "all_labels shape: (466,)\n",
      "all_probs shape: (466, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 0]), Shape: torch.Size([93])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([422])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.41495369374752045\n",
      "Epoch 2, Loss: 0.08240864332765341\n",
      "Epoch 3, Loss: 0.07677368462706606\n",
      "Epoch 4, Loss: 0.002214247351124262\n",
      "Epoch 5, Loss: 0.005438247166845637\n",
      "Epoch 6, Loss: 0.003789764167474156\n",
      "all_preds shape: (422,)\n",
      "all_labels shape: (422,)\n",
      "all_probs shape: (422, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([93])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([483])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.4018068437774976\n",
      "Epoch 2, Loss: 0.03423183993436396\n",
      "Epoch 3, Loss: 0.0011695966277329717\n",
      "Epoch 4, Loss: 1.452943024560227e-05\n",
      "Epoch 5, Loss: 1.5930781671613659e-06\n",
      "Epoch 6, Loss: 7.578526499685267e-07\n",
      "all_preds shape: (483,)\n",
      "all_labels shape: (483,)\n",
      "all_probs shape: (483, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 1]), Shape: torch.Size([93])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([464])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.6544055143992106\n",
      "Epoch 2, Loss: 0.1686141143242518\n",
      "Epoch 3, Loss: 0.06619263735289375\n",
      "Epoch 4, Loss: 0.012454726966097951\n",
      "Epoch 5, Loss: 0.0008107925441436237\n",
      "Epoch 6, Loss: 0.00010370795628962999\n",
      "all_preds shape: (464,)\n",
      "all_labels shape: (464,)\n",
      "all_probs shape: (464, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([93])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([463])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.4037896742423375\n",
      "Epoch 2, Loss: 0.03662273660302162\n",
      "Epoch 3, Loss: 0.003089467941511733\n",
      "Epoch 4, Loss: 0.00010526584871210314\n",
      "Epoch 5, Loss: 8.025619393947636e-06\n",
      "Epoch 6, Loss: 5.558034956720803e-06\n",
      "all_preds shape: (463,)\n",
      "all_labels shape: (463,)\n",
      "all_probs shape: (463, 2)\n",
      "train_Изъяты лексемы с частотой выше 3.csv\n",
      "test_Изъяты лексемы с частотой выше 3.csv\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([88])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([677])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.43617219229539234\n",
      "Epoch 2, Loss: 0.1012405597915252\n",
      "Epoch 3, Loss: 0.03711152901329721\n",
      "Epoch 4, Loss: 0.017019101879365433\n",
      "Epoch 5, Loss: 0.001191271821880946\n",
      "Epoch 6, Loss: 0.0006929130492305072\n",
      "all_preds shape: (677,)\n",
      "all_labels shape: (677,)\n",
      "all_probs shape: (677, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([88])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([689])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.5390973749260107\n",
      "Epoch 2, Loss: 0.20910155835251013\n",
      "Epoch 3, Loss: 0.05043032253161073\n",
      "Epoch 4, Loss: 0.014250618871301413\n",
      "Epoch 5, Loss: 0.0010150938808995609\n",
      "Epoch 6, Loss: 0.0015339121940390517\n",
      "all_preds shape: (689,)\n",
      "all_labels shape: (689,)\n",
      "all_probs shape: (689, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([88])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([717])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.4879672701160113\n",
      "Epoch 2, Loss: 0.1624983251094818\n",
      "Epoch 3, Loss: 0.03805757143224279\n",
      "Epoch 4, Loss: 0.01939927627487729\n",
      "Epoch 5, Loss: 0.001127897048718296\n",
      "Epoch 6, Loss: 0.0005483151156416474\n",
      "all_preds shape: (717,)\n",
      "all_labels shape: (717,)\n",
      "all_probs shape: (717, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 0]), Shape: torch.Size([88])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([667])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.43417620410521823\n",
      "Epoch 2, Loss: 0.06844493475121756\n",
      "Epoch 3, Loss: 0.032144589291419834\n",
      "Epoch 4, Loss: 0.017262973473407328\n",
      "Epoch 5, Loss: 0.010176144942912893\n",
      "Epoch 6, Loss: 0.00037779114609293174\n",
      "all_preds shape: (667,)\n",
      "all_labels shape: (667,)\n",
      "all_probs shape: (667, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([88])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([704])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.7206833002467951\n",
      "Epoch 2, Loss: 0.3181949779391289\n",
      "Epoch 3, Loss: 0.18050112885733446\n",
      "Epoch 4, Loss: 0.07918838070084651\n",
      "Epoch 5, Loss: 0.014032576233148575\n",
      "Epoch 6, Loss: 0.010682518982018033\n",
      "all_preds shape: (704,)\n",
      "all_labels shape: (704,)\n",
      "all_probs shape: (704, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 1]), Shape: torch.Size([88])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([677])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.6273326103885969\n",
      "Epoch 2, Loss: 0.16046843926111856\n",
      "Epoch 3, Loss: 0.07209148490801454\n",
      "Epoch 4, Loss: 0.02724714563616241\n",
      "Epoch 5, Loss: 0.0038287903007585555\n",
      "Epoch 6, Loss: 0.002970752777400776\n",
      "all_preds shape: (677,)\n",
      "all_labels shape: (677,)\n",
      "all_probs shape: (677, 2)\n",
      "train_Без прилагательных первый-второй жанр.csv\n",
      "test_Без прилагательных первый-второй жанр.csv\n",
      "Train labels sample: tensor([1, 0, 1, 0, 1]), Shape: torch.Size([797])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([202])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.23203526705503463\n",
      "Epoch 2, Loss: 0.04376802120357752\n",
      "Epoch 3, Loss: 0.013407629844732583\n",
      "Epoch 4, Loss: 0.007124049342819489\n",
      "all_preds shape: (202,)\n",
      "all_labels shape: (202,)\n",
      "all_probs shape: (202, 2)\n",
      "Train labels sample: tensor([1, 1, 1, 0, 1]), Shape: torch.Size([797])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([191])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.2655216170847416\n",
      "Epoch 2, Loss: 0.08087966123595834\n",
      "Epoch 3, Loss: 0.044848604518920185\n",
      "Epoch 4, Loss: 0.03804372019134462\n",
      "all_preds shape: (191,)\n",
      "all_labels shape: (191,)\n",
      "all_probs shape: (191, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([797])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([204])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.34473862940445543\n",
      "Epoch 2, Loss: 0.0728783131763339\n",
      "Epoch 3, Loss: 0.027782140640192667\n",
      "Epoch 4, Loss: 0.03887560501694679\n",
      "all_preds shape: (204,)\n",
      "all_labels shape: (204,)\n",
      "all_probs shape: (204, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([797])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([195])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.31757875204086305\n",
      "Epoch 2, Loss: 0.06015239991247654\n",
      "Epoch 3, Loss: 0.03284487314987928\n",
      "Epoch 4, Loss: 0.024844386639306323\n",
      "all_preds shape: (195,)\n",
      "all_labels shape: (195,)\n",
      "all_probs shape: (195, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 1, 0]), Shape: torch.Size([797])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([200])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.3749828645586967\n",
      "Epoch 2, Loss: 0.07981521712616085\n",
      "Epoch 3, Loss: 0.03920861831866205\n",
      "Epoch 4, Loss: 0.009718752515036613\n",
      "all_preds shape: (200,)\n",
      "all_labels shape: (200,)\n",
      "all_probs shape: (200, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 1, 1]), Shape: torch.Size([797])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([209])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.45080864429473877\n",
      "Epoch 2, Loss: 0.09970317907631397\n",
      "Epoch 3, Loss: 0.01828074207296595\n",
      "Epoch 4, Loss: 0.012298886321950704\n",
      "all_preds shape: (209,)\n",
      "all_labels shape: (209,)\n",
      "all_probs shape: (209, 2)\n",
      "train_1.Первый жанр исходная выборка.csv\n",
      "test_1.Первый жанр исходная выборка.csv\n",
      "Train labels sample: tensor([0, 0, 0, 1, 1]), Shape: torch.Size([856])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([170])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.22024319778817394\n",
      "Epoch 2, Loss: 0.03297305147188362\n",
      "Epoch 3, Loss: 0.12657020226564514\n",
      "Epoch 4, Loss: 0.05291883840389481\n",
      "Epoch 5, Loss: 0.00561018423906245\n",
      "Epoch 6, Loss: 0.0018055521771168936\n",
      "all_preds shape: (170,)\n",
      "all_labels shape: (170,)\n",
      "all_probs shape: (170, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([856])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([174])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.20634097812688668\n",
      "Epoch 2, Loss: 0.09056351722608304\n",
      "Epoch 3, Loss: 0.0434738366986696\n",
      "Epoch 4, Loss: 0.016149166393141193\n",
      "Epoch 5, Loss: 0.017690480955463026\n",
      "Epoch 6, Loss: 0.01802542492506139\n",
      "all_preds shape: (174,)\n",
      "all_labels shape: (174,)\n",
      "all_probs shape: (174, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([856])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([163])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.16666757192514423\n",
      "Epoch 2, Loss: 0.03747132561154474\n",
      "Epoch 3, Loss: 0.013585718348959172\n",
      "Epoch 4, Loss: 0.007711701353804933\n",
      "Epoch 5, Loss: 0.020762960823019597\n",
      "Epoch 6, Loss: 0.037249860895422496\n",
      "all_preds shape: (163,)\n",
      "all_labels shape: (163,)\n",
      "all_probs shape: (163, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([856])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([167])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.1535045541585768\n",
      "Epoch 2, Loss: 0.03528652892095528\n",
      "Epoch 3, Loss: 0.0211260088864812\n",
      "Epoch 4, Loss: 0.03860992313329242\n",
      "Epoch 5, Loss: 0.0030308831146949496\n",
      "Epoch 6, Loss: 0.0003302234010704894\n",
      "all_preds shape: (167,)\n",
      "all_labels shape: (167,)\n",
      "all_probs shape: (167, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([856])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([169])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.21526808262785413\n",
      "Epoch 2, Loss: 0.06434696081689456\n",
      "Epoch 3, Loss: 0.0027792625568197225\n",
      "Epoch 4, Loss: 0.0006375355115027207\n",
      "Epoch 5, Loss: 0.026204601805937325\n",
      "Epoch 6, Loss: 0.010309942981410603\n",
      "all_preds shape: (169,)\n",
      "all_labels shape: (169,)\n",
      "all_probs shape: (169, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([856])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([167])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.3593292211147922\n",
      "Epoch 2, Loss: 0.0697320579342044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.03621752520235465\n",
      "Epoch 4, Loss: 0.021917535391265928\n",
      "Epoch 5, Loss: 0.026037222176059745\n",
      "Epoch 6, Loss: 0.020089760474469906\n",
      "all_preds shape: (167,)\n",
      "all_labels shape: (167,)\n",
      "all_probs shape: (167, 2)\n",
      "train_2.Первый жанр без клауз, включающих наречия.csv\n",
      "test_2.Первый жанр без клауз, включающих наречия.csv\n",
      "Train labels sample: tensor([0, 0, 0, 1, 1]), Shape: torch.Size([840])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([182])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.14114807347573763\n",
      "Epoch 2, Loss: 0.04471643440774642\n",
      "Epoch 3, Loss: 0.005174568980674969\n",
      "Epoch 4, Loss: 0.032632017809835406\n",
      "Epoch 5, Loss: 0.016764722270454085\n",
      "Epoch 6, Loss: 0.004841218141732626\n",
      "all_preds shape: (182,)\n",
      "all_labels shape: (182,)\n",
      "all_probs shape: (182, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 1, 0]), Shape: torch.Size([840])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([177])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.2704862722301118\n",
      "Epoch 2, Loss: 0.08683925915966886\n",
      "Epoch 3, Loss: 0.05603898286309866\n",
      "Epoch 4, Loss: 0.05253833692843744\n",
      "Epoch 5, Loss: 0.7261885819640362\n",
      "Epoch 6, Loss: 0.5947110759762099\n",
      "all_preds shape: (177,)\n",
      "all_labels shape: (177,)\n",
      "all_probs shape: (177, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 1]), Shape: torch.Size([840])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([182])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.2485541607042388\n",
      "Epoch 2, Loss: 0.10038627234489639\n",
      "Epoch 3, Loss: 0.0364572885048132\n",
      "Epoch 4, Loss: 0.004325734567056264\n",
      "Epoch 5, Loss: 0.07148020207894464\n",
      "Epoch 6, Loss: 0.03236321723313765\n",
      "all_preds shape: (182,)\n",
      "all_labels shape: (182,)\n",
      "all_probs shape: (182, 2)\n",
      "Train labels sample: tensor([0, 1, 0, 0, 0]), Shape: torch.Size([840])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([175])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.3306870345742319\n",
      "Epoch 2, Loss: 0.06400169800189412\n",
      "Epoch 3, Loss: 0.0241140878181641\n",
      "Epoch 4, Loss: 0.04940061707336253\n",
      "Epoch 5, Loss: 0.020646221757796977\n",
      "Epoch 6, Loss: 0.017490517215693852\n",
      "all_preds shape: (175,)\n",
      "all_labels shape: (175,)\n",
      "all_probs shape: (175, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 1, 1]), Shape: torch.Size([840])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([176])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.21866335333237108\n",
      "Epoch 2, Loss: 0.0860307797247858\n",
      "Epoch 3, Loss: 0.019907077751616632\n",
      "Epoch 4, Loss: 0.04152510548815286\n",
      "Epoch 5, Loss: 0.022860018737189403\n",
      "Epoch 6, Loss: 0.033304129294128015\n",
      "all_preds shape: (176,)\n",
      "all_labels shape: (176,)\n",
      "all_probs shape: (176, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([840])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([181])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.2816385242929858\n",
      "Epoch 2, Loss: 0.07372629964495746\n",
      "Epoch 3, Loss: 0.11698554389130429\n",
      "Epoch 4, Loss: 0.021544007928756433\n",
      "Epoch 5, Loss: 0.0024036020547083476\n",
      "Epoch 6, Loss: 0.0011639509365917514\n",
      "all_preds shape: (181,)\n",
      "all_labels shape: (181,)\n",
      "all_probs shape: (181, 2)\n",
      "train_3.Первый жанр без клауз, включающих глаголы.csv\n",
      "test_3.Первый жанр без клауз, включающих глаголы.csv\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([789])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([219])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.27262678628554565\n",
      "Epoch 2, Loss: 0.06364189118146896\n",
      "Epoch 3, Loss: 0.04606569698400562\n",
      "Epoch 4, Loss: 0.030996577732730658\n",
      "Epoch 5, Loss: 0.013226990751372797\n",
      "Epoch 6, Loss: 0.041572948723487574\n",
      "all_preds shape: (219,)\n",
      "all_labels shape: (219,)\n",
      "all_probs shape: (219, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 0]), Shape: torch.Size([789])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([215])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.153337324061431\n",
      "Epoch 2, Loss: 0.021488284061197193\n",
      "Epoch 3, Loss: 0.04442842124713934\n",
      "Epoch 4, Loss: 0.011738987009630364\n",
      "Epoch 5, Loss: 0.1007670931541361\n",
      "Epoch 6, Loss: 0.02541742321336642\n",
      "all_preds shape: (215,)\n",
      "all_labels shape: (215,)\n",
      "all_probs shape: (215, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([789])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([218])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.20640371328336188\n",
      "Epoch 2, Loss: 0.07388545178633649\n",
      "Epoch 3, Loss: 0.02903495221398771\n",
      "Epoch 4, Loss: 0.001933596571907401\n",
      "Epoch 5, Loss: 0.08469980754590324\n",
      "Epoch 6, Loss: 0.06335851742747764\n",
      "all_preds shape: (218,)\n",
      "all_labels shape: (218,)\n",
      "all_probs shape: (218, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 1, 0]), Shape: torch.Size([789])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([219])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.2899102064408362\n",
      "Epoch 2, Loss: 0.04150021226363606\n",
      "Epoch 3, Loss: 0.036956653847155393\n",
      "Epoch 4, Loss: 0.038844514426073146\n",
      "Epoch 5, Loss: 0.07809897931671003\n",
      "Epoch 6, Loss: 0.009543955738772638\n",
      "all_preds shape: (219,)\n",
      "all_labels shape: (219,)\n",
      "all_probs shape: (219, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([789])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([225])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.19778635942842812\n",
      "Epoch 2, Loss: 0.05964402639132459\n",
      "Epoch 3, Loss: 0.06330779878117028\n",
      "Epoch 4, Loss: 0.0058522250093665205\n",
      "Epoch 5, Loss: 0.0034877005416637987\n",
      "Epoch 6, Loss: 0.00031386808031129475\n",
      "all_preds shape: (225,)\n",
      "all_labels shape: (225,)\n",
      "all_probs shape: (225, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 1]), Shape: torch.Size([789])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([229])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.17510924307629466\n",
      "Epoch 2, Loss: 0.03079456176783424\n",
      "Epoch 3, Loss: 0.01064322041325795\n",
      "Epoch 4, Loss: 0.0007125488973724714\n",
      "Epoch 5, Loss: 0.0012277366055212723\n",
      "Epoch 6, Loss: 0.027197783724639067\n",
      "all_preds shape: (229,)\n",
      "all_labels shape: (229,)\n",
      "all_probs shape: (229, 2)\n",
      "train_4.Первый жанр без клауз, включающих глаголы и наречия.csv\n",
      "test_4.Первый жанр без клауз, включающих глаголы и наречия.csv\n",
      "Train labels sample: tensor([1, 0, 0, 0, 1]), Shape: torch.Size([768])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([243])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.2767781444223753\n",
      "Epoch 2, Loss: 0.044161666890128494\n",
      "Epoch 3, Loss: 0.14032801324719912\n",
      "Epoch 4, Loss: 0.03998975630505205\n",
      "Epoch 5, Loss: 0.009079731644836405\n",
      "Epoch 6, Loss: 0.006951296622219161\n",
      "all_preds shape: (243,)\n",
      "all_labels shape: (243,)\n",
      "all_probs shape: (243, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 1, 1]), Shape: torch.Size([768])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([247])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.2970257053093519\n",
      "Epoch 2, Loss: 0.045529002405601204\n",
      "Epoch 3, Loss: 0.04135531345273572\n",
      "Epoch 4, Loss: 0.06590069881228071\n",
      "Epoch 5, Loss: 0.030471103005766054\n",
      "Epoch 6, Loss: 0.0156089363377608\n",
      "all_preds shape: (247,)\n",
      "all_labels shape: (247,)\n",
      "all_probs shape: (247, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 0, 0]), Shape: torch.Size([768])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([238])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.4551239528421623\n",
      "Epoch 2, Loss: 0.15014397766935872\n",
      "Epoch 3, Loss: 0.04687150580866728\n",
      "Epoch 4, Loss: 0.012808273673120615\n",
      "Epoch 5, Loss: 0.011554144134947819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.0011468363828347112\n",
      "all_preds shape: (238,)\n",
      "all_labels shape: (238,)\n",
      "all_probs shape: (238, 2)\n",
      "Train labels sample: tensor([1, 1, 1, 1, 1]), Shape: torch.Size([768])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([250])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.28196541920866974\n",
      "Epoch 2, Loss: 0.055791372161062704\n",
      "Epoch 3, Loss: 0.01662124648388878\n",
      "Epoch 4, Loss: 0.006277429808316508\n",
      "Epoch 5, Loss: 0.056863441041324826\n",
      "Epoch 6, Loss: 0.02573689605196705\n",
      "all_preds shape: (250,)\n",
      "all_labels shape: (250,)\n",
      "all_probs shape: (250, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 1, 0]), Shape: torch.Size([768])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([245])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.1880098757270995\n",
      "Epoch 2, Loss: 0.042428759539082726\n",
      "Epoch 3, Loss: 0.021698317178864574\n",
      "Epoch 4, Loss: 0.004424026111090977\n",
      "Epoch 5, Loss: 0.02619920974850724\n",
      "Epoch 6, Loss: 0.033608849970354036\n",
      "all_preds shape: (245,)\n",
      "all_labels shape: (245,)\n",
      "all_probs shape: (245, 2)\n",
      "Train labels sample: tensor([0, 1, 1, 0, 1]), Shape: torch.Size([768])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([246])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.29292195589126396\n",
      "Epoch 2, Loss: 0.05232519434260515\n",
      "Epoch 3, Loss: 0.020425336287189566\n",
      "Epoch 4, Loss: 0.0401726078151133\n",
      "Epoch 5, Loss: 0.2672093104144248\n",
      "Epoch 6, Loss: 0.05383566089161226\n",
      "all_preds shape: (246,)\n",
      "all_labels shape: (246,)\n",
      "all_probs shape: (246, 2)\n",
      "train_5.без клауз, включающих местоимения.txt.csv\n",
      "test_5.без клауз, включающих местоимения.txt.csv\n",
      "Train labels sample: tensor([1, 1, 1, 0, 1]), Shape: torch.Size([824])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([190])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.12783302473802066\n",
      "Epoch 2, Loss: 0.015257344440005208\n",
      "Epoch 3, Loss: 0.0027746134963728343\n",
      "Epoch 4, Loss: 0.03138844523520955\n",
      "Epoch 5, Loss: 0.06533188488594113\n",
      "Epoch 6, Loss: 0.005126260068515456\n",
      "all_preds shape: (190,)\n",
      "all_labels shape: (190,)\n",
      "all_probs shape: (190, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([824])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([189])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.26929547395019865\n",
      "Epoch 2, Loss: 0.0698430514178024\n",
      "Epoch 3, Loss: 0.14566473068239597\n",
      "Epoch 4, Loss: 0.016376676064178006\n",
      "Epoch 5, Loss: 0.1975458607573728\n",
      "Epoch 6, Loss: 0.04536246247544813\n",
      "all_preds shape: (189,)\n",
      "all_labels shape: (189,)\n",
      "all_probs shape: (189, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([824])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([189])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.2541254011493248\n",
      "Epoch 2, Loss: 0.04353773808375431\n",
      "Epoch 3, Loss: 0.012065817703632301\n",
      "Epoch 4, Loss: 0.002784942771191149\n",
      "Epoch 5, Loss: 0.0002570534374556771\n",
      "Epoch 6, Loss: 0.0001518612589476421\n",
      "all_preds shape: (189,)\n",
      "all_labels shape: (189,)\n",
      "all_probs shape: (189, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 0, 1]), Shape: torch.Size([824])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([199])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.2535136606132325\n",
      "Epoch 2, Loss: 0.07463520124124792\n",
      "Epoch 3, Loss: 0.048039316228648835\n",
      "Epoch 4, Loss: 0.028543922785991944\n",
      "Epoch 5, Loss: 0.003975796064976906\n",
      "Epoch 6, Loss: 0.0028823300440362887\n",
      "all_preds shape: (199,)\n",
      "all_labels shape: (199,)\n",
      "all_probs shape: (199, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 0]), Shape: torch.Size([824])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([184])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.2208683954892107\n",
      "Epoch 2, Loss: 0.033313612346249175\n",
      "Epoch 3, Loss: 0.039126830816642796\n",
      "Epoch 4, Loss: 0.003993816439044797\n",
      "Epoch 5, Loss: 0.0005977210470947424\n",
      "Epoch 6, Loss: 0.00012168058712693262\n",
      "all_preds shape: (184,)\n",
      "all_labels shape: (184,)\n",
      "all_probs shape: (184, 2)\n",
      "Train labels sample: tensor([1, 0, 1, 0, 1]), Shape: torch.Size([824])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([198])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.24060108301749167\n",
      "Epoch 2, Loss: 0.1425868445456637\n",
      "Epoch 3, Loss: 0.09830760304556371\n",
      "Epoch 4, Loss: 0.05728686742636805\n",
      "Epoch 5, Loss: 0.030819642604071235\n",
      "Epoch 6, Loss: 0.03932245561274893\n",
      "all_preds shape: (198,)\n",
      "all_labels shape: (198,)\n",
      "all_probs shape: (198, 2)\n",
      "train_6.без слов функциональных.txt.csv\n",
      "test_6.без слов функциональных.txt.csv\n",
      "Train labels sample: tensor([1, 0, 0, 0, 1]), Shape: torch.Size([876])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([199])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.17850286304019392\n",
      "Epoch 2, Loss: 0.018641962684837796\n",
      "Epoch 3, Loss: 0.05274839206668234\n",
      "Epoch 4, Loss: 0.061965396117871434\n",
      "Epoch 5, Loss: 0.010131734579904746\n",
      "Epoch 6, Loss: 0.03854301909050264\n",
      "all_preds shape: (199,)\n",
      "all_labels shape: (199,)\n",
      "all_probs shape: (199, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([876])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([197])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.2306526835330508\n",
      "Epoch 2, Loss: 0.028174502069172873\n",
      "Epoch 3, Loss: 0.006510919538149293\n",
      "Epoch 4, Loss: 0.0182665611613841\n",
      "Epoch 5, Loss: 0.03292291704504102\n",
      "Epoch 6, Loss: 0.06324732267689383\n",
      "all_preds shape: (197,)\n",
      "all_labels shape: (197,)\n",
      "all_probs shape: (197, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 0]), Shape: torch.Size([876])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([199])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.2080864164639603\n",
      "Epoch 2, Loss: 0.0802162106720392\n",
      "Epoch 3, Loss: 0.04541478643355235\n",
      "Epoch 4, Loss: 0.008480136980704794\n",
      "Epoch 5, Loss: 0.008368627960060786\n",
      "Epoch 6, Loss: 0.0004630514092820125\n",
      "all_preds shape: (199,)\n",
      "all_labels shape: (199,)\n",
      "all_probs shape: (199, 2)\n",
      "Train labels sample: tensor([1, 0, 0, 0, 1]), Shape: torch.Size([876])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([197])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.16882493313063274\n",
      "Epoch 2, Loss: 0.014521024135915054\n",
      "Epoch 3, Loss: 0.05163279742171818\n",
      "Epoch 4, Loss: 0.014619092460965145\n",
      "Epoch 5, Loss: 0.006665200731351268\n",
      "Epoch 6, Loss: 0.0007285660558491972\n",
      "all_preds shape: (197,)\n",
      "all_labels shape: (197,)\n",
      "all_probs shape: (197, 2)\n",
      "Train labels sample: tensor([1, 1, 0, 1, 0]), Shape: torch.Size([876])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([192])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.25592432977229523\n",
      "Epoch 2, Loss: 0.024697112730196254\n",
      "Epoch 3, Loss: 0.029742793059631366\n",
      "Epoch 4, Loss: 0.025474351201145592\n",
      "Epoch 5, Loss: 0.06167726956813236\n",
      "Epoch 6, Loss: 0.08554985877549784\n",
      "all_preds shape: (192,)\n",
      "all_labels shape: (192,)\n",
      "all_probs shape: (192, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 1]), Shape: torch.Size([876])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([195])\n",
      "##########################sberbank-ai/rugpt3small_based_on_gpt2---gpt####################\n",
      "Epoch 1, Loss: 0.19063221731709995\n",
      "Epoch 2, Loss: 0.025633982037172907\n",
      "Epoch 3, Loss: 0.008354835412054937\n",
      "Epoch 4, Loss: 0.06878774659893348\n",
      "Epoch 5, Loss: 0.0022479163180336896\n",
      "Epoch 6, Loss: 0.00031312469557178704\n",
      "all_preds shape: (195,)\n",
      "all_labels shape: (195,)\n",
      "all_probs shape: (195, 2)\n"
     ]
    }
   ],
   "source": [
    "import funciones\n",
    "from utils import train_wrapper\n",
    "import warnings\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "from contextlib import redirect_stderr\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "#import wandb\n",
    "import nbformat\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import gc\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from razdel import sentenize\n",
    "import numpy as np\n",
    "\n",
    "# Suprimir warnings específicos\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def convert_numpy(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    if isinstance(obj, (np.float32, np.float64, np.int32, np.int64)):\n",
    "        return obj.item()\n",
    "    return obj\n",
    "\n",
    "def save_results(results, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=4, ensure_ascii=False, default=convert_numpy)\n",
    "\n",
    "        \n",
    "def create_custom_config(model_name, model_type, dataset):\n",
    "    \"\"\"Crea configuración de entrenamiento adaptativa\"\"\"\n",
    "    common_config = {\n",
    "#         'dataset_name':dataset.get('name', ''),\n",
    "        'model_name': model_name,\n",
    "        'model_type': model_type,\n",
    "        'num_repeats': 6,\n",
    "        'test_size': 0.2,\n",
    "        'threshold': 0.5\n",
    "    }\n",
    "\n",
    "    # Configuraciones basadas en frecuencia\n",
    "    freq_threshold = dataset.get('freq_threshold')\n",
    "    if freq_threshold in [100, 49, 29, 9, 5, 3]:\n",
    "        configs = {\n",
    "            100: (52, 16, 6, 2e-5),\n",
    "            49: (60, 16, 6, 3e-5),\n",
    "            29: (51, 16, 6, 3e-5),\n",
    "            9: (45, 16, 6, 4e-5),\n",
    "            5: (150, 16, 6, 5e-5),\n",
    "            3: (150, 16, 6, 5e-5)\n",
    "        }\n",
    "        max_len, batch, epochs, lr = configs[freq_threshold]\n",
    "        return funciones.TrainingConfig(\n",
    "            **common_config,\n",
    "            max_length=max_len,\n",
    "            batch_size=batch,\n",
    "            epochs=epochs,\n",
    "            learning_rate=lr\n",
    "        )\n",
    "\n",
    "    # Configuración por nombre de dataset\n",
    "    dataset_name = dataset.get('name', '')\n",
    "    for x in ['1', '2', '3', '4', '5', '6']:\n",
    "        if x in dataset_name:\n",
    "            configs = {\n",
    "                '1': (60, 16, 6, 5e-5),\n",
    "                '2': (60, 16, 6, 5e-5),\n",
    "                '3': (60, 16, 6, 5e-5),\n",
    "                '4': (60, 16, 6, 5e-5),\n",
    "                '5': (60, 16, 6, 5e-5),\n",
    "                '6': (60, 16, 6, 5e-5)\n",
    "            }\n",
    "            max_len, batch, epochs, lr = configs[x]\n",
    "            return funciones.TrainingConfig(\n",
    "                **common_config,\n",
    "                max_length=max_len,\n",
    "                batch_size=batch,\n",
    "                epochs=epochs,\n",
    "                learning_rate=lr\n",
    "            )\n",
    "\n",
    "\n",
    "    # Configuración por defecto\n",
    "    return funciones.TrainingConfig(\n",
    "        **common_config,\n",
    "        max_length=60,\n",
    "        batch_size=32,\n",
    "        epochs=4,\n",
    "        learning_rate=3e-5\n",
    "    )\n",
    "\n",
    "models = [\n",
    "    {'model':'DeepPavlov/rubert-base-cased',\n",
    "     'name':'DeepPavlov-rubert-base',\n",
    "     'type': 'bert'}, # Modelo original (ruso)\n",
    "    {'model':'bert-base-multilingual-cased',\n",
    "     'name':'BERT multilingual',\n",
    "     'type': 'bert'}, # BERT multilingüe\n",
    "    {'model':'distilbert-base-multilingual-cased',\n",
    "     'name':'distilbert-base-multilingual',\n",
    "     'type': 'bert'}, # Versión ligera de BERT multilingüe\n",
    "    {'model':'roberta-base',\n",
    "     'name':'roberta-base',\n",
    "     'type': 'bert'}, # RoBERTa (principalmente inglés)\n",
    "\n",
    "    {'model':'xlnet-base-cased',\n",
    "     'name':'XLNet Base',\n",
    "     'type': 'bert'}, # XLNet (multilingüe efectivo)\n",
    "    {'model':'xlm-roberta-base',\n",
    "     'name':'XLM-RoBERTa Base',\n",
    "     'type': 'bert'}, # XLM-RoBERTa (multilingüe)\n",
    "\n",
    "\n",
    "    {'model': 'gpt2',\n",
    "     'name': 'gpt2',\n",
    "     'type': 'gpt'},  # GPT model\n",
    "    {'model': 'facebook/opt-125m',\n",
    "     'name': 'facebook/opt-125m',\n",
    "     'type': 'gpt'},  # GPT model\n",
    "    {'model': 'sberbank-ai/rugpt3small_based_on_gpt2',\n",
    "     'name': 'rugpt3small',\n",
    "     'type': 'gpt'}, # GPT model (ruso)\n",
    "#         {\n",
    "#         \"model\": \"facebook/mbart-large-50\",\n",
    "#         \"name\": \"mBART Large 50\",\n",
    "#         \"type\": \"gpt\"\n",
    "#         }\n",
    "    ]\n",
    "\n",
    "datasets = [\n",
    "    #deberia adicionar el set original?\n",
    "        {\n",
    "            'path1': '../dataset/сокращение по частотности/1б_Изъяты лексемы с частотой выше 100.txt',\n",
    "            'path2': '../dataset/сокращение по частотности/2б_Изъяты лексемы с частотой выше 100.txt',\n",
    "            'name': 'Изъяты лексемы с частотой выше 100',\n",
    "            'type': 'freq',\n",
    "            'freq': 100\n",
    "        },\n",
    "        {\n",
    "            'path1': '../dataset/сокращение по частотности/1в_Изъяты лексемы с частотой выше 49.txt',\n",
    "            'path2': '../dataset/сокращение по частотности/2в_Изъяты лексемы с частотой выше 49.txt',\n",
    "            'name': 'Изъяты лексемы с частотой выше 49',\n",
    "            'type': 'freq',\n",
    "            'freq': 49\n",
    "        },\n",
    "        {\n",
    "            'path1': '../dataset/сокращение по частотности/1г_Изъяты лексемы с частотой выше 29.txt',\n",
    "            'path2': '../dataset/сокращение по частотности/2г_Изъяты лексемы с частотой выше 29.txt',\n",
    "            'name': 'Изъяты лексемы с частотой выше 29',\n",
    "            'type': 'freq',\n",
    "            'freq': 29\n",
    "        },\n",
    "        {\n",
    "            'path1': '../dataset/сокращение по частотности/1д_Изъяты лексемы с частотой выше 9.txt',\n",
    "            'path2': '../dataset/сокращение по частотности/2д_Изъяты лексемы с частотой выше 9.txt',\n",
    "            'name': 'Изъяты лексемы с частотой выше 9',\n",
    "            'type': 'freq',\n",
    "            'freq': 9\n",
    "        },\n",
    "        {\n",
    "            'path1': '../dataset/сокращение по частотности/1е_Изъяты лексемы с частотой выше 5.txt',\n",
    "            'path2': '../dataset/сокращение по частотности/2е_Изъяты лексемы с частотой выше 5.txt',\n",
    "            'name': 'Изъяты лексемы с частотой выше 5',\n",
    "            'type': 'freq',\n",
    "            'freq': 5\n",
    "        },\n",
    "        {\n",
    "            'path1': '../dataset/сокращение по частотности/1ё_Изъяты лексемы с частотой выше 3.txt',\n",
    "            'path2': '../dataset/сокращение по частотности/2ё_Изъяты лексемы с частотой выше 3.txt',\n",
    "            'name': 'Изъяты лексемы с частотой выше 3',\n",
    "            'type': 'freq',\n",
    "            'freq': 3\n",
    "        },\n",
    "\n",
    "            {\n",
    "            'path1': '../dataset/Сокращение по частям речи/Без прилагательных первый жанр.txt',\n",
    "            'path2': '../dataset/Сокращение по частям речи/Без прилагательных второй жанр.txt',\n",
    "            'name': 'Без прилагательных первый-второй жанр',#litle correction\n",
    "            'type': 'pos',\n",
    "            'freq': None\n",
    "            },\n",
    "          {\n",
    "            'path1': '../dataset/Сокращение по частям речи/1.Первый жанр исходная выборка.txt',\n",
    "            'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "            'name': '1.Первый жанр исходная выборка',# este el el orignal\n",
    "            'type': 'pos',\n",
    "            'freq': None\n",
    "\n",
    "        },\n",
    "        {\n",
    "            'path1': '../dataset/Сокращение по частям речи/2.Первый жанр без клауз, включающих наречия.txt',\n",
    "            'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "            'name': '2.Первый жанр без клауз, включающих наречия',\n",
    "            'type': 'pos',\n",
    "            'freq': None\n",
    "        },\n",
    "        {\n",
    "            'path1': '../dataset/Сокращение по частям речи/3.Первый жанр без клауз, включающих глаголы.txt',\n",
    "            'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "            'name': '3.Первый жанр без клауз, включающих глаголы',\n",
    "            'type': 'pos',\n",
    "            'freq': None\n",
    "        },\n",
    "        {\n",
    "            'path1': '../dataset/Сокращение по частям речи/4.Первый жанр без клауз, включающих глаголы и наречия.txt',\n",
    "            'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "            'name': '4.Первый жанр без клауз, включающих глаголы и наречия',\n",
    "            'type': 'pos',\n",
    "            'freq': None\n",
    "        },\n",
    "        {\n",
    "            'path1': '../dataset/Сокращение по частям речи/5.без клауз, включающих местоимения.txt',\n",
    "            'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "            'name': '5.без клауз, включающих местоимения.txt',\n",
    "            'type': 'pos',\n",
    "            'freq': None\n",
    "        },\n",
    "        {\n",
    "            'path1': '../dataset/Сокращение по частям речи/6.без слов функциональных.txt',\n",
    "            'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "            'name': '6.без слов функциональных.txt',\n",
    "            'type': 'pos',\n",
    "            'freq': None\n",
    "        },\n",
    "    \n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "russian_templates = [\n",
    "    \"Этот литературный фрагмент принадлежит к жанру {}.\",\n",
    "    \"Данный текст является примером жанра {}.\",\n",
    "    \"Стилистические особенности указывают на жанр {}.\",\n",
    "    \"Жанровая принадлежность этого текста - {}.\",\n",
    "    \"Эксперты классифицируют этот текст как жанр {}.\",\n",
    "    \"Это типичный пример жанра {} в русской литературе.\",\n",
    "    \"По ключевым характеристикам это текст жанра {}.\",\n",
    "    \"Данное произведение относится к направлению {}.\",\n",
    "    \"Анализ содержания позволяет отнести текст к жанру {}.\",\n",
    "    \"Этот отрывок характерен для жанра {}.\"\n",
    "]\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    results = []\n",
    "    for model in models:\n",
    "        for dataset in datasets:\n",
    "            config = create_custom_config(model['model'], model['type'], dataset)\n",
    "            \n",
    "            \n",
    "            # Entrenar y limpiar memoria\n",
    "            torch.cuda.empty_cache()\n",
    "            result = funciones.train_and_evaluate_dataset(\n",
    "                dataset['path1'],\n",
    "                dataset['path2'],\n",
    "                config,\n",
    "                dataset['name'],\n",
    "                dataset['type']\n",
    "                \n",
    "            )\n",
    "            results.append(result) \n",
    "#             if model['type'] = 'gpt': #aplicar zero shot\n",
    "                \n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    \n",
    "    save_results(results, 'resultados_Step_8_test_2.json') \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cd31457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7dfbb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1ac659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import IPython\n",
    "\n",
    "# Reiniciar el kernel\n",
    "IPython.display.display(IPython.display.Javascript(\"Jupyter.notebook.kernel.restart()\"))\n",
    "\n",
    "# Apagar el kernel después de reiniciar\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8fee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## hacer zero shot para todos los  modelos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "467642de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a652ae34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluando modelo: Rugpt3small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando: 100%|██████████| 1064/1064 [01:22<00:00, 12.88it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import gc\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from razdel import sentenize\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sklearn.metrics import (accuracy_score, f1_score, roc_auc_score, \n",
    "                            average_precision_score, log_loss, confusion_matrix)\n",
    "\n",
    "# Configuración de warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Clase de configuración para el experimento zero-shot.\"\"\"\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.max_length = 256\n",
    "        self.min_length_threshold = 6\n",
    "        self.num_repeats = 1\n",
    "        self.model_type = 'gpt'\n",
    "        self.genres = (\"определение\", \"описание\")\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"Cargador de datos mejorado con manejo de errores y validación.\"\"\"\n",
    "    def __init__(self, file_paths: Tuple[str, str]):\n",
    "        self.file_paths = file_paths\n",
    "        self.validate_paths()\n",
    "    \n",
    "    def validate_paths(self):\n",
    "        \"\"\"Valida que los archivos existan.\"\"\"\n",
    "        for path in self.file_paths:\n",
    "            if not os.path.exists(path):\n",
    "                raise FileNotFoundError(f\"Archivo no encontrado: {path}\")\n",
    "    \n",
    "    def load_file(self, path: str, label: int) -> List[Tuple[str, int]]:\n",
    "        \"\"\"Carga un archivo y devuelve una lista de (texto, etiqueta).\"\"\"\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            return [(line.strip(), label) for line in f if line.strip()]\n",
    "    \n",
    "    def __call__(self) -> Dict[str, List[Tuple[str, int]]]:\n",
    "        \"\"\"Carga todos los datos.\"\"\"\n",
    "        data1 = self.load_file(self.file_paths[0], 0)\n",
    "        data2 = self.load_file(self.file_paths[1], 1)\n",
    "        return {'original_data': data1 + data2}\n",
    "\n",
    "class TextProcessor:\n",
    "    \"\"\"Procesador de texto con limpieza y división en oraciones.\"\"\"\n",
    "    def __init__(self, tokenizer, min_length: int = 6):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.min_length = min_length\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Limpia el texto para una mejor división en oraciones.\"\"\"\n",
    "        # Manejo de casos especiales de puntuación\n",
    "        replacements = [\n",
    "            (r'\\.,', '. TEMP_MARKER ,'),\n",
    "            (r'\\.;', '. TEMP_MARKER ;'),\n",
    "            (r'\\. ([a-zа-я])', r'. TEMP_MARKER \\1'),\n",
    "            (r'(\\w)([А-Я])', r'\\1. \\2'),\n",
    "            (r'\\.(\\s*\\d)', r'\\1')  # Eliminar puntos antes de números\n",
    "        ]\n",
    "        \n",
    "        for pattern, repl in replacements:\n",
    "            text = re.sub(pattern, repl, text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def split_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"Divide el texto en oraciones usando razdel.\"\"\"\n",
    "        return [sentence.text for sentence in sentenize(text)]\n",
    "    \n",
    "    def process_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Procesa un texto completo y devuelve oraciones limpias.\"\"\"\n",
    "        cleaned = self.clean_text(text)\n",
    "        no_brackets = re.sub(r'\\[.*?\\]', '', cleaned).strip()\n",
    "        sentences = self.split_sentences(no_brackets)\n",
    "        return [re.sub(r'\\s*TEMP_MARKER', ' ', s).strip() for s in sentences]\n",
    "    \n",
    "    def filter_sentences(self, sentences: List[str]) -> List[str]:\n",
    "        \"\"\"Filtra oraciones basadas en la longitud de tokens.\"\"\"\n",
    "        return [\n",
    "            s for s in sentences \n",
    "            if len(self.tokenizer.encode(s, truncation=False)) >= self.min_length\n",
    "        ]\n",
    "    \n",
    "    def __call__(self, data: List[Tuple[str, int]]) -> List[Tuple[str, int]]:\n",
    "        \"\"\"Procesa una lista de textos con etiquetas.\"\"\"\n",
    "        processed = []\n",
    "        for text, label in data:\n",
    "            sentences = self.process_text(text)\n",
    "            valid_sentences = self.filter_sentences(sentences)\n",
    "            processed.extend([(s, label) for s in valid_sentences])\n",
    "        return processed\n",
    "\n",
    "class ZeroShotClassifier:\n",
    "    \"\"\"Clasificador zero-shot con manejo de modelos generativos.\"\"\"\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.model = self.load_model()\n",
    "    \n",
    "    def load_model(self) -> AutoModelForCausalLM:\n",
    "        \"\"\"Carga el modelo con configuraciones optimizadas.\"\"\"\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.config.model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True\n",
    "        ).to(self.config.device)\n",
    "        model.eval()\n",
    "        return model\n",
    "    \n",
    "    def create_prompt(self, text: str) -> str:\n",
    "        \"\"\"Genera el prompt para clasificación zero-shot.\"\"\"\n",
    "        genre1, genre2 = self.config.genres\n",
    "        return f\"\"\"Определите жанр текста (только цифру):\n",
    "1 - {genre1} (строгое определение понятия)\n",
    "2 - {genre2} (подробное описание характеристик)\n",
    "\n",
    "Текст: {text}\n",
    "Жанр (1 или 2): \"\"\"\n",
    "    \n",
    "    def parse_response(self, response: str) -> int:\n",
    "        \"\"\"Interpreta la respuesta del modelo.\"\"\"\n",
    "        response = response.strip().lower()\n",
    "        genre1, genre2 = (g.lower() for g in self.config.genres)\n",
    "        \n",
    "        if genre1 in response:\n",
    "            return 0\n",
    "        elif genre2 in response:\n",
    "            return 1\n",
    "        return -1  # Respuesta no válida\n",
    "    \n",
    "    def predict(self, text: str) -> Tuple[int, str]:\n",
    "        \"\"\"Realiza una predicción para un texto dado.\"\"\"\n",
    "        prompt = self.create_prompt(text)\n",
    "        inputs = self.config.tokenizer(\n",
    "            prompt, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True, \n",
    "            max_length=self.config.max_length\n",
    "        ).to(self.config.device)\n",
    "        \n",
    "        try:\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=10,\n",
    "                pad_token_id=self.config.tokenizer.pad_token_id\n",
    "            )\n",
    "            response = self.config.tokenizer.decode(\n",
    "                outputs[0], \n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            return self.parse_response(response), response\n",
    "        except Exception as e:\n",
    "            print(f\"Error en predicción: {e}\")\n",
    "            return -1, \"\"\n",
    "    \n",
    "    def evaluate(self, test_data: List[Tuple[str, int]]) -> Dict[str, Any]:\n",
    "        \"\"\"Evalúa el modelo en un conjunto de datos.\"\"\"\n",
    "        true_labels = []\n",
    "        predictions = []\n",
    "        logits = []\n",
    "        responses = []\n",
    "        \n",
    "        for text, label in tqdm(test_data, desc=\"Evaluando\"):\n",
    "            pred, response = self.predict(text)\n",
    "            if pred != -1:\n",
    "                true_labels.append(label)\n",
    "                predictions.append(pred)\n",
    "                logits.append([1.0 - pred, pred])  # Logits simulados\n",
    "                responses.append(response)\n",
    "        \n",
    "        if not true_labels:\n",
    "            return self.empty_metrics()\n",
    "        \n",
    "        return self.calculate_metrics(true_labels, predictions, logits, responses)\n",
    "    \n",
    "    @staticmethod\n",
    "    def empty_metrics() -> Dict[str, Any]:\n",
    "        \"\"\"Métricas vacías para casos sin predicciones válidas.\"\"\"\n",
    "        return {\n",
    "            'accuracy': 0.0,\n",
    "            'f1_scores': {'weighted': 0.0, 'macro': 0.0, 'class_0': 0.0, 'class_1': 0.0},\n",
    "            'roc_auc': 0.0,\n",
    "            'pr_auc': 0.0,\n",
    "            'log_loss': 0.0,\n",
    "            'confusion_matrix': [[0, 0], [0, 0]],\n",
    "            'true_labels': [],\n",
    "            'predictions': [],\n",
    "            'responses': []\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_metrics(true_labels, predictions, logits, responses) -> Dict[str, Any]:\n",
    "        \"\"\"Calcula todas las métricas de evaluación.\"\"\"\n",
    "        return {\n",
    "            'accuracy': accuracy_score(true_labels, predictions),\n",
    "            'f1_scores': {\n",
    "                'weighted': f1_score(true_labels, predictions, average='weighted'),\n",
    "                'macro': f1_score(true_labels, predictions, average='macro'),\n",
    "                'class_0': f1_score(true_labels, predictions, average=None)[0],\n",
    "                'class_1': f1_score(true_labels, predictions, average=None)[1]\n",
    "            },\n",
    "            'roc_auc': roc_auc_score(true_labels, [l[1] for l in logits]),\n",
    "            'pr_auc': average_precision_score(true_labels, [l[1] for l in logits]),\n",
    "            'log_loss': log_loss(true_labels, logits),\n",
    "            'confusion_matrix': confusion_matrix(true_labels, predictions).tolist(),\n",
    "            'true_labels': true_labels,\n",
    "            'predictions': predictions,\n",
    "            'responses': responses\n",
    "        }\n",
    "\n",
    "class ExperimentRunner:\n",
    "    \"\"\"Ejecuta el experimento completo con múltiples modelos.\"\"\"\n",
    "    def __init__(self, data_paths: Tuple[str, str], models: List[Dict[str, str]]):\n",
    "        self.data_paths = data_paths\n",
    "        self.models = models\n",
    "        self.results = []\n",
    "    \n",
    "    def run_single_model(self, model_info: Dict[str, str]) -> Dict[str, Any]:\n",
    "        \"\"\"Ejecuta el experimento para un solo modelo.\"\"\"\n",
    "        print(f\"\\nEvaluando modelo: {model_info['name']}\")\n",
    "        \n",
    "        config = Config(model_info['model'])\n",
    "        data_loader = DataLoader(self.data_paths)\n",
    "        processor = TextProcessor(config.tokenizer, config.min_length_threshold)\n",
    "        \n",
    "        # Cargar y procesar datos\n",
    "        raw_data = data_loader()['original_data']\n",
    "        processed_data = processor(raw_data)\n",
    "        \n",
    "        # Ejecutar múltiples repeticiones\n",
    "        metrics = {\n",
    "            'model_name': model_info['name'],\n",
    "            'dataset_name': 'Datos originales',\n",
    "            'repetitions': []\n",
    "        }\n",
    "        \n",
    "        for _ in range(config.num_repeats):\n",
    "            classifier = ZeroShotClassifier(config)\n",
    "            result = classifier.evaluate(processed_data)\n",
    "            metrics['repetitions'].append(result)\n",
    "            \n",
    "            # Limpieza de memoria\n",
    "            del classifier\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calcular promedios\n",
    "        self.calculate_averages(metrics)\n",
    "        return metrics\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_averages(metrics: Dict[str, Any]):\n",
    "        \"\"\"Calcula promedios y desviaciones estándar de las repeticiones.\"\"\"\n",
    "        keys = ['accuracy', 'roc_auc', 'pr_auc', 'log_loss']\n",
    "        f1_keys = ['weighted', 'macro', 'class_0', 'class_1']\n",
    "        \n",
    "        # Métricas principales\n",
    "        for key in keys:\n",
    "            values = [rep[key] for rep in metrics['repetitions']]\n",
    "            metrics[f'avg_{key}'] = float(np.mean(values))\n",
    "            metrics[f'std_{key}'] = float(np.std(values))\n",
    "        \n",
    "        # Métricas F1\n",
    "        f1_metrics = {}\n",
    "        for f_key in f1_keys:\n",
    "            values = [rep['f1_scores'][f_key] for rep in metrics['repetitions']]\n",
    "            f1_metrics[f'avg_{f_key}'] = float(np.mean(values))\n",
    "            f1_metrics[f'std_{f_key}'] = float(np.std(values))\n",
    "        metrics['f1_metrics'] = f1_metrics\n",
    "        \n",
    "        # Matriz de confusión promedio\n",
    "        conf_matrices = [rep['confusion_matrix'] for rep in metrics['repetitions']]\n",
    "        metrics['avg_confusion_matrix'] = np.mean(conf_matrices, axis=0).tolist()\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Ejecuta el experimento para todos los modelos.\"\"\"\n",
    "        for model in self.models:\n",
    "            try:\n",
    "                result = self.run_single_model(model)\n",
    "                self.results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error con modelo {model['name']}: {str(e)}\")\n",
    "        \n",
    "        self.save_results('zero_shot_results.json')\n",
    "    \n",
    "    def save_results(self, filename: str):\n",
    "        \"\"\"Guarda los resultados en un archivo JSON.\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.results, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "def main():\n",
    "    # Configuración del experimento\n",
    "    DATA_PATHS = (\n",
    "        '../dataset/Первый_жанр_исходная.txt',\n",
    "        '../dataset/Второй_жанр_исходная.txt'\n",
    "    )\n",
    "    \n",
    "    MODELS = [\n",
    "#         {'model': 'gpt2', 'name': 'GPT-2'},\n",
    "#         {'model': 'facebook/opt-125m', 'name': 'OPT-125M'},#no clasifica nada en ruso\n",
    "        {'model': 'sberbank-ai/rugpt3small_based_on_gpt2', 'name': 'Rugpt3small'}\n",
    "    ]\n",
    "    \n",
    "    # Ejecutar experimento\n",
    "    runner = ExperimentRunner(DATA_PATHS, MODELS)\n",
    "    runner.run()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4b0888f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7390/492774795.py:8: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resumen de Métricas:\n",
      "\n",
      "Modelo: Rugpt3small\n",
      "Accuracy: 0.615 ± 0.000\n",
      "F1 Macro: 0.381 ± 0.000\n",
      "ROC AUC: 0.500 ± 0.000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuración de estilos para los gráficos\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "def load_results(filename: str = 'zero_shot_results.json') -> list:\n",
    "    \"\"\"Carga los resultados desde el archivo JSON.\"\"\"\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def plot_metrics(results: list):\n",
    "    \"\"\"Genera gráficos para las métricas principales.\"\"\"\n",
    "    # Extraer datos en formato DataFrame\n",
    "    metrics_data = []\n",
    "    for model in results:\n",
    "        model_name = model['model_name']\n",
    "        metrics = {\n",
    "            'Modelo': model_name,\n",
    "            'Accuracy': model['avg_accuracy'],\n",
    "            'F1 Macro': model['f1_metrics']['avg_macro'],\n",
    "            'ROC AUC': model['avg_roc_auc'],\n",
    "            'PR AUC': model['avg_pr_auc']\n",
    "        }\n",
    "        metrics_data.append(metrics)\n",
    "    \n",
    "    df = pd.DataFrame(metrics_data).set_index('Modelo')\n",
    "    \n",
    "    # Gráfico de barras para métricas principales\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    df.plot(kind='bar', ax=ax)\n",
    "    ax.set_title('Comparación de Métricas por Modelo', fontsize=14)\n",
    "    ax.set_ylabel('Puntuación', fontsize=12)\n",
    "    ax.set_xlabel('Modelo', fontsize=12)\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('metricas_principales.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Gráfico de matriz de confusión promedio para cada modelo\n",
    "    for model in results:\n",
    "        fig, ax = plt.subplots(figsize=(6, 5))\n",
    "        conf_matrix = np.array(model['avg_confusion_matrix'])\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt='.1f', cmap='Blues', \n",
    "                    xticklabels=['Definición', 'Descripción'],\n",
    "                    yticklabels=['Definición', 'Descripción'])\n",
    "        ax.set_title(f'Matriz de Confusión - {model[\"model_name\"]}')\n",
    "        ax.set_xlabel('Predicción')\n",
    "        ax.set_ylabel('Verdadero')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'matriz_confusion_{model[\"model_name\"]}.png', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "def plot_f1_scores(results: list):\n",
    "    \"\"\"Gráfico detallado de las métricas F1.\"\"\"\n",
    "    f1_data = []\n",
    "    for model in results:\n",
    "        model_name = model['model_name']\n",
    "        f1_data.append({\n",
    "            'Modelo': model_name,\n",
    "            'F1 Macro': model['f1_metrics']['avg_macro'],\n",
    "            'F1 Ponderado': model['f1_metrics']['avg_weighted'],\n",
    "            'F1 Definición': model['f1_metrics']['avg_class_0'],\n",
    "            'F1 Descripción': model['f1_metrics']['avg_class_1']\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(f1_data).set_index('Modelo')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    df.plot(kind='bar', ax=ax)\n",
    "    ax.set_title('Desglose de Métricas F1', fontsize=14)\n",
    "    ax.set_ylabel('Puntuación F1', fontsize=12)\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('f1_scores.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def plot_error_bars(results: list):\n",
    "    \"\"\"Gráfico con barras de error para mostrar variabilidad.\"\"\"\n",
    "    metrics = ['accuracy', 'roc_auc', 'pr_auc']\n",
    "    model_names = [m['model_name'] for m in results]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        means = [m[f'avg_{metric}'] for m in results]\n",
    "        stds = [m[f'std_{metric}'] for m in results]\n",
    "        \n",
    "        axes[i].bar(model_names, means, yerr=stds, capsize=5)\n",
    "        axes[i].set_title(metric.upper())\n",
    "        axes[i].set_ylim(0, 1)\n",
    "        axes[i].grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.setp(axes[i].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    fig.suptitle('Métricas con Barras de Error (media ± desviación estándar)', y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('metricas_con_error_bars.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    results = load_results()\n",
    "    \n",
    "    print(\"\\nResumen de Métricas:\")\n",
    "    for model in results:\n",
    "        print(f\"\\nModelo: {model['model_name']}\")\n",
    "        print(f\"Accuracy: {model['avg_accuracy']:.3f} ± {model['std_accuracy']:.3f}\")\n",
    "        print(f\"F1 Macro: {model['f1_metrics']['avg_macro']:.3f} ± {model['f1_metrics']['std_macro']:.3f}\")\n",
    "        print(f\"ROC AUC: {model['avg_roc_auc']:.3f} ± {model['std_roc_auc']:.3f}\")\n",
    "    \n",
    "    plot_metrics(results)\n",
    "    plot_f1_scores(results)\n",
    "    plot_error_bars(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e312904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2025-05-04 12:37:26.056652: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Текст: Фотосинтез - это процесс преобразования света в хи...\n",
      "Истинный класс: Определение\n",
      "Ответ модели: 'эт'\n",
      "Предсказание: ✓ (Определение)\n",
      "\n",
      "Текст: Лес наполнен высокими соснами, их стволы покрыты г...\n",
      "Истинный класс: Описание\n",
      "Ответ модели: 'тол'\n",
      "Предсказание: ✗ (Определение)\n",
      "\n",
      "Текст: Квадрат - геометрическая фигура с четырьмя равными...\n",
      "Истинный класс: Определение\n",
      "Ответ модели: 'хо'\n",
      "Предсказание: ✓ (Определение)\n",
      "\n",
      "Текст: Река бурлила, неся пенящиеся воды между крутыми бе...\n",
      "Истинный класс: Описание\n",
      "Ответ модели: 'то'\n",
      "Предсказание: ✗ (Определение)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 1. Configuración con manejo de errores\n",
    "model_name = \"facebook/opt-125m\"\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    # Configuración importante para evitar overflow\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar el modelo: {str(e)}\")\n",
    "    exit()\n",
    "\n",
    "# 2. Ejemplos de prueba\n",
    "test_cases = [\n",
    "    (\"Фотосинтез - это процесс преобразования света в химическую энергию\", 0),\n",
    "    (\"Лес наполнен высокими соснами, их стволы покрыты грубой корой\", 1),\n",
    "    (\"Квадрат - геометрическая фигура с четырьмя равными сторонами\", 0),\n",
    "    (\"Река бурлила, неся пенящиеся воды между крутыми берегами\", 1)\n",
    "]\n",
    "\n",
    "# 3. Prompt optimizado\n",
    "def create_prompt(text: str) -> str:\n",
    "    return f\"\"\"Определи тип текста (ответь только цифрой):\n",
    "1 - определение понятия\n",
    "2 - описание объекта\n",
    "\n",
    "Текст: {text}\n",
    "Тип: \"\"\"\n",
    "\n",
    "# 4. Función de generación segura\n",
    "def safe_generate(text: str) -> str:\n",
    "    try:\n",
    "        prompt = create_prompt(text)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Configuración crítica para evitar overflow\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=3,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50\n",
    "        )\n",
    "        \n",
    "        # Decodificación segura\n",
    "        response = tokenizer.decode(\n",
    "            outputs[0][inputs.input_ids.shape[-1]:], \n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "        \n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error en generación: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "# 5. Test mejorado\n",
    "def run_test():\n",
    "    for text, true_label in test_cases:\n",
    "        response = safe_generate(text)\n",
    "        pred = -1\n",
    "        \n",
    "        # Análisis robusto de la respuesta\n",
    "        if response:\n",
    "            if \"1\" in response[:2]:  # Busca en los primeros caracteres\n",
    "                pred = 0\n",
    "            elif \"2\" in response[:2]:\n",
    "                pred = 1\n",
    "            else:\n",
    "                # Búsqueda de palabras clave si no hay números\n",
    "                desc_words = [\"описание\", \"описан\", \"2\"]\n",
    "                if any(word in response.lower() for word in desc_words):\n",
    "                    pred = 1\n",
    "                else:\n",
    "                    pred = 0  # Por defecto como definición\n",
    "        \n",
    "        print(f\"\\nТекст: {text[:50]}...\")\n",
    "        print(f\"Истинный класс: {'Описание' if true_label else 'Определение'}\")\n",
    "        print(f\"Ответ модели: '{response}'\")\n",
    "        print(f\"Предсказание: {'✓' if pred == true_label else '✗'} ({'Описание' if pred else 'Определение'})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fa5dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182ca189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mi Entorno (Python 3.9)",
   "language": "python",
   "name": "mi_entorno"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
