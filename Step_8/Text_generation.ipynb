{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60a20057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m../dataset\u001b[0m\r\n",
      "├── \u001b[01;34men_espanol\u001b[0m\r\n",
      "│   ├── \u001b[00mdocx2txt.py\u001b[0m\r\n",
      "│   ├── \u001b[00mВторой_жанр_исходная.txt\u001b[0m\r\n",
      "│   └── \u001b[00mПервый_жанр_исходная.txt\u001b[0m\r\n",
      "├── \u001b[00mВторой_жанр_исходная.txt\u001b[0m\r\n",
      "├── \u001b[00mПервый_жанр_исходная.txt\u001b[0m\r\n",
      "├── \u001b[01;34mСокращение по частям речи\u001b[0m\r\n",
      "│   ├── \u001b[00m1.Первый жанр исходная выборка.txt\u001b[0m\r\n",
      "│   ├── \u001b[00m2.Первый жанр без клауз, включающих наречия.txt\u001b[0m\r\n",
      "│   ├── \u001b[00m3.Первый жанр без клауз, включающих глаголы.txt\u001b[0m\r\n",
      "│   ├── \u001b[00m4.Первый жанр без клауз, включающих глаголы и наречия.txt\u001b[0m\r\n",
      "│   ├── \u001b[00m5.без клауз, включающих существительные.txt\u001b[0m\r\n",
      "│   ├── \u001b[00m6.без клауз, включающих местоимения.txt\u001b[0m\r\n",
      "│   ├── \u001b[00m7.без клауз, включающих существительные и прилагательные.txt\u001b[0m\r\n",
      "│   ├── \u001b[00m8.без слов функциональных.txt\u001b[0m\r\n",
      "│   ├── \u001b[00mБез прилагательных второй жанр.txt\u001b[0m\r\n",
      "│   ├── \u001b[00mБез прилагательных первый жанр.txt\u001b[0m\r\n",
      "│   ├── \u001b[00mСлучайные выборки.txt\u001b[0m\r\n",
      "│   ├── \u001b[00mбез клауз, включающих местоимения.txt\u001b[0m\r\n",
      "│   └── \u001b[00mбез слов функциональных.txt\u001b[0m\r\n",
      "└── \u001b[01;34mсокращение по частотности\u001b[0m\r\n",
      "    ├── \u001b[00m1а_ без сокращений.txt\u001b[0m\r\n",
      "    ├── \u001b[00m1б_Изъяты лексемы с частотой выше 100.txt\u001b[0m\r\n",
      "    ├── \u001b[00m1в_Изъяты лексемы с частотой выше 49.txt\u001b[0m\r\n",
      "    ├── \u001b[00m1г_Изъяты лексемы с частотой выше 29.txt\u001b[0m\r\n",
      "    ├── \u001b[00m1д_Изъяты лексемы с частотой выше 9.txt\u001b[0m\r\n",
      "    ├── \u001b[00m1е_Изъяты лексемы с частотой выше 5.txt\u001b[0m\r\n",
      "    ├── \u001b[00m1ё_Изъяты лексемы с частотой выше 3.txt\u001b[0m\r\n",
      "    ├── \u001b[00m2а_ без сокращений.txt\u001b[0m\r\n",
      "    ├── \u001b[00m2б_Изъяты лексемы с частотой выше 100.txt\u001b[0m\r\n",
      "    ├── \u001b[00m2в_Изъяты лексемы с частотой выше 49.txt\u001b[0m\r\n",
      "    ├── \u001b[00m2г_Изъяты лексемы с частотой выше 29.txt\u001b[0m\r\n",
      "    ├── \u001b[00m2д_Изъяты лексемы с частотой выше 9.txt\u001b[0m\r\n",
      "    ├── \u001b[00m2е_Изъяты лексемы с частотой выше 5.txt\u001b[0m\r\n",
      "    └── \u001b[00m2ё_Изъяты лексемы с частотой выше 3.txt\u001b[0m\r\n",
      "\r\n",
      "3 directories, 32 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree ../dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0997943",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 7.76MB/s]                    \n",
      "2025-04-22 18:33:07 INFO: Downloaded file to /root/stanza_resources/resources.json\n",
      "2025-04-22 18:33:07 INFO: Downloading default packages for language: ru (Russian) ...\n",
      "2025-04-22 18:33:08 INFO: File exists: /root/stanza_resources/ru/default.zip\n",
      "2025-04-22 18:33:11 INFO: Finished downloading models and saved to /root/stanza_resources\n",
      "2025-04-22 18:33:11 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 15.6MB/s]                    \n",
      "2025-04-22 18:33:11 INFO: Downloaded file to /root/stanza_resources/resources.json\n",
      "2025-04-22 18:33:11 INFO: Loading these models for language: ru (Russian):\n",
      "==================================\n",
      "| Processor | Package            |\n",
      "----------------------------------\n",
      "| tokenize  | syntagrus          |\n",
      "| pos       | syntagrus_charlm   |\n",
      "| lemma     | syntagrus_nocharlm |\n",
      "==================================\n",
      "\n",
      "2025-04-22 18:33:11 INFO: Using device: cuda\n",
      "2025-04-22 18:33:11 INFO: Loading: tokenize\n",
      "2025-04-22 18:33:11 INFO: Loading: pos\n",
      "2025-04-22 18:33:14 INFO: Loading: lemma\n",
      "2025-04-22 18:33:16 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generado: ../dataset/Сокращение по частям речи/5.без клауз, включающих местоимения.txt\n",
      "Generado: ../dataset/Сокращение по частям речи/6.без слов функциональных.txt\n",
      "Generado: ../dataset/Сокращение по частям речи/Второй_жанр без клауз, включающих местоимения.txt\n",
      "Generado: ../dataset/Сокращение по частям речи/Второй_жанр без слов функциональных.txt\n",
      "Procesamiento completado.\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configurar Stanza para ruso\n",
    "stanza.download('ru')\n",
    "nlp = stanza.Pipeline('ru', processors='tokenize,pos,lemma')\n",
    "\n",
    "# Directorios\n",
    "input_dir = '.'  # Directorio con Первый_жанр_исходная.txt y Второй_жанр_исходная.txt\n",
    "output_dir = '../dataset/Сокращение по частям речи'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Archivos de entrada\n",
    "input_files = {\n",
    "    'Первый_жанр': '../dataset/Первый_жанр_исходная.txt',\n",
    "    'Второй_жанр': '../dataset/Второй_жанр_исходная.txt'\n",
    "}\n",
    "\n",
    "# Configuración de modificaciones\n",
    "modifications = [\n",
    "#     {\n",
    "#         'name': 'без клауз, включающих существительные',\n",
    "#         'prefix': {'Первый_жанр': '5.', 'Второй_жанр': 'Второй_жанр Без существительных'},\n",
    "#         'pos_to_remove': ['NOUN'],\n",
    "#         'remove_clause': True\n",
    "#     },\n",
    "    {\n",
    "        'name': 'без клауз, включающих местоимения',#funciona\n",
    "        'prefix': {'Первый_жанр': '5.', 'Второй_жанр': 'Второй_жанр '},\n",
    "        'pos_to_remove': ['PRON'],\n",
    "        'remove_clause': True\n",
    "    },\n",
    "#     {\n",
    "#         'name': 'без клауз, включающих существительные и прилагательные',\n",
    "#         'prefix': {'Первый_жанр': '7.', 'Второй_жанр': 'Второй_жанр Без существительных и прилагательных'},\n",
    "#         'pos_to_remove': ['NOUN', 'ADJ'],\n",
    "#         'remove_clause': True\n",
    "#     },\n",
    "    {\n",
    "        'name': 'без слов функциональных',#funciona\n",
    "        'prefix': {'Первый_жанр': '6.', 'Второй_жанр': 'Второй_жанр '},\n",
    "        'pos_to_remove': ['PART', 'ADP', 'CCONJ'],\n",
    "        'remove_clause': False\n",
    "    }\n",
    "]\n",
    "\n",
    "def process_text(text, modification):\n",
    "    doc = nlp(text)\n",
    "    output_sentences = []\n",
    "    pos_to_remove = modification['pos_to_remove']\n",
    "    remove_clause = modification['remove_clause']\n",
    "    \n",
    "    for sent in doc.sentences:\n",
    "        if remove_clause:\n",
    "            # Verificar si la oración contiene alguna palabra con las POS a eliminar\n",
    "            has_pos = any(word.upos in pos_to_remove for word in sent.words)\n",
    "            if not has_pos:\n",
    "                output_sentences.append(sent.text)\n",
    "        else:\n",
    "            # Eliminar palabras individuales con las POS especificadas\n",
    "            filtered_words = [word.text for word in sent.words if word.upos not in pos_to_remove]\n",
    "            if filtered_words:  # Solo añadir si hay palabras restantes\n",
    "                output_sentences.append(' '.join(filtered_words))\n",
    "    \n",
    "    return '\\n'.join(output_sentences)\n",
    "\n",
    "# Procesar cada archivo\n",
    "for genre, input_file in input_files.items():\n",
    "    input_path = Path(input_dir) / input_file\n",
    "    if not input_path.exists():\n",
    "        print(f\"Archivo no encontrado: {input_path}\")\n",
    "        continue\n",
    "    \n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    for mod in modifications:\n",
    "        # Generar texto modificado\n",
    "        modified_text = process_text(text, mod)\n",
    "        if not modified_text.strip():\n",
    "            print(f\"Advertencia: El texto procesado para {mod['name']} en {genre} está vacío\")\n",
    "        \n",
    "        # Crear nombre del archivo de salida\n",
    "        prefix = mod['prefix'][genre]\n",
    "        output_filename = f\"{prefix}{mod['name']}.txt\"\n",
    "        output_path = Path(output_dir) / output_filename\n",
    "        \n",
    "        # Guardar resultado\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(modified_text)\n",
    "        print(f\"Generado: {output_path}\")\n",
    "\n",
    "print(\"Procesamiento completado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4081602c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
