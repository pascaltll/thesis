{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e66a70c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m../dataset\u001b[0m\r\n",
      "├── \u001b[01;34men_espanol\u001b[0m\r\n",
      "│   ├── \u001b[00mdocx2txt.py\u001b[0m\r\n",
      "│   ├── \u001b[00mВторой_жанр_исходная.txt\u001b[0m\r\n",
      "│   └── \u001b[00mПервый_жанр_исходная.txt\u001b[0m\r\n",
      "├── \u001b[00mВторой_жанр_исходная.txt\u001b[0m\r\n",
      "├── \u001b[00mПервый_жанр_исходная.txt\u001b[0m\r\n",
      "├── \u001b[01;34mСокращение по частям речи\u001b[0m\r\n",
      "│   ├── \u001b[00m1.Первый жанр исходная выборка.txt\u001b[0m\r\n",
      "│   ├── \u001b[00m2.Первый жанр без клауз, включающих наречия.txt\u001b[0m\r\n",
      "│   ├── \u001b[00m3.Первый жанр без клауз, включающих глаголы.txt\u001b[0m\r\n",
      "│   ├── \u001b[00m4.Первый жанр без клауз, включающих глаголы и наречия.txt\u001b[0m\r\n",
      "│   ├── \u001b[00m5.без клауз, включающих местоимения.txt\u001b[0m\r\n",
      "│   ├── \u001b[00m6.без слов функциональных.txt\u001b[0m\r\n",
      "│   ├── \u001b[00mБез прилагательных второй жанр.txt\u001b[0m\r\n",
      "│   ├── \u001b[00mБез прилагательных первый жанр.txt\u001b[0m\r\n",
      "│   ├── \u001b[00mВторой_жанр без клауз, включающих местоимения.txt\u001b[0m\r\n",
      "│   ├── \u001b[00mВторой_жанр без слов функциональных.txt\u001b[0m\r\n",
      "│   └── \u001b[00mСлучайные выборки.txt\u001b[0m\r\n",
      "└── \u001b[01;34mсокращение по частотности\u001b[0m\r\n",
      "    ├── \u001b[00m1а_ без сокращений.txt\u001b[0m\r\n",
      "    ├── \u001b[00m1б_Изъяты лексемы с частотой выше 100.txt\u001b[0m\r\n",
      "    ├── \u001b[00m1в_Изъяты лексемы с частотой выше 49.txt\u001b[0m\r\n",
      "    ├── \u001b[00m1г_Изъяты лексемы с частотой выше 29.txt\u001b[0m\r\n",
      "    ├── \u001b[00m1д_Изъяты лексемы с частотой выше 9.txt\u001b[0m\r\n",
      "    ├── \u001b[00m1е_Изъяты лексемы с частотой выше 5.txt\u001b[0m\r\n",
      "    ├── \u001b[00m1ё_Изъяты лексемы с частотой выше 3.txt\u001b[0m\r\n",
      "    ├── \u001b[00m2а_ без сокращений.txt\u001b[0m\r\n",
      "    ├── \u001b[00m2б_Изъяты лексемы с частотой выше 100.txt\u001b[0m\r\n",
      "    ├── \u001b[00m2в_Изъяты лексемы с частотой выше 49.txt\u001b[0m\r\n",
      "    ├── \u001b[00m2г_Изъяты лексемы с частотой выше 29.txt\u001b[0m\r\n",
      "    ├── \u001b[00m2д_Изъяты лексемы с частотой выше 9.txt\u001b[0m\r\n",
      "    ├── \u001b[00m2е_Изъяты лексемы с частотой выше 5.txt\u001b[0m\r\n",
      "    └── \u001b[00m2ё_Изъяты лексемы с частотой выше 3.txt\u001b[0m\r\n",
      "\r\n",
      "3 directories, 30 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree ../dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b299fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_results(results, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c11038bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfunciones\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_wrapper\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/notebooks/thesis/Step_8/funciones.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrazdel\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m  \u001b[38;5;66;03m# Importar torchfrom torch.utils.data import Dataset, DataLoader\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/__init__.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev  \u001b[38;5;66;03m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hashtable \u001b[38;5;28;01mas\u001b[39;00m _hashtable, lib \u001b[38;5;28;01mas\u001b[39;00m _lib, tslib \u001b[38;5;28;01mas\u001b[39;00m _tslib\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/compat/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m F\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     is_numpy_dev,\n\u001b[1;32m     20\u001b[0m     np_version_under1p21,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     pa_version_under1p01,\n\u001b[1;32m     24\u001b[0m     pa_version_under2p0,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     pa_version_under9p0,\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/compat/numpy/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\" support numpy compatibility across versions \"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# numpy versioning\u001b[39;00m\n\u001b[1;32m      7\u001b[0m _np_version \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39m__version__\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# pyright: reportUnusedImport = false\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     Appender,\n\u001b[1;32m      4\u001b[0m     Substitution,\n\u001b[1;32m      5\u001b[0m     cache_readonly,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhashing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     hash_array,\n\u001b[1;32m     10\u001b[0m     hash_pandas_object,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(name):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py:14\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     Any,\n\u001b[1;32m      8\u001b[0m     Callable,\n\u001b[1;32m      9\u001b[0m     Mapping,\n\u001b[1;32m     10\u001b[0m     cast,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproperties\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cache_readonly\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     F,\n\u001b[1;32m     17\u001b[0m     T,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_exceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find_stack_level\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/__init__.py:13\u001b[0m\n\u001b[1;32m      1\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaTType\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterval\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m ]\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     NaT,\n\u001b[1;32m     16\u001b[0m     NaTType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     iNaT,\n\u001b[1;32m     22\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/interval.pyx:1\u001b[0m, in \u001b[0;36minit pandas._libs.interval\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import funciones\n",
    "from utils import train_wrapper\n",
    "import warnings\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "from contextlib import redirect_stderr\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "#import wandb\n",
    "import nbformat\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import gc\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from razdel import sentenize\n",
    "import numpy as np\n",
    "\n",
    "# Suprimir warnings específicos\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def convert_numpy(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    if isinstance(obj, (np.float32, np.float64, np.int32, np.int64)):\n",
    "        return obj.item()\n",
    "    return obj\n",
    "\n",
    "def save_results(results, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=4, ensure_ascii=False, default=convert_numpy)\n",
    "\n",
    "# import funciones\n",
    "# from utils import train_wrapper\n",
    "# import warnings\n",
    "# import os\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Suprimir warnings específicos\n",
    "# warnings.filterwarnings('ignore', category=UserWarning)  # Para sklearn y otros\n",
    "# warnings.filterwarnings('ignore', category=FutureWarning)  # Para huggingface y transformers\n",
    "# from concurrent.futures import ProcessPoolExecutor\n",
    "# from tqdm import tqdm\n",
    "# from contextlib import redirect_stderr\n",
    "# from concurrent.futures import ProcessPoolExecutor\n",
    "# import multiprocessing as mp\n",
    "# import numpy as np\n",
    "# mp.set_start_method('spawn', force=True)\n",
    "# #os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "\n",
    "###############################################################################\n",
    "# import wandb\n",
    "# import nbformat\n",
    "# # Configurar la clave de la API como variable de entorno\n",
    "# os.environ[\"WANDB_API_KEY\"] = \"ca85316ed713b2425615fb3a613d7eb414c9f57f\"  # Reemplaza con tu clave\n",
    "# # Iniciar wandb sin especificar entity (se detecta automáticamente)\n",
    "# wandb.init(project=\"model-clasification\")\n",
    "# wandb.init(\n",
    "#     settings=wandb.Settings(\n",
    "#         start_method=\"thread\",\n",
    "#         timeout=30,\n",
    "#         sync_dir=\"/tmp/wandb\",  # Usa un directorio temporal\n",
    "#         disable_code=True       # Mejora la estabilidad\n",
    "#     )\n",
    "# )\n",
    "#os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"сокращение по частотности.ipynb\"\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def create_custom_config(model_name, model_type, dataset):\n",
    "    \"\"\"Crea configuración de entrenamiento adaptativa\"\"\"\n",
    "    common_config = {\n",
    "#         'dataset_name':dataset.get('name', ''),\n",
    "        'model_name': model_name,\n",
    "        'model_type': model_type,\n",
    "        'num_repeats': 6,\n",
    "        'test_size': 0.2,\n",
    "        'threshold': 0.5\n",
    "    }\n",
    "\n",
    "    # Configuración específica para GPT\n",
    "    if model_type == 'gpt':\n",
    "        return funciones.TrainingConfig(\n",
    "            **common_config,\n",
    "            max_length=52,\n",
    "            batch_size=64,\n",
    "            epochs=6,\n",
    "            learning_rate=1e-5\n",
    "        )\n",
    "#         freq_threshold = dataset.get('freq_threshold')\n",
    "#         if freq_threshold in [100, 49, 29, 9, 5, 3]:\n",
    "#             configs = {\n",
    "#                 100: (52, 32, 6, 1e-5),#test 1\n",
    "#                 49: (60, 128, 4, 3e-5),\n",
    "#                 29: (51, 128, 4, 3e-5),\n",
    "#                 9: (45, 128, 5, 4e-5),\n",
    "#                 5: (150, 32, 6, 5e-5),\n",
    "#                 3: (150, 32, 6, 5e-5)\n",
    "#             }\n",
    "#             max_len, batch, epochs, lr = configs[freq_threshold]\n",
    "#             return funciones.TrainingConfig(\n",
    "#                 **common_config,\n",
    "#                 max_length=max_len,\n",
    "#                 batch_size=batch,\n",
    "#                 epochs=epochs,\n",
    "#                 learning_rate=lr\n",
    "#             )\n",
    "\n",
    "        # Configuración por nombre de dataset\n",
    "        dataset_name = dataset.get('name', '')\n",
    "        if any(x in dataset_name for x in ['1', '2', '3', '4']):\n",
    "            return funciones.TrainingConfig(\n",
    "                **common_config,\n",
    "                max_length=60,\n",
    "                batch_size=128,\n",
    "                epochs=6,\n",
    "                learning_rate=5e-5\n",
    "            )\n",
    "\n",
    "\n",
    "    # Configuraciones basadas en frecuencia\n",
    "    freq_threshold = dataset.get('freq_threshold')\n",
    "    if freq_threshold in [100, 49, 29, 9, 5, 3]:\n",
    "        configs = {\n",
    "            100: (52, 128, 3, 2e-5),\n",
    "            49: (60, 128, 4, 3e-5),\n",
    "            29: (51, 128, 4, 3e-5),\n",
    "            9: (45, 128, 5, 4e-5),\n",
    "            5: (150, 32, 6, 5e-5),\n",
    "            3: (150, 32, 6, 5e-5)\n",
    "        }\n",
    "        max_len, batch, epochs, lr = configs[freq_threshold]\n",
    "        return funciones.TrainingConfig(\n",
    "            **common_config,\n",
    "            max_length=max_len,\n",
    "            batch_size=batch,\n",
    "            epochs=epochs,\n",
    "            learning_rate=lr\n",
    "        )\n",
    "\n",
    "    # Configuración por nombre de dataset\n",
    "    dataset_name = dataset.get('name', '')\n",
    "    if any(x in dataset_name for x in ['1', '2', '3', '4','5','6']):\n",
    "        configs = {\n",
    "            1: (60, 128, 3, 5e-5),\n",
    "            2: (60, 128, 4, 5e-5),\n",
    "            3: (60, 128, 4, 5e-5),\n",
    "            4: (60, 128, 5, 5e-5),\n",
    "            5: (60, 128, 6, 5e-5),\n",
    "            6: (60, 128, 6, 5e-5)\n",
    "        }\n",
    "        max_len, batch, epochs, lr = configs[x]\n",
    "        \n",
    "        return funciones.TrainingConfig(\n",
    "            **common_config,\n",
    "            max_length=max_len,\n",
    "            batch_size=batch,\n",
    "            epochs=epochs,\n",
    "            learning_rate=lr\n",
    "        )\n",
    "\n",
    "    # Configuración por defecto\n",
    "    return funciones.TrainingConfig(\n",
    "        **common_config,\n",
    "        max_length=60,\n",
    "        batch_size=32,\n",
    "        epochs=4,\n",
    "        learning_rate=3e-5\n",
    "    )\n",
    "\n",
    "models = [\n",
    "        {'model':'DeepPavlov/rubert-base-cased',\n",
    "         'name':'DeepPavlov-rubert-base',\n",
    "         'type': 'bert'},# Modelo original\n",
    "        {'model':'bert-base-multilingual-cased',\n",
    "         'name':'BERT multilingual',\n",
    "         'type': 'bert'},# BERT multilingüe\n",
    "        {'model':'distilbert-base-multilingual-cased',\n",
    "         'name':'distilbert-base-multilingual',\n",
    "         'type': 'bert'},# Versión ligera de BERT\n",
    "        {'model':'roberta-base',\n",
    "         'name':'roberta-base', \n",
    "        'type': 'bert'}, # RoBERTa \n",
    "        {'model': 'gpt2',\n",
    "         'name': 'gpt2',\n",
    "         'type': 'gpt'},  #  GPT model\n",
    "        {'model': 'facebook/opt-125m',\n",
    "         'name': 'facebook',\n",
    "         'type': 'gpt'},  #  GPT model\n",
    "        {'model': 'sberbank-ai/rugpt3small_based_on_gpt2',\n",
    "         'name': 'rugpt3',\n",
    "         'type': 'gpt'}\n",
    "    ]\n",
    "\n",
    "datasets = [\n",
    "        {\n",
    "            'path1': '../dataset/сокращение по частотности/1б_Изъяты лексемы с частотой выше 100.txt',\n",
    "            'path2': '../dataset/сокращение по частотности/2б_Изъяты лексемы с частотой выше 100.txt',\n",
    "            'name': 'Изъяты лексемы с частотой выше 100',\n",
    "            'type': 'freq',\n",
    "            'freq': 100\n",
    "        },\n",
    "        {\n",
    "            'path1': '../dataset/сокращение по частотности/1в_Изъяты лексемы с частотой выше 49.txt',\n",
    "            'path2': '../dataset/сокращение по частотности/2в_Изъяты лексемы с частотой выше 49.txt',\n",
    "            'name': 'Изъяты лексемы с частотой выше 49',\n",
    "            'type': 'freq',\n",
    "            'freq': 49\n",
    "        },\n",
    "        {\n",
    "            'path1': '../dataset/сокращение по частотности/1г_Изъяты лексемы с частотой выше 29.txt',\n",
    "            'path2': '../dataset/сокращение по частотности/2г_Изъяты лексемы с частотой выше 29.txt',\n",
    "            'name': 'Изъяты лексемы с частотой выше 29',\n",
    "            'type': 'freq',\n",
    "            'freq': 29\n",
    "        },\n",
    "        {\n",
    "            'path1': '../dataset/сокращение по частотности/1д_Изъяты лексемы с частотой выше 9.txt',\n",
    "            'path2': '../dataset/сокращение по частотности/2д_Изъяты лексемы с частотой выше 9.txt',\n",
    "            'name': 'Изъяты лексемы с частотой выше 9',\n",
    "            'type': 'freq',\n",
    "            'freq': 9\n",
    "        },\n",
    "        {\n",
    "            'path1': '../dataset/сокращение по частотности/1е_Изъяты лексемы с частотой выше 5.txt',\n",
    "            'path2': '../dataset/сокращение по частотности/2е_Изъяты лексемы с частотой выше 5.txt',\n",
    "            'name': 'Изъяты лексемы с частотой выше 5',\n",
    "            'type': 'freq',\n",
    "            'freq': 5\n",
    "        },\n",
    "        {\n",
    "            'path1': '../dataset/сокращение по частотности/1ё_Изъяты лексемы с частотой выше 3.txt',\n",
    "            'path2': '../dataset/сокращение по частотности/2ё_Изъяты лексемы с частотой выше 3.txt',\n",
    "            'name': 'Изъяты лексемы с частотой выше 3',\n",
    "            'type': 'freq',\n",
    "            'freq': 3\n",
    "        },\n",
    "\n",
    "            {\n",
    "            'path1': '../dataset/Сокращение по частям речи/Без прилагательных первый жанр.txt',\n",
    "            'path2': '../dataset/Сокращение по частям речи/Без прилагательных второй жанр.txt',\n",
    "            'name': 'Без прилагательных первый-второй жанр',#litle correction\n",
    "            'type': 'pos',\n",
    "            'freq': None\n",
    "            },\n",
    "          {\n",
    "            'path1': '../dataset/Сокращение по частям речи/1.Первый жанр исходная выборка.txt',\n",
    "            'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "            'name': '1.Первый жанр исходная выборка',\n",
    "            'type': 'pos',\n",
    "            'freq': None\n",
    "\n",
    "        },\n",
    "        {\n",
    "            'path1': '../dataset/Сокращение по частям речи/2.Первый жанр без клауз, включающих наречия.txt',\n",
    "            'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "            'name': '2.Первый жанр без клауз, включающих наречия',\n",
    "            'type': 'pos',\n",
    "            'freq': None\n",
    "        },\n",
    "        {\n",
    "            'path1': '../dataset/Сокращение по частям речи/3.Первый жанр без клауз, включающих глаголы.txt',\n",
    "            'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "            'name': '3.Первый жанр без клауз, включающих глаголы',\n",
    "            'type': 'pos',\n",
    "            'freq': None\n",
    "        },\n",
    "        {\n",
    "            'path1': '../dataset/Сокращение по частям речи/4.Первый жанр без клауз, включающих глаголы и наречия.txt',\n",
    "            'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "            'name': '4.Первый жанр без клауз, включающих глаголы и наречия',\n",
    "            'type': 'pos',\n",
    "            'freq': None\n",
    "        },\n",
    "        {\n",
    "            'path1': '../dataset/Сокращение по частям речи/5.без клауз, включающих местоимения.txt',\n",
    "            'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "            'name': '5.без клауз, включающих местоимения.txt',\n",
    "            'type': 'pos',\n",
    "            'freq': None\n",
    "        },\n",
    "        {\n",
    "            'path1': '../dataset/Сокращение по частям речи/6.без слов функциональных.txt',\n",
    "            'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "            'name': '6.без слов функциональных.txt',\n",
    "            'type': 'pos',\n",
    "            'freq': None\n",
    "        },\n",
    "    \n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "russian_templates = [\n",
    "    \"Этот литературный фрагмент принадлежит к жанру {}.\",\n",
    "    \"Данный текст является примером жанра {}.\",\n",
    "    \"Стилистические особенности указывают на жанр {}.\",\n",
    "    \"Жанровая принадлежность этого текста - {}.\",\n",
    "    \"Эксперты классифицируют этот текст как жанр {}.\",\n",
    "    \"Это типичный пример жанра {} в русской литературе.\",\n",
    "    \"По ключевым характеристикам это текст жанра {}.\",\n",
    "    \"Данное произведение относится к направлению {}.\",\n",
    "    \"Анализ содержания позволяет отнести текст к жанру {}.\",\n",
    "    \"Этот отрывок характерен для жанра {}.\"\n",
    "]\n",
    "\n",
    "def main():\n",
    "\n",
    "    results = []\n",
    "    for model in models:\n",
    "        for dataset in datasets:\n",
    "            config = create_custom_config(model['model'], model['type'], dataset)\n",
    "            \n",
    "            \n",
    "            # Entrenar y limpiar memoria\n",
    "            torch.cuda.empty_cache()\n",
    "            result = funciones.train_and_evaluate_dataset(\n",
    "                dataset['path1'],\n",
    "                dataset['path2'],\n",
    "                config,\n",
    "                dataset['name'],\n",
    "                dataset['type']\n",
    "                \n",
    "            )\n",
    "            results.append(result) \n",
    "#             if model['type'] = 'gpt':\n",
    "                \n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    \n",
    "    save_results(results, 'resultados_Step_8.json') \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd31457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dfbb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1ac659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import IPython\n",
    "\n",
    "# Reiniciar el kernel\n",
    "IPython.display.display(IPython.display.Javascript(\"Jupyter.notebook.kernel.restart()\"))\n",
    "\n",
    "# Apagar el kernel después de reiniciar\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8fee8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08043990",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
