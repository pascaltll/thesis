{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e66a70c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m../dataset\u001b[0m\r\n",
      "├── \u001b[01;34men_espanol\u001b[0m\r\n",
      "│   ├── \u001b[00mdocx2txt.py\u001b[0m\r\n",
      "│   ├── \u001b[00mВторой_жанр_исходная.txt\u001b[0m\r\n",
      "│   └── \u001b[00mПервый_жанр_исходная.txt\u001b[0m\r\n",
      "├── \u001b[00mВторой_жанр_исходная.txt\u001b[0m\r\n",
      "├── \u001b[00mПервый_жанр_исходная.txt\u001b[0m\r\n",
      "├── \u001b[01;34mСокращение по частям речи\u001b[0m\r\n",
      "│   ├── \u001b[00m1.Первый жанр исходная выборка.txt\u001b[0m\r\n",
      "│   ├── \u001b[00m2.Первый жанр без клауз, включающих наречия.txt\u001b[0m\r\n",
      "│   ├── \u001b[00m3.Первый жанр без клауз, включающих глаголы.txt\u001b[0m\r\n",
      "│   ├── \u001b[00m4.Первый жанр без клауз, включающих глаголы и наречия.txt\u001b[0m\r\n",
      "│   ├── \u001b[00mБез прилагательных второй жанр.txt\u001b[0m\r\n",
      "│   ├── \u001b[00mБез прилагательных первый жанр.txt\u001b[0m\r\n",
      "│   └── \u001b[00mСлучайные выборки.txt\u001b[0m\r\n",
      "└── \u001b[01;34mсокращение по частотности\u001b[0m\r\n",
      "    ├── \u001b[00m1а_ без сокращений.txt\u001b[0m\r\n",
      "    ├── \u001b[00m1б_Изъяты лексемы с частотой выше 100.txt\u001b[0m\r\n",
      "    ├── \u001b[00m1в_Изъяты лексемы с частотой выше 49.txt\u001b[0m\r\n",
      "    ├── \u001b[00m1г_Изъяты лексемы с частотой выше 29.txt\u001b[0m\r\n",
      "    ├── \u001b[00m1д_Изъяты лексемы с частотой выше 9.txt\u001b[0m\r\n",
      "    ├── \u001b[00m1е_Изъяты лексемы с частотой выше 5.txt\u001b[0m\r\n",
      "    ├── \u001b[00m1ё_Изъяты лексемы с частотой выше 3.txt\u001b[0m\r\n",
      "    ├── \u001b[00m2а_ без сокращений.txt\u001b[0m\r\n",
      "    ├── \u001b[00m2б_Изъяты лексемы с частотой выше 100.txt\u001b[0m\r\n",
      "    ├── \u001b[00m2в_Изъяты лексемы с частотой выше 49.txt\u001b[0m\r\n",
      "    ├── \u001b[00m2г_Изъяты лексемы с частотой выше 29.txt\u001b[0m\r\n",
      "    ├── \u001b[00m2д_Изъяты лексемы с частотой выше 9.txt\u001b[0m\r\n",
      "    ├── \u001b[00m2е_Изъяты лексемы с частотой выше 5.txt\u001b[0m\r\n",
      "    └── \u001b[00m2ё_Изъяты лексемы с частотой выше 3.txt\u001b[0m\r\n",
      "\r\n",
      "3 directories, 26 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree ../dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accb463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \u001b[0m\u001b[01;34m__pycache__\u001b[0m/                          loss_comparison.png\r\n",
      " confronto_prestazioni.png             model_results.json\r\n",
      " frecuencia.png                        model_results_GPT.json\r\n",
      " frecuencia_lineas.png                 parte_2.png\r\n",
      " frecuencia_profesional.png            parte_2_Step1.png\r\n",
      " frecuencia_profesional_ajustada.png   partes_discurso.png\r\n",
      " frecuencia_step_1.png                 performance_comparison_other.png\r\n",
      " frecuencia_test_1.png                 pos_analysis.png\r\n",
      " freq_analysis.png                     utils.py\r\n",
      " freq_comparison.png                   \u001b[01;34mwandb\u001b[0m/\r\n",
      " funciones.py                         \u001b[01;34m'сокращение по частотности'\u001b[0m\u001b[K/\r\n",
      " generador.ipynb                      'сокращение по частотности.ipynb'\r\n",
      " herramientas.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b299fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_results(results, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11038bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Изъяты лексемы с частотой выше 100.csv\n",
      "test_Изъяты лексемы с частотой выше 100.csv\n",
      "Train labels sample: tensor([1, 1, 1, 0, 0]), Shape: torch.Size([871])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([190])\n",
      "Epoch 1, Loss: 0.8994767325265067\n",
      "Epoch 2, Loss: 0.6768577354294913\n",
      "Epoch 3, Loss: 0.669078162738255\n",
      "Epoch 4, Loss: 0.663601951939719\n",
      "Epoch 5, Loss: 0.6583544782229832\n",
      "Epoch 6, Loss: 0.6454630408968244\n",
      "all_preds shape: (190,)\n",
      "all_labels shape: (190,)\n",
      "all_probs shape: (190, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([871])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([192])\n",
      "Epoch 1, Loss: 1.0376296298844474\n",
      "Epoch 2, Loss: 0.6917891076632908\n",
      "Epoch 3, Loss: 0.6706198028155735\n",
      "Epoch 4, Loss: 0.6723543235233852\n",
      "Epoch 5, Loss: 0.6649869169507708\n",
      "Epoch 6, Loss: 0.6722741808210101\n",
      "all_preds shape: (192,)\n",
      "all_labels shape: (192,)\n",
      "all_probs shape: (192, 2)\n",
      "Train labels sample: tensor([0, 0, 1, 1, 1]), Shape: torch.Size([871])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([190])\n",
      "Epoch 1, Loss: 0.8988303116389683\n",
      "Epoch 2, Loss: 0.6658287048339844\n",
      "Epoch 3, Loss: 0.6600809948784965\n",
      "Epoch 4, Loss: 0.662475952080318\n",
      "Epoch 5, Loss: 0.6607816900525775\n",
      "Epoch 6, Loss: 0.6531431078910828\n",
      "all_preds shape: (190,)\n",
      "all_labels shape: (190,)\n",
      "all_probs shape: (190, 2)\n",
      "Train labels sample: tensor([1, 1, 1, 0, 1]), Shape: torch.Size([871])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([200])\n",
      "Epoch 1, Loss: 0.90070093529565\n",
      "Epoch 2, Loss: 0.6930326904569354\n",
      "Epoch 3, Loss: 0.6824758393423898\n",
      "Epoch 4, Loss: 0.6788151434489659\n",
      "Epoch 5, Loss: 0.6695941856929234\n",
      "Epoch 6, Loss: 0.6639471820422581\n",
      "all_preds shape: (200,)\n",
      "all_labels shape: (200,)\n",
      "all_probs shape: (200, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([871])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([189])\n",
      "Epoch 1, Loss: 0.8846057994025094\n",
      "Epoch 2, Loss: 0.6796760729381016\n",
      "Epoch 3, Loss: 0.6699240377971104\n",
      "Epoch 4, Loss: 0.6650093027523586\n",
      "Epoch 5, Loss: 0.665345983845847\n",
      "Epoch 6, Loss: 0.6589238473347255\n",
      "all_preds shape: (189,)\n",
      "all_labels shape: (189,)\n",
      "all_probs shape: (189, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([871])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([191])\n",
      "Epoch 1, Loss: 0.8621262397084918\n",
      "Epoch 2, Loss: 0.7083470736231122\n",
      "Epoch 3, Loss: 0.6835184863635472\n",
      "Epoch 4, Loss: 0.6828646659851074\n",
      "Epoch 5, Loss: 0.6783349003110614\n",
      "Epoch 6, Loss: 0.6784114837646484\n",
      "all_preds shape: (191,)\n",
      "all_labels shape: (191,)\n",
      "all_probs shape: (191, 2)\n",
      "train_Изъяты лексемы с частотой выше 100.csv\n",
      "test_Изъяты лексемы с частотой выше 100.csv\n",
      "Train labels sample: tensor([0, 0, 1, 0, 0]), Shape: torch.Size([876])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([205])\n",
      "Epoch 1, Loss: 0.5738989835664562\n",
      "Epoch 2, Loss: 0.5193933028064363\n",
      "all_preds shape: (205,)\n",
      "all_labels shape: (205,)\n",
      "all_probs shape: (205, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 0, 1]), Shape: torch.Size([876])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([210])\n",
      "Epoch 1, Loss: 0.6046457816612775\n",
      "Epoch 2, Loss: 0.5508470206636272\n",
      "all_preds shape: (210,)\n",
      "all_labels shape: (210,)\n",
      "all_probs shape: (210, 2)\n",
      "Train labels sample: tensor([0, 0, 0, 1, 0]), Shape: torch.Size([876])\n",
      "Test labels sample: tensor([0, 0, 0, 0, 0]), Shape: torch.Size([190])\n",
      "Epoch 1, Loss: 0.6511212731094937\n"
     ]
    }
   ],
   "source": [
    "import funciones\n",
    "from utils import train_wrapper\n",
    "import warnings\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "from contextlib import redirect_stderr\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "#import wandb\n",
    "import nbformat\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import gc\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from razdel import sentenize\n",
    "import numpy as np\n",
    "\n",
    "# Suprimir warnings específicos\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "def save_results(results, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "# import funciones\n",
    "# from utils import train_wrapper\n",
    "# import warnings\n",
    "# import os\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Suprimir warnings específicos\n",
    "# warnings.filterwarnings('ignore', category=UserWarning)  # Para sklearn y otros\n",
    "# warnings.filterwarnings('ignore', category=FutureWarning)  # Para huggingface y transformers\n",
    "# from concurrent.futures import ProcessPoolExecutor\n",
    "# from tqdm import tqdm\n",
    "# from contextlib import redirect_stderr\n",
    "# from concurrent.futures import ProcessPoolExecutor\n",
    "# import multiprocessing as mp\n",
    "# import numpy as np\n",
    "# mp.set_start_method('spawn', force=True)\n",
    "# #os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "\n",
    "###############################################################################\n",
    "# import wandb\n",
    "# import nbformat\n",
    "# # Configurar la clave de la API como variable de entorno\n",
    "# os.environ[\"WANDB_API_KEY\"] = \"ca85316ed713b2425615fb3a613d7eb414c9f57f\"  # Reemplaza con tu clave\n",
    "# # Iniciar wandb sin especificar entity (se detecta automáticamente)\n",
    "# wandb.init(project=\"model-clasification\")\n",
    "# wandb.init(\n",
    "#     settings=wandb.Settings(\n",
    "#         start_method=\"thread\",\n",
    "#         timeout=30,\n",
    "#         sync_dir=\"/tmp/wandb\",  # Usa un directorio temporal\n",
    "#         disable_code=True       # Mejora la estabilidad\n",
    "#     )\n",
    "# )\n",
    "#os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"сокращение по частотности.ipynb\"\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def create_custom_config(model_name, model_type, dataset):\n",
    "    \"\"\"Crea configuración de entrenamiento adaptativa\"\"\"\n",
    "    common_config = {\n",
    "#         'dataset_name':dataset.get('name', ''),\n",
    "        'model_name': model_name,\n",
    "        'model_type': model_type,\n",
    "        'num_repeats': 6,\n",
    "        'test_size': 0.2,\n",
    "        'threshold': 0.5\n",
    "    }\n",
    "\n",
    "    # Configuración específica para GPT\n",
    "    if model_type == 'gpt':\n",
    "        return funciones.TrainingConfig(\n",
    "            **common_config,\n",
    "            max_length=512,\n",
    "            batch_size=4,\n",
    "            epochs=2,\n",
    "            learning_rate=1e-5\n",
    "        )\n",
    "\n",
    "    # Configuraciones basadas en frecuencia\n",
    "    freq_threshold = dataset.get('freq_threshold')\n",
    "    if freq_threshold in [100, 49, 29, 9, 5, 3]:\n",
    "        configs = {\n",
    "            100: (52, 128, 3, 2e-5),\n",
    "            49: (60, 128, 4, 3e-5),\n",
    "            29: (51, 128, 4, 3e-5),\n",
    "            9: (45, 128, 5, 4e-5),\n",
    "            5: (150, 32, 6, 5e-5),\n",
    "            3: (150, 32, 6, 5e-5)\n",
    "        }\n",
    "        max_len, batch, epochs, lr = configs[freq_threshold]\n",
    "        return funciones.TrainingConfig(\n",
    "            **common_config,\n",
    "            max_length=max_len,\n",
    "            batch_size=batch,\n",
    "            epochs=epochs,\n",
    "            learning_rate=lr\n",
    "        )\n",
    "\n",
    "    # Configuración por nombre de dataset\n",
    "    dataset_name = dataset.get('name', '')\n",
    "    if any(x in dataset_name for x in ['1', '2', '3', '4']):\n",
    "        return funciones.TrainingConfig(\n",
    "            **common_config,\n",
    "            max_length=60,\n",
    "            batch_size=128,\n",
    "            epochs=6,\n",
    "            learning_rate=5e-5\n",
    "        )\n",
    "\n",
    "    # Configuración por defecto\n",
    "    return funciones.TrainingConfig(\n",
    "        **common_config,\n",
    "        max_length=60,\n",
    "        batch_size=128,\n",
    "        epochs=4,\n",
    "        learning_rate=3e-5\n",
    "    )\n",
    "\n",
    "models = [\n",
    "#         {'model':'DeepPavlov/rubert-base-cased',\n",
    "#          'name':'DeepPavlov-rubert-base',\n",
    "#          'type': 'bert'},# Modelo original\n",
    "#         {'model':'bert-base-multilingual-cased',\n",
    "#          'name':'BERT multilingual',\n",
    "#          'type': 'bert'},# BERT multilingüe\n",
    "        {'model':'distilbert-base-multilingual-cased',\n",
    "         'name':'distilbert-base-multilingual',\n",
    "         'type': 'bert'},# Versión ligera de BERT\n",
    "#         {'model':'roberta-base',\n",
    "#          'name':'roberta-base', \n",
    "#         'type': 'bert'}, # RoBERTa \n",
    "        {'model': 'gpt2',\n",
    "         'name': 'gpt2',\n",
    "         'type': 'gpt'},  #  GPT model\n",
    "#         {'model': 'facebook/opt-125m',\n",
    "#          'name': 'facebook',\n",
    "#          'type': 'gpt'},  #  GPT model\n",
    "#         {'model': 'sberbank-ai/rugpt3small_based_on_gpt2',\n",
    "#          'name': 'rugpt3',\n",
    "#          'type': 'gpt'}\n",
    "    ]\n",
    "\n",
    "datasets = [\n",
    "        {\n",
    "            'path1': '../dataset/сокращение по частотности/1б_Изъяты лексемы с частотой выше 100.txt',\n",
    "            'path2': '../dataset/сокращение по частотности/2б_Изъяты лексемы с частотой выше 100.txt',\n",
    "            'name': 'Изъяты лексемы с частотой выше 100',\n",
    "            'type': 'freq',\n",
    "            'freq': 100\n",
    "        },\n",
    "#         {\n",
    "#             'path1': '../dataset/сокращение по частотности/1в_Изъяты лексемы с частотой выше 49.txt',\n",
    "#             'path2': '../dataset/сокращение по частотности/2в_Изъяты лексемы с частотой выше 49.txt',\n",
    "#             'name': 'Изъяты лексемы с частотой выше 49',\n",
    "#             'type': 'freq',\n",
    "#             'freq': 49\n",
    "#         },\n",
    "#         {\n",
    "#             'path1': '../dataset/сокращение по частотности/1г_Изъяты лексемы с частотой выше 29.txt',\n",
    "#             'path2': '../dataset/сокращение по частотности/2г_Изъяты лексемы с частотой выше 29.txt',\n",
    "#             'name': 'Изъяты лексемы с частотой выше 29',\n",
    "#             'type': 'freq',\n",
    "#             'freq': 29\n",
    "#         },\n",
    "#         {\n",
    "#             'path1': '../dataset/сокращение по частотности/1д_Изъяты лексемы с частотой выше 9.txt',\n",
    "#             'path2': '../dataset/сокращение по частотности/2д_Изъяты лексемы с частотой выше 9.txt',\n",
    "#             'name': 'Изъяты лексемы с частотой выше 9',\n",
    "#             'type': 'freq',\n",
    "#             'freq': 9\n",
    "#         },\n",
    "#         {\n",
    "#             'path1': '../dataset/сокращение по частотности/1е_Изъяты лексемы с частотой выше 5.txt',\n",
    "#             'path2': '../dataset/сокращение по частотности/2е_Изъяты лексемы с частотой выше 5.txt',\n",
    "#             'name': 'Изъяты лексемы с частотой выше 5',\n",
    "#             'type': 'freq',\n",
    "#             'freq': 5\n",
    "#         },\n",
    "#         {\n",
    "#             'path1': '../dataset/сокращение по частотности/1ё_Изъяты лексемы с частотой выше 3.txt',\n",
    "#             'path2': '../dataset/сокращение по частотности/2ё_Изъяты лексемы с частотой выше 3.txt',\n",
    "#             'name': 'Изъяты лексемы с частотой выше 3',\n",
    "#             'type': 'freq',\n",
    "#             'freq': 3\n",
    "#         },\n",
    "\n",
    "#             {\n",
    "#             'path1': '../dataset/Сокращение по частям речи/Без прилагательных первый жанр.txt',\n",
    "#             'path2': '../dataset/Сокращение по частям речи/Без прилагательных второй жанр.txt',\n",
    "#             'name': 'Без прилагательных второй жанр',\n",
    "#             'type': 'pos',\n",
    "#             'freq': None\n",
    "#             },\n",
    "#           {\n",
    "#             'path1': '../dataset/Сокращение по частям речи/1.Первый жанр исходная выборка.txt',\n",
    "#             'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "#             'name': '1.Первый жанр исходная выборка',\n",
    "#             'type': 'pos',\n",
    "#             'freq': None\n",
    "\n",
    "#         },\n",
    "#         {\n",
    "#             'path1': '../dataset/Сокращение по частям речи/2.Первый жанр без клауз, включающих наречия.txt',\n",
    "#             'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "#             'name': '2.Первый жанр без клауз, включающих наречия',\n",
    "#             'type': 'pos',\n",
    "#             'freq': None\n",
    "#         },\n",
    "#         {\n",
    "#             'path1': '../dataset/Сокращение по частям речи/3.Первый жанр без клауз, включающих глаголы.txt',\n",
    "#             'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "#             'name': '3.Первый жанр без клауз, включающих глаголы',\n",
    "#             'type': 'pos',\n",
    "#             'freq': None\n",
    "#         },\n",
    "#         {\n",
    "#             'path1': '../dataset/Сокращение по частям речи/4.Первый жанр без клауз, включающих глаголы и наречия.txt',\n",
    "#             'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "#             'name': '4.Первый жанр без клауз, включающих глаголы и наречия',\n",
    "#             'type': 'pos',\n",
    "#             'freq': None\n",
    "#         },\n",
    "    ]\n",
    "\n",
    "def main():\n",
    "\n",
    "    results = []\n",
    "    for model in models:\n",
    "        for dataset in datasets:\n",
    "            config = create_custom_config(model['model'], model['type'], dataset)\n",
    "            \n",
    "            # Configurar wandb para este experimento\n",
    "#             wandb_run = wandb.init(\n",
    "#                 project=\"model-comparison\",\n",
    "#                 name=f\"{dataset['name']} - {model['name']}\",\n",
    "#                 group=f\"{dataset['type']}_{dataset.get('freq', 'base')}\",\n",
    "#                 config={\n",
    "#                     \"model\": model['model'],\n",
    "#                     \"model_type\": model['type'],\n",
    "#                     \"dataset\": dataset['name'],\n",
    "#                     \"type\": dataset['type'],\n",
    "#                     \"freq\": dataset.get('freq', None),\n",
    "#                     **config.__dict__\n",
    "#                 },\n",
    "#                 reinit=True\n",
    "#             )\n",
    "            \n",
    "            # Entrenar y limpiar memoria\n",
    "            torch.cuda.empty_cache()\n",
    "            result = funciones.train_and_evaluate_dataset(\n",
    "                dataset['path1'],\n",
    "                dataset['path2'],\n",
    "                config,\n",
    "                dataset['name'],\n",
    "                dataset['type']\n",
    "                \n",
    "#                 wandb_run=wandb_run\n",
    "            )\n",
    "            results.append(result) \n",
    "            \n",
    "#             wandb.log({\n",
    "#                 \"final_avg_accuracy\": result['avg_accuracy'],\n",
    "#                 \"final_std_accuracy\": result['std_accuracy'],\n",
    "#                 \"final_avg_loss\": result['avg_loss']\n",
    "#             })\n",
    "            \n",
    "            # Crear tablas de resumen\n",
    "#             summary_table = wandb.Table(columns=[\"Metric\", \"Value\"])\n",
    "#             summary_table.add_data(\"Avg Accuracy\", result['avg_accuracy'])\n",
    "#             summary_table.add_data(\"Std Accuracy\", result['std_accuracy'])\n",
    "#             summary_table.add_data(\"Avg Loss\", result['avg_loss'])\n",
    "            \n",
    "#             wandb.log({\n",
    "#                 f\"summary/{dataset['name']}_{clean_model_name}\": summary_table,\n",
    "#                 \"epoch\": config.epochs  # Para alinear con otras métricas\n",
    "#             })\n",
    "            \n",
    "#             wandb_run.finish()\n",
    "        \n",
    "    \n",
    "    save_results(results, 'model_results_GPT_bert.json')    \n",
    "#     funciones.plot_model_performance(datasets,\n",
    "#                                      models,\n",
    "#                                      results,\n",
    "#                                      save_path='frecuencia_profesional.png'\n",
    "#                                     )\n",
    "\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd31457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dfbb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1ac659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import IPython\n",
    "\n",
    "# Reiniciar el kernel\n",
    "IPython.display.display(IPython.display.Javascript(\"Jupyter.notebook.kernel.restart()\"))\n",
    "\n",
    "# Apagar el kernel después de reiniciar\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f207b0e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5ccc6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0fdd27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
