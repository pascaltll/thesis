{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e66a70c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m../dataset\u001b[00m\r\n",
      "├── \u001b[01;34men_espanol\u001b[00m\r\n",
      "│   ├── docx2txt.py\r\n",
      "│   ├── Второй_жанр_исходная.txt\r\n",
      "│   └── Первый_жанр_исходная.txt\r\n",
      "├── Второй_жанр_исходная.txt\r\n",
      "├── Первый_жанр_исходная.txt\r\n",
      "├── \u001b[01;34mСокращение по частям речи\u001b[00m\r\n",
      "│   ├── 1.Первый жанр исходная выборка.txt\r\n",
      "│   ├── 2.Первый жанр без клауз, включающих наречия.txt\r\n",
      "│   ├── 3.Первый жанр без клауз, включающих глаголы.txt\r\n",
      "│   ├── 4.Первый жанр без клауз, включающих глаголы и наречия.txt\r\n",
      "│   ├── Без прилагательных второй жанр.txt\r\n",
      "│   ├── Без прилагательных первый жанр.txt\r\n",
      "│   └── Случайные выборки.txt\r\n",
      "└── \u001b[01;34mсокращение по частотности\u001b[00m\r\n",
      "    ├── 1а_ без сокращений.txt\r\n",
      "    ├── 1б_Изъяты лексемы с частотой выше 100.txt\r\n",
      "    ├── 1в_Изъяты лексемы с частотой выше 49.txt\r\n",
      "    ├── 1г_Изъяты лексемы с частотой выше 29.txt\r\n",
      "    ├── 1д_Изъяты лексемы с частотой выше 9.txt\r\n",
      "    ├── 1е_Изъяты лексемы с частотой выше 5.txt\r\n",
      "    ├── 1ё_Изъяты лексемы с частотой выше 3.txt\r\n",
      "    ├── 2а_ без сокращений.txt\r\n",
      "    ├── 2б_Изъяты лексемы с частотой выше 100.txt\r\n",
      "    ├── 2в_Изъяты лексемы с частотой выше 49.txt\r\n",
      "    ├── 2г_Изъяты лексемы с частотой выше 29.txt\r\n",
      "    ├── 2д_Изъяты лексемы с частотой выше 9.txt\r\n",
      "    ├── 2е_Изъяты лексемы с частотой выше 5.txt\r\n",
      "    └── 2ё_Изъяты лексемы с частотой выше 3.txt\r\n",
      "\r\n",
      "3 directories, 26 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree ../dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accb463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \u001b[0m\u001b[01;34m__pycache__\u001b[0m/                          model_results.json\r\n",
      " frecuencia.png                        parte_2.png\r\n",
      " frecuencia_lineas.png                 parte_2_Step1.png\r\n",
      " frecuencia_profesional.png            partes_discurso.png\r\n",
      " frecuencia_profesional_ajustada.png   utils.py\r\n",
      " frecuencia_step_1.png                 \u001b[01;34mwandb\u001b[0m/\r\n",
      " frecuencia_test_1.png                \u001b[01;34m'Сокращение по частям речи'\u001b[0m\u001b[K/\r\n",
      " funciones.py                         \u001b[01;34m'сокращение по частотности'\u001b[0m\u001b[K/\r\n",
      " generador.ipynb                      'сокращение по частотности.ipynb'\r\n",
      " herramientas.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b299fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_results(results, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11038bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 06:05:59.085692: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Изъяты лексемы с частотой выше 100.csv\n",
      "test_Изъяты лексемы с частотой выше 100.csv\n",
      "Epoch 1, Loss: 0.9472208023071289\n",
      "Epoch 2, Loss: 0.6758400542395455\n",
      "Epoch 3, Loss: 0.6637617009026664\n"
     ]
    }
   ],
   "source": [
    "import funciones\n",
    "from utils import train_wrapper\n",
    "import warnings\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "from contextlib import redirect_stderr\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import wandb\n",
    "import nbformat\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import gc\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from razdel import sentenize\n",
    "import numpy as np\n",
    "\n",
    "# Suprimir warnings específicos\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "def save_results(results, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "# import funciones\n",
    "# from utils import train_wrapper\n",
    "# import warnings\n",
    "# import os\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Suprimir warnings específicos\n",
    "# warnings.filterwarnings('ignore', category=UserWarning)  # Para sklearn y otros\n",
    "# warnings.filterwarnings('ignore', category=FutureWarning)  # Para huggingface y transformers\n",
    "# from concurrent.futures import ProcessPoolExecutor\n",
    "# from tqdm import tqdm\n",
    "# from contextlib import redirect_stderr\n",
    "# from concurrent.futures import ProcessPoolExecutor\n",
    "# import multiprocessing as mp\n",
    "# import numpy as np\n",
    "# mp.set_start_method('spawn', force=True)\n",
    "# #os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "\n",
    "###############################################################################\n",
    "# import wandb\n",
    "# import nbformat\n",
    "# # Configurar la clave de la API como variable de entorno\n",
    "# os.environ[\"WANDB_API_KEY\"] = \"ca85316ed713b2425615fb3a613d7eb414c9f57f\"  # Reemplaza con tu clave\n",
    "# # Iniciar wandb sin especificar entity (se detecta automáticamente)\n",
    "# wandb.init(project=\"model-clasification\")\n",
    "# wandb.init(\n",
    "#     settings=wandb.Settings(\n",
    "#         start_method=\"thread\",\n",
    "#         timeout=30,\n",
    "#         sync_dir=\"/tmp/wandb\",  # Usa un directorio temporal\n",
    "#         disable_code=True       # Mejora la estabilidad\n",
    "#     )\n",
    "# )\n",
    "#os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"сокращение по частотности.ipynb\"\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "# def train_wrapper(args):\n",
    "#     dataset, config = args  # Desempaquetar los argumentos\n",
    "#     result = funciones.train_and_evaluate_dataset(\n",
    "#         dataset['path1'],\n",
    "#         dataset['path2'],\n",
    "#         config,\n",
    "#         dataset['name'],\n",
    "#     )\n",
    "#     result['type'] = dataset['type']\n",
    "#     return result\n",
    "#     \"\"\"\n",
    "#       return {\n",
    "#         'dataset_name': dataset_name,\n",
    "#         'avg_accuracy': avg_accuracy,\n",
    "#         'std_accuracy': std_accuracy,\n",
    "#         'accuracies': accuracies,\n",
    "#         'type': type\n",
    "#         }\n",
    "#     \"\"\"\n",
    "\n",
    "###########################################################################\n",
    "def create_custom_config(dataset, model_name, model_type):\n",
    "    base_config = {\n",
    "        'model_name': model_name,\n",
    "        'num_repeats': 6,\n",
    "        'test_size': 0.2,\n",
    "        'threshold': 0.5,\n",
    "        'model_type': model_type\n",
    "    }\n",
    "    if model_type == 'gpt':\n",
    "        return funciones.TrainingConfig(\n",
    "            **base_config,\n",
    "            max_length=512,  # GPT models typically handle longer sequences\n",
    "            batch_size=4,    # Smaller batch size for GPT due to memory constraints\n",
    "            epochs=2,        # Fewer epochs for few-shot learning\n",
    "            learning_rate=1e-5\n",
    "        )\n",
    "    \n",
    "    if dataset.get('freq_threshold') == 100:\n",
    "        return funciones.TrainingConfig(\n",
    "            **base_config,\n",
    "            max_length=52,\n",
    "            batch_size=128,\n",
    "            epochs=3,\n",
    "            learning_rate=2e-5\n",
    "        )\n",
    "    elif dataset.get('freq_threshold') == 49:\n",
    "        return funciones.TrainingConfig(\n",
    "            **base_config,\n",
    "            max_length=60,\n",
    "            batch_size=128,\n",
    "            epochs=4,\n",
    "            learning_rate=3e-5\n",
    "        )\n",
    "    elif dataset.get('freq_threshold') == 29:\n",
    "        return funciones.TrainingConfig(\n",
    "            **base_config,\n",
    "            max_length=51,\n",
    "            batch_size=128,\n",
    "            epochs=4,\n",
    "            learning_rate=3e-5\n",
    "        )\n",
    "    elif dataset.get('freq_threshold') == 9:\n",
    "        return funciones.TrainingConfig(\n",
    "            **base_config,\n",
    "            max_length=45,\n",
    "            batch_size=128,\n",
    "            epochs=5,\n",
    "            learning_rate=4e-5\n",
    "        )\n",
    "    elif dataset.get('freq_threshold') == 5:\n",
    "        return funciones.TrainingConfig(\n",
    "            **base_config,\n",
    "            max_length=150,\n",
    "            batch_size=32,\n",
    "            epochs=6,\n",
    "            learning_rate=5e-5\n",
    "        )\n",
    "    elif dataset.get('freq_threshold') == 3:\n",
    "        return funciones.TrainingConfig(\n",
    "            **base_config,\n",
    "            max_length=150,\n",
    "            batch_size=32,\n",
    "            epochs=6,\n",
    "            learning_rate=5e-5\n",
    "        )\n",
    "    # Second dataset naming pattern\n",
    "    elif \"1\" in dataset.get('name', ''):\n",
    "        return funciones.TrainingConfig(\n",
    "            **base_config,\n",
    "            max_length=60,\n",
    "            batch_size=128,\n",
    "            epochs=6,\n",
    "            learning_rate=5e-5\n",
    "        )\n",
    "    elif \"2\" in dataset.get('name', ''):\n",
    "        return funciones.TrainingConfig(\n",
    "            **base_config,\n",
    "            max_length=60,\n",
    "            batch_size=128,\n",
    "            epochs=6,\n",
    "            learning_rate=5e-5\n",
    "        )\n",
    "    elif \"3\" in dataset.get('name', ''):\n",
    "        return funciones.TrainingConfig(\n",
    "            **base_config,\n",
    "            max_length=60,\n",
    "            batch_size=128,\n",
    "            epochs=6,\n",
    "            learning_rate=5e-5\n",
    "        )\n",
    "    elif \"4\" in dataset.get('name', ''):\n",
    "        return funciones.TrainingConfig(\n",
    "            **base_config,\n",
    "            max_length=60,\n",
    "            batch_size=128,\n",
    "            epochs=6,\n",
    "            learning_rate=5e-5\n",
    "        )\n",
    "\n",
    "    # Default configuration if no conditions match\n",
    "    return funciones.TrainingConfig(\n",
    "        **base_config,\n",
    "        max_length=60,\n",
    "        batch_size=128,\n",
    "        epochs=4,\n",
    "        learning_rate=3e-5\n",
    "    )\n",
    "        \n",
    "\n",
    "models = [\n",
    "#         {'model':'DeepPavlov/rubert-base-cased',\n",
    "#          'name':'rubert-base',\n",
    "#          'type': 'bert'},# Modelo original\n",
    "#         {'model':'bert-base-multilingual-cased',\n",
    "#          'name':'BERT multilingual',\n",
    "#          'type': 'bert'},# BERT multilingüe\n",
    "#         {'model':'distilbert-base-multilingual-cased',\n",
    "#          'name':'distilbert-base-multilingual',\n",
    "#          'type': 'bert'},# Versión ligera de BERT\n",
    "        {'model':'roberta-base',\n",
    "         'name':'roberta-base', \n",
    "        'type': 'bert'}, # RoBERTa \n",
    "#         {'model': 'gpt2',\n",
    "#          'name': 'gpt2',\n",
    "#          'type': 'gpt'},  #  GPT model\n",
    "#         {'model': 'facebook/opt-125m',\n",
    "#          'name': 'opt-125m',\n",
    "#          'type': 'gpt'},  #  GPT model\n",
    "#         {'model': 'sberbank-ai/rugpt3small_based_on_gpt2',\n",
    "#          'name': 'rugpt3',\n",
    "#          'type': 'gpt'}\n",
    "    ]\n",
    "\n",
    "datasets = [\n",
    "        {\n",
    "            'path1': '../dataset/сокращение по частотности/1б_Изъяты лексемы с частотой выше 100.txt',\n",
    "            'path2': '../dataset/сокращение по частотности/2б_Изъяты лексемы с частотой выше 100.txt',\n",
    "            'name': 'Изъяты лексемы с частотой выше 100',\n",
    "            'type': 'freq',\n",
    "            'freq': 100\n",
    "        },\n",
    "        {\n",
    "            'path1': '../dataset/сокращение по частотности/1в_Изъяты лексемы с частотой выше 49.txt',\n",
    "            'path2': '../dataset/сокращение по частотности/2в_Изъяты лексемы с частотой выше 49.txt',\n",
    "            'name': 'Изъяты лексемы с частотой выше 49',\n",
    "            'type': 'freq',\n",
    "            'freq': 49\n",
    "        },\n",
    "#         {\n",
    "#             'path1': '../dataset/сокращение по частотности/1г_Изъяты лексемы с частотой выше 29.txt',\n",
    "#             'path2': '../dataset/сокращение по частотности/2г_Изъяты лексемы с частотой выше 29.txt',\n",
    "#             'name': 'Изъяты лексемы с частотой выше 29',\n",
    "#             'type': 'freq',\n",
    "#             'freq': 29\n",
    "#         },\n",
    "#         {\n",
    "#             'path1': '../dataset/сокращение по частотности/1д_Изъяты лексемы с частотой выше 9.txt',\n",
    "#             'path2': '../dataset/сокращение по частотности/2д_Изъяты лексемы с частотой выше 9.txt',\n",
    "#             'name': 'Изъяты лексемы с частотой выше 9',\n",
    "#             'type': 'freq',\n",
    "#             'freq': 9\n",
    "#         },\n",
    "#         {\n",
    "#             'path1': '../dataset/сокращение по частотности/1е_Изъяты лексемы с частотой выше 5.txt',\n",
    "#             'path2': '../dataset/сокращение по частотности/2е_Изъяты лексемы с частотой выше 5.txt',\n",
    "#             'name': 'Изъяты лексемы с частотой выше 5',\n",
    "#             'type': 'freq',\n",
    "#             'freq': 5\n",
    "#         },\n",
    "#         {\n",
    "#             'path1': '../dataset/сокращение по частотности/1ё_Изъяты лексемы с частотой выше 3.txt',\n",
    "#             'path2': '../dataset/сокращение по частотности/2ё_Изъяты лексемы с частотой выше 3.txt',\n",
    "#             'name': 'Изъяты лексемы с частотой выше 3',\n",
    "#             'type': 'freq',\n",
    "#             'freq': 3\n",
    "#         },\n",
    "\n",
    "#             {\n",
    "#             'path1': '../dataset/Сокращение по частям речи/Без прилагательных первый жанр.txt',\n",
    "#             'path2': '../dataset/Сокращение по частям речи/Без прилагательных второй жанр.txt',\n",
    "#             'name': 'Без прилагательных второй жанр',\n",
    "#             'type': 'pos',\n",
    "#             'freq': None\n",
    "#             },\n",
    "#           {\n",
    "#             'path1': '../dataset/Сокращение по частям речи/1.Первый жанр исходная выборка.txt',\n",
    "#             'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "#             'name': '1.Первый жанр исходная выборка',\n",
    "#             'type': 'pos',\n",
    "#             'freq': None\n",
    "\n",
    "#         },\n",
    "#         {\n",
    "#             'path1': '../dataset/Сокращение по частям речи/2.Первый жанр без клауз, включающих наречия.txt',\n",
    "#             'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "#             'name': '2.Первый жанр без клауз, включающих наречия',\n",
    "#             'type': 'pos',\n",
    "#             'freq': None\n",
    "#         },\n",
    "        {\n",
    "            'path1': '../dataset/Сокращение по частям речи/3.Первый жанр без клауз, включающих глаголы.txt',\n",
    "            'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "            'name': '3.Первый жанр без клауз, включающих глаголы',\n",
    "            'type': 'pos',\n",
    "            'freq': None\n",
    "        },\n",
    "        {\n",
    "            'path1': '../dataset/Сокращение по частям речи/4.Первый жанр без клауз, включающих глаголы и наречия.txt',\n",
    "            'path2': '../dataset/Второй_жанр_исходная.txt',\n",
    "            'name': '4.Первый жанр без клауз, включающих глаголы и наречия',\n",
    "            'type': 'pos',\n",
    "            'freq': None\n",
    "        },\n",
    "    ]\n",
    "\n",
    "def main():\n",
    "\n",
    "    results = []\n",
    "    for model in models:\n",
    "        for dataset in datasets:\n",
    "            dataset_copy = dataset.copy()\n",
    "            dataset_copy['name'] = f\"{dataset['name']} - {model['name']}\"\n",
    "            config = create_custom_config(dataset, model['model'], model['type'])\n",
    "            args = (dataset_copy, config)\n",
    "            \n",
    "            # Configurar wandb para este experimento\n",
    "          \n",
    "#             wandb_run = wandb.init(\n",
    "#                 project=\"model-comparison\",\n",
    "#                 name=f\"{dataset['name']} - {model['name']}\",\n",
    "#                 group=f\"{dataset['type']}_{dataset.get('freq', 'base')}\",\n",
    "#                 config={\n",
    "#                     \"model\": model['model'],\n",
    "#                     \"model_type\": model['type'],\n",
    "#                     \"dataset\": dataset['name'],\n",
    "#                     \"type\": dataset['type'],\n",
    "#                     \"freq\": dataset.get('freq', None),\n",
    "#                     **config.__dict__\n",
    "#                 },\n",
    "#                 reinit=True\n",
    "#             )\n",
    "            \n",
    "            # Entrenar y limpiar memoria\n",
    "            torch.cuda.empty_cache()\n",
    "            result = funciones.train_and_evaluate_dataset(\n",
    "                dataset['path1'],\n",
    "                dataset['path2'],\n",
    "                config,\n",
    "                dataset['name'],\n",
    "#                 wandb_run=wandb_run\n",
    "            )\n",
    "            results.append(result) \n",
    "            \n",
    "#             wandb.log({\n",
    "#                 \"final_avg_accuracy\": result['avg_accuracy'],\n",
    "#                 \"final_std_accuracy\": result['std_accuracy'],\n",
    "#                 \"final_avg_loss\": result['avg_loss']\n",
    "#             })\n",
    "            \n",
    "            # Crear tablas de resumen\n",
    "            summary_table = wandb.Table(columns=[\"Metric\", \"Value\"])\n",
    "            summary_table.add_data(\"Avg Accuracy\", result['avg_accuracy'])\n",
    "            summary_table.add_data(\"Std Accuracy\", result['std_accuracy'])\n",
    "            summary_table.add_data(\"Avg Loss\", result['avg_loss'])\n",
    "            \n",
    "#             wandb.log({\n",
    "#                 f\"summary/{dataset['name']}_{clean_model_name}\": summary_table,\n",
    "#                 \"epoch\": config.epochs  # Para alinear con otras métricas\n",
    "#             })\n",
    "            \n",
    "#             wandb_run.finish()\n",
    "        \n",
    "    save_results(results, 'model_results.json')    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927a716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd31457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size=128v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dfbb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1ac659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import IPython\n",
    "\n",
    "# Reiniciar el kernel\n",
    "IPython.display.display(IPython.display.Javascript(\"Jupyter.notebook.kernel.restart()\"))\n",
    "\n",
    "# Apagar el kernel después de reiniciar\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0cb97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def plot_model_performance(datasets, models, results, save_path='frecuencia_profesional.png'):\n",
    "    \"\"\"\n",
    "    Genera un gráfico de barras comparando el rendimiento de modelos por dataset.\n",
    "\n",
    "    Args:\n",
    "        datasets (list): Lista de diccionarios con nombres de datasets (cada uno con clave 'name').\n",
    "        models (list): Lista de diccionarios con nombres de modelos (cada uno con clave 'name').\n",
    "        results (list): Lista de resultados con 'dataset_name', 'model_name' y 'avg_accuracy'.\n",
    "        save_path (str): Ruta donde guardar el gráfico (por defecto 'frecuencia_profesional.png').\n",
    "    \"\"\"\n",
    "    # Extraer nombres\n",
    "    dataset_names = [d['name'] for d in datasets]\n",
    "    model_names = [m['name'] for m in models]\n",
    "\n",
    "    # Crear matriz de precisiones\n",
    "    accuracies = np.zeros((len(dataset_names), len(model_names)))\n",
    "    for i, dataset_name in enumerate(dataset_names):\n",
    "        for j, model_name in enumerate(model_names):\n",
    "            for result in results:\n",
    "                if dataset_name in result['dataset_name'] and model_name in result['model_name']:\n",
    "                    accuracies[i, j] = result['avg_accuracy']\n",
    "                    break  # Salir del bucle una vez encontrada la precisión\n",
    "\n",
    "    # Configuración de estilo profesional\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "\n",
    "    # Crear figura con mayor resolución\n",
    "    fig, ax = plt.subplots(figsize=(20, 8), dpi=100)\n",
    "\n",
    "    # Parámetros dinámicos según cantidad de modelos\n",
    "    n_models = len(model_names)\n",
    "    n_datasets = len(dataset_names)\n",
    "\n",
    "    # Ajustar el ancho de las barras en función del número de modelos y datasets\n",
    "    # Reducir el ancho máximo para evitar que los números se vean demasiado juntos\n",
    "    max_bar_width = 0.8 / (n_models * 1.2) if n_models > 0 else 0.15\n",
    "    bar_width = min(max_bar_width, 0.1)\n",
    "    index = np.arange(n_datasets)\n",
    "    colors = cm.Set2(np.linspace(0, 1, n_models))  # Paleta de colores profesional\n",
    "\n",
    "    # Espacio entre grupos de barras (datasets)\n",
    "    group_spacing = 0.5\n",
    "\n",
    "    # Calcular el ancho total ocupado por las barras de un dataset\n",
    "    total_bar_width_per_dataset = n_models * bar_width\n",
    "\n",
    "    # Calcular el desplazamiento para centrar el grupo de barras de cada dataset\n",
    "    group_offset = (total_bar_width_per_dataset + (n_models - 1) * 0.02) / 2 if n_models > 1 else bar_width / 2 # Añadir un pequeño espacio entre barras del mismo dataset\n",
    "\n",
    "    # Crear barras con mejoras visuales\n",
    "    for i, (model_name, color) in enumerate(zip(model_names, colors)):\n",
    "        positions = index + (i - (n_models - 1) / 2) * bar_width\n",
    "        bars = ax.bar(positions,\n",
    "                       accuracies[:, i],\n",
    "                       bar_width,\n",
    "                       label=model_name,\n",
    "                       color=color,\n",
    "                       edgecolor='black',\n",
    "                       alpha=0.8)\n",
    "\n",
    "        # Añadir valores encima de cada barra solo si no hay demasiados modelos\n",
    "        if n_models <= 10:  # Límite para evitar clutter\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                        f'{height:.3f}',\n",
    "                        ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    # Personalización de ejes\n",
    "    plt.xlabel('Dataset', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Precisión Promedio', fontsize=12, fontweight='bold')\n",
    "    plt.title('Rendimiento de Modelos por Dataset', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "    # Ajustar marcas del eje X\n",
    "    plt.xticks(index,\n",
    "               dataset_names,\n",
    "               rotation=45,\n",
    "               ha='right',\n",
    "               fontsize=10)\n",
    "\n",
    "    # Añadir grid\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Mover la leyenda fuera del gráfico\n",
    "    plt.legend(title='Modelos',\n",
    "               title_fontsize=12,\n",
    "               fontsize=10,\n",
    "               loc='center left',\n",
    "               bbox_to_anchor=(1.05, 0.5),\n",
    "               frameon=True,\n",
    "               borderaxespad=0.)\n",
    "\n",
    "    # Ajustar límites del eje Y\n",
    "    plt.ylim(0, max(accuracies.max() * 1.1, 1))\n",
    "\n",
    "    # Ajustar layout para incluir la leyenda externa\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Guardar con alta calidad\n",
    "    plt.savefig(save_path,\n",
    "                dpi=300,\n",
    "                bbox_inches='tight')\n",
    "\n",
    "    # Mostrar gráfico\n",
    "    plt.show()\n",
    "\n",
    "# Ejemplo de uso:\n",
    "datasets = [{'name': 'dataset1'}, {'name': 'dataset2'}, {'name': 'dataset3'}, {'name': 'dataset4'}, {'name': 'dataset5'}]\n",
    "models = [{'name': 'model1'}, {'name': 'model2'}, {'name': 'model3'}, {'name': 'model4'}]\n",
    "results = [\n",
    "    {'dataset_name': 'dataset1', 'model_name': 'model1', 'avg_accuracy': 0.85},\n",
    "    {'dataset_name': 'dataset1', 'model_name': 'model2', 'avg_accuracy': 0.88},\n",
    "    {'dataset_name': 'dataset1', 'model_name': 'model3', 'avg_accuracy': 0.82},\n",
    "    {'dataset_name': 'dataset1', 'model_name': 'model4', 'avg_accuracy': 0.90},\n",
    "    {'dataset_name': 'dataset2', 'model_name': 'model1', 'avg_accuracy': 0.78},\n",
    "    {'dataset_name': 'dataset2', 'model_name': 'model2', 'avg_accuracy': 0.81},\n",
    "    {'dataset_name': 'dataset2', 'model_name': 'model3', 'avg_accuracy': 0.75},\n",
    "    {'dataset_name': 'dataset2', 'model_name': 'model4', 'avg_accuracy': 0.83},\n",
    "    {'dataset_name': 'dataset3', 'model_name': 'model1', 'avg_accuracy': 0.92},\n",
    "    {'dataset_name': 'dataset3', 'model_name': 'model2', 'avg_accuracy': 0.95},\n",
    "    {'dataset_name': 'dataset3', 'model_name': 'model3', 'avg_accuracy': 0.90},\n",
    "    {'dataset_name': 'dataset3', 'model_name': 'model4', 'avg_accuracy': 0.96},\n",
    "    {'dataset_name': 'dataset4', 'model_name': 'model1', 'avg_accuracy': 0.65},\n",
    "    {'dataset_name': 'dataset4', 'model_name': 'model2', 'avg_accuracy': 0.68},\n",
    "    {'dataset_name': 'dataset4', 'model_name': 'model3', 'avg_accuracy': 0.62},\n",
    "    {'dataset_name': 'dataset4', 'model_name': 'model4', 'avg_accuracy': 0.70},\n",
    "    {'dataset_name': 'dataset5', 'model_name': 'model1', 'avg_accuracy': 0.70},\n",
    "    {'dataset_name': 'dataset5', 'model_name': 'model2', 'avg_accuracy': 0.73},\n",
    "    {'dataset_name': 'dataset5', 'model_name': 'model3', 'avg_accuracy': 0.68},\n",
    "    {'dataset_name': 'dataset5', 'model_name': 'model4', 'avg_accuracy': 0.75},\n",
    "]\n",
    "plot_model_performance(datasets, models, results, save_path='frecuencia_profesional_ajustada.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe52c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mi Entorno (Python 3.9)",
   "language": "python",
   "name": "mi_entorno"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
