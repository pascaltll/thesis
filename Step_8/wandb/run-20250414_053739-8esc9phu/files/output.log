train_Изъяты лексемы с частотой выше 29 - roberta-base.csv
test_Изъяты лексемы с частотой выше 29 - roberta-base.csv
Epoch 1, Loss: 0.7684487700462341
Epoch 2, Loss: 0.709457516670227
Epoch 3, Loss: 0.7003021463751793
Epoch 4, Loss: 0.6412631198763847
Epoch 5, Loss: 0.6564519926905632
Epoch 6, Loss: 0.6600999012589455
Epoch 1, Loss: 0.8368428498506546
Epoch 2, Loss: 0.6619893312454224
Epoch 3, Loss: 0.6563945561647415
Epoch 4, Loss: 0.647433765232563
Epoch 5, Loss: 0.6331170424818993
Epoch 6, Loss: 0.6359624341130257
Epoch 1, Loss: 0.8237422928214073
Epoch 2, Loss: 0.6518904268741608
Epoch 3, Loss: 0.6224824674427509
Epoch 4, Loss: 0.6519310623407364
Epoch 5, Loss: 0.6243006065487862
Epoch 6, Loss: 0.6752382665872574
Epoch 1, Loss: 0.8520992621779442
Epoch 2, Loss: 0.7028329521417618
Epoch 3, Loss: 0.6539945006370544
Epoch 4, Loss: 0.6749939024448395
Epoch 5, Loss: 0.6544471606612206
Epoch 6, Loss: 0.630101777613163
Epoch 1, Loss: 0.8734422251582146
Epoch 2, Loss: 0.6886186376214027
Epoch 3, Loss: 0.6645302809774876
Epoch 4, Loss: 0.664147924631834
Epoch 5, Loss: 0.6485554650425911
Epoch 6, Loss: 0.62361766025424
Epoch 1, Loss: 0.747392076998949
Epoch 2, Loss: 0.7259609773755074
Epoch 3, Loss: 0.7033558562397957
Epoch 4, Loss: 0.6573704853653908
Epoch 5, Loss: 0.6432277709245682
Epoch 6, Loss: 0.6476660221815109
