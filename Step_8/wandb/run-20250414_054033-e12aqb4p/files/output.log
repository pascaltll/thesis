train_Изъяты лексемы с частотой выше 3 - roberta-base.csv
test_Изъяты лексемы с частотой выше 3 - roberta-base.csv
Epoch 1, Loss: 0.5849227905273438
Epoch 2, Loss: 1.2024846076965332
Epoch 3, Loss: 0.7778657674789429
Epoch 4, Loss: 0.5593113899230957
Epoch 5, Loss: 0.6743637919425964
Epoch 6, Loss: 0.5973158478736877
Epoch 1, Loss: 0.7891380190849304
Epoch 2, Loss: 0.8451620936393738
Epoch 3, Loss: 0.6899152994155884
Epoch 4, Loss: 0.5132622718811035
Epoch 5, Loss: 0.5166494250297546
Epoch 6, Loss: 0.5636377334594727
Epoch 1, Loss: 0.6176642179489136
Epoch 2, Loss: 1.3022968769073486
Epoch 3, Loss: 0.8790634274482727
Epoch 4, Loss: 0.5479739904403687
Epoch 5, Loss: 0.7389026284217834
Epoch 6, Loss: 0.6337132453918457
Epoch 1, Loss: 0.6260741949081421
Epoch 2, Loss: 1.0162773132324219
Epoch 3, Loss: 0.8308279514312744
Epoch 4, Loss: 0.5560603737831116
Epoch 5, Loss: 0.5100993514060974
Epoch 6, Loss: 0.587424099445343
Epoch 1, Loss: 0.6558327674865723
Epoch 2, Loss: 1.1169594526290894
Epoch 3, Loss: 0.8253811001777649
Epoch 4, Loss: 0.5501148104667664
Epoch 5, Loss: 0.7090151309967041
Epoch 6, Loss: 0.6224015951156616
Epoch 1, Loss: 0.8128132224082947
Epoch 2, Loss: 1.1657546758651733
Epoch 3, Loss: 0.9198805093765259
Epoch 4, Loss: 0.6098858118057251
Epoch 5, Loss: 0.637432336807251
Epoch 6, Loss: 0.6809271574020386
