train_Изъяты лексемы с частотой выше 100 - roberta-base.csv
test_Изъяты лексемы с частотой выше 100 - roberta-base.csv
Epoch 1, Loss: 0.8827295984540667
Epoch 2, Loss: 0.6875455209187099
Epoch 3, Loss: 0.6649338943617684
Epoch 4, Loss: 0.662821216242654
Epoch 5, Loss: 0.64964588199343
Epoch 6, Loss: 0.6554411734853473
Epoch 1, Loss: 0.9202324662889753
Epoch 2, Loss: 0.7251595514161246
Epoch 3, Loss: 0.6909465789794922
Epoch 4, Loss: 0.6751772505896432
Epoch 5, Loss: 0.6658353124346051
Epoch 6, Loss: 0.6606651544570923
Epoch 1, Loss: 0.8706994141851153
Epoch 2, Loss: 0.6850394606590271
Epoch 3, Loss: 0.6744277647563389
Epoch 4, Loss: 0.6681598254612514
Epoch 5, Loss: 0.6612927998815264
Epoch 6, Loss: 0.6630338685853141
Epoch 1, Loss: 0.9228829060282026
Epoch 2, Loss: 0.6875826290675572
Epoch 3, Loss: 0.6803466251918248
Epoch 4, Loss: 0.6677534920828683
Epoch 5, Loss: 0.6595164452280317
Epoch 6, Loss: 0.6623275194849286
Epoch 1, Loss: 0.871202392237527
Epoch 2, Loss: 0.6753783992358616
Epoch 3, Loss: 0.6759277922766549
Epoch 4, Loss: 0.6645287105015346
Epoch 5, Loss: 0.6600608655384609
Epoch 6, Loss: 0.6642300486564636
Epoch 1, Loss: 0.9786990880966187
Epoch 2, Loss: 0.6899299025535583
Epoch 3, Loss: 0.6793850234576634
Epoch 4, Loss: 0.6696644340242658
Epoch 5, Loss: 0.6672549758638654
Epoch 6, Loss: 0.6700235775538853
