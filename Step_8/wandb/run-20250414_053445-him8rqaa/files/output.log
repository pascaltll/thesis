train_Изъяты лексемы с частотой выше 100 - roberta-base.csv
test_Изъяты лексемы с частотой выше 100 - roberta-base.csv
Epoch 1, Loss: 0.9193515607288906
Epoch 2, Loss: 0.6832482474190849
Epoch 3, Loss: 0.6630917702402387
Epoch 4, Loss: 0.6572997484888349
Epoch 5, Loss: 0.6545722484588623
Epoch 6, Loss: 0.6585832153047834
Epoch 1, Loss: 0.9715333751269749
Epoch 2, Loss: 0.6929280417306083
Epoch 3, Loss: 0.6771782636642456
Epoch 4, Loss: 0.6747039471353803
Epoch 5, Loss: 0.6637855342456273
Epoch 6, Loss: 0.6599262356758118
Epoch 1, Loss: 1.0635254723685128
Epoch 2, Loss: 0.7113901121275765
Epoch 3, Loss: 0.7071063603673663
Epoch 4, Loss: 0.6859892351286752
Epoch 5, Loss: 0.664901852607727
Epoch 6, Loss: 0.6665713276181903
Epoch 1, Loss: 0.8378735269818988
Epoch 2, Loss: 0.6889271225248065
Epoch 3, Loss: 0.6717211859566825
Epoch 4, Loss: 0.6678068552698407
Epoch 5, Loss: 0.6585152915545872
Epoch 6, Loss: 0.6640682390757969
Epoch 1, Loss: 0.75092784847532
Epoch 2, Loss: 0.6594895209584918
Epoch 3, Loss: 0.6618478042738778
Epoch 4, Loss: 0.6645898733820234
Epoch 5, Loss: 0.6563012855393546
Epoch 6, Loss: 0.6447745391300747
Epoch 1, Loss: 0.8603659357343402
Epoch 2, Loss: 0.6659713217190334
Epoch 3, Loss: 0.6683519397463117
Epoch 4, Loss: 0.6673898356301444
Epoch 5, Loss: 0.6748150501932416
Epoch 6, Loss: 0.6699807558740888
