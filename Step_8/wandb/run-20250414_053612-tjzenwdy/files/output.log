train_Изъяты лексемы с частотой выше 49 - roberta-base.csv
test_Изъяты лексемы с частотой выше 49 - roberta-base.csv
Epoch 1, Loss: 0.840633362531662
Epoch 2, Loss: 0.6606167480349541
Epoch 3, Loss: 0.6333172395825386
Epoch 4, Loss: 0.6450367644429207
Epoch 5, Loss: 0.620570458471775
Epoch 6, Loss: 0.6570648178458214
Epoch 1, Loss: 0.8362561538815498
Epoch 2, Loss: 0.664011649787426
Epoch 3, Loss: 0.6683088839054108
Epoch 4, Loss: 0.6699238941073418
Epoch 5, Loss: 0.6591667234897614
Epoch 6, Loss: 0.6462287828326225
Epoch 1, Loss: 0.8703054636716843
Epoch 2, Loss: 0.6917585954070091
Epoch 3, Loss: 0.6805534362792969
Epoch 4, Loss: 0.6599666997790337
Epoch 5, Loss: 0.6657307520508766
Epoch 6, Loss: 0.6579445451498032
Epoch 1, Loss: 0.8513788059353828
Epoch 2, Loss: 0.6754409894347191
Epoch 3, Loss: 0.6409899070858955
Epoch 4, Loss: 0.6492816582322121
Epoch 5, Loss: 0.643934428691864
Epoch 6, Loss: 0.6527174785733223
Epoch 1, Loss: 0.8852879479527473
Epoch 2, Loss: 0.657201461493969
Epoch 3, Loss: 0.682875283062458
Epoch 4, Loss: 0.6772305741906166
Epoch 5, Loss: 0.6570115834474564
Epoch 6, Loss: 0.6528801918029785
Epoch 1, Loss: 0.8635457158088684
Epoch 2, Loss: 0.7078971564769745
Epoch 3, Loss: 0.6799390763044357
Epoch 4, Loss: 0.6643739864230156
Epoch 5, Loss: 0.6567211151123047
Epoch 6, Loss: 0.6500348448753357
