train_Изъяты лексемы с частотой выше 49 - roberta-base.csv
test_Изъяты лексемы с частотой выше 49 - roberta-base.csv
Epoch 1, Loss: 0.8959359005093575
Epoch 2, Loss: 0.6639107763767242
Epoch 3, Loss: 0.6559010744094849
Epoch 4, Loss: 0.6553951725363731
Epoch 5, Loss: 0.6310548111796379
Epoch 6, Loss: 0.6448049917817116
Epoch 1, Loss: 0.7999957352876663
Epoch 2, Loss: 0.6871495544910431
Epoch 3, Loss: 0.6734132319688797
Epoch 4, Loss: 0.6709643751382828
Epoch 5, Loss: 0.6583855152130127
Epoch 6, Loss: 0.649377167224884
Epoch 1, Loss: 0.8337244093418121
Epoch 2, Loss: 0.6845335513353348
Epoch 3, Loss: 0.6827273443341255
Epoch 4, Loss: 0.6721359565854073
Epoch 5, Loss: 0.6693597510457039
Epoch 6, Loss: 0.6637519299983978
Epoch 1, Loss: 0.827017568051815
Epoch 2, Loss: 0.6641072332859039
Epoch 3, Loss: 0.6555475667119026
Epoch 4, Loss: 0.6437567099928856
Epoch 5, Loss: 0.6440246403217316
Epoch 6, Loss: 0.6502379104495049
Epoch 1, Loss: 0.8983100950717926
Epoch 2, Loss: 0.6507524698972702
Epoch 3, Loss: 0.6665217503905296
Epoch 4, Loss: 0.6548389941453934
Epoch 5, Loss: 0.6376567780971527
Epoch 6, Loss: 0.6537763103842735
Epoch 1, Loss: 0.8734593614935875
Epoch 2, Loss: 0.6672452762722969
Epoch 3, Loss: 0.6768804863095284
Epoch 4, Loss: 0.6626663133502007
Epoch 5, Loss: 0.6643569022417068
Epoch 6, Loss: 0.6620961055159569
