train_Изъяты лексемы с частотой выше 3 - distilbert-base-multilingual.csv
test_Изъяты лексемы с частотой выше 3 - distilbert-base-multilingual.csv
Epoch 1, Loss: 0.6511746644973755
Epoch 2, Loss: 0.9865032434463501
Epoch 3, Loss: 0.732892632484436
Epoch 4, Loss: 0.5313929319381714
Epoch 5, Loss: 0.6585916876792908
Epoch 6, Loss: 0.5798968076705933
Epoch 1, Loss: 0.6674595475196838
Epoch 2, Loss: 0.9055814743041992
Epoch 3, Loss: 0.6963115334510803
Epoch 4, Loss: 0.4902630150318146
Epoch 5, Loss: 0.5496230721473694
Epoch 6, Loss: 0.5599715113639832
Epoch 1, Loss: 0.875535249710083
Epoch 2, Loss: 1.2873529195785522
Epoch 3, Loss: 1.0209485292434692
Epoch 4, Loss: 0.6999765038490295
Epoch 5, Loss: 0.6289507746696472
Epoch 6, Loss: 0.7150105834007263
Epoch 1, Loss: 0.7988365292549133
Epoch 2, Loss: 0.8789626955986023
Epoch 3, Loss: 0.7634371519088745
Epoch 4, Loss: 0.5583915710449219
Epoch 5, Loss: 0.4798729717731476
Epoch 6, Loss: 0.5935410261154175
Epoch 1, Loss: 0.6733526587486267
Epoch 2, Loss: 1.0679337978363037
Epoch 3, Loss: 0.7811846137046814
Epoch 4, Loss: 0.5571750402450562
Epoch 5, Loss: 0.7005930542945862
Epoch 6, Loss: 0.6477083563804626
Epoch 1, Loss: 0.5883649587631226
Epoch 2, Loss: 1.082607626914978
Epoch 3, Loss: 0.76224684715271
Epoch 4, Loss: 0.546476423740387
Epoch 5, Loss: 0.6863389611244202
Epoch 6, Loss: 0.5897036790847778
