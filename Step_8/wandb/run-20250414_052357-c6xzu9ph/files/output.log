train_Изъяты лексемы с частотой выше 3 - roberta-base.csv
test_Изъяты лексемы с частотой выше 3 - roberta-base.csv
Epoch 1, Loss: 0.6961812376976013
Epoch 2, Loss: 1.195231556892395
Epoch 3, Loss: 0.9031571745872498
Epoch 4, Loss: 0.5708696246147156
Epoch 5, Loss: 0.6513190269470215
Epoch 6, Loss: 0.6682087779045105
Epoch 1, Loss: 0.7341281771659851
Epoch 2, Loss: 0.8528159856796265
Epoch 3, Loss: 0.7144830822944641
Epoch 4, Loss: 0.5135068893432617
Epoch 5, Loss: 0.5459040999412537
Epoch 6, Loss: 0.5597227215766907
Epoch 1, Loss: 0.6157523989677429
Epoch 2, Loss: 1.188690423965454
Epoch 3, Loss: 0.8066917657852173
Epoch 4, Loss: 0.5406864285469055
Epoch 5, Loss: 0.7249035835266113
Epoch 6, Loss: 0.637852132320404
Epoch 1, Loss: 0.603952169418335
Epoch 2, Loss: 0.9036726951599121
Epoch 3, Loss: 0.7025806307792664
Epoch 4, Loss: 0.48588845133781433
Epoch 5, Loss: 0.5798473358154297
Epoch 6, Loss: 0.5665649175643921
Epoch 1, Loss: 0.7006255388259888
Epoch 2, Loss: 1.1455159187316895
Epoch 3, Loss: 0.8816789388656616
Epoch 4, Loss: 0.5821440815925598
Epoch 5, Loss: 0.6390033960342407
Epoch 6, Loss: 0.646004319190979
Epoch 1, Loss: 0.7646798491477966
Epoch 2, Loss: 1.0799371004104614
Epoch 3, Loss: 0.8236657977104187
Epoch 4, Loss: 0.5507758259773254
Epoch 5, Loss: 0.6523865461349487
Epoch 6, Loss: 0.6512559652328491
