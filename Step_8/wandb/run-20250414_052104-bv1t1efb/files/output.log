train_Изъяты лексемы с частотой выше 29 - roberta-base.csv
test_Изъяты лексемы с частотой выше 29 - roberta-base.csv
Epoch 1, Loss: 0.8022376671433449
Epoch 2, Loss: 0.6443080678582191
Epoch 3, Loss: 0.6429001241922379
Epoch 4, Loss: 0.6486800462007523
Epoch 5, Loss: 0.6433950066566467
Epoch 6, Loss: 0.6315721645951271
Epoch 1, Loss: 0.8863076195120811
Epoch 2, Loss: 0.7109676599502563
Epoch 3, Loss: 0.6717220768332481
Epoch 4, Loss: 0.6945605650544167
Epoch 5, Loss: 0.645058386027813
Epoch 6, Loss: 0.6566458642482758
Epoch 1, Loss: 0.8161585628986359
Epoch 2, Loss: 0.6505480706691742
Epoch 3, Loss: 0.656417541205883
Epoch 4, Loss: 0.6482648700475693
Epoch 5, Loss: 0.6408271491527557
Epoch 6, Loss: 0.6692809835076332
Epoch 1, Loss: 0.870636872947216
Epoch 2, Loss: 0.647402286529541
Epoch 3, Loss: 0.6178552471101284
Epoch 4, Loss: 0.6535058617591858
Epoch 5, Loss: 0.6413346752524376
Epoch 6, Loss: 0.6584295630455017
Epoch 1, Loss: 0.8795220404863358
Epoch 2, Loss: 0.6415347903966904
Epoch 3, Loss: 0.6352280676364899
Epoch 4, Loss: 0.6339453235268593
Epoch 5, Loss: 0.6187302395701408
Epoch 6, Loss: 0.6503717377781868
Epoch 1, Loss: 0.7113891877233982
Epoch 2, Loss: 0.6995023488998413
Epoch 3, Loss: 0.6451361626386642
Epoch 4, Loss: 0.6366267800331116
Epoch 5, Loss: 0.6433209627866745
Epoch 6, Loss: 0.6455042883753777
