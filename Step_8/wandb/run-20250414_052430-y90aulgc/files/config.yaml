_wandb:
    value:
        cli_version: 0.19.9
        m: []
        python_version: 3.8.13
        t:
            "1":
                - 1
                - 2
                - 3
                - 5
                - 11
                - 49
                - 53
                - 55
                - 71
            "2":
                - 1
                - 2
                - 3
                - 5
                - 11
                - 49
                - 53
                - 55
                - 71
            "3":
                - 2
                - 13
                - 16
                - 23
                - 55
            "4": 3.8.13
            "5": 0.19.9
            "6": 4.30.0
            "8":
                - 1
                - 5
            "10":
                - 20
            "12": 0.19.9
            "13": linux-x86_64
batch_size:
    value: 128
dataset:
    value: 1.Первый жанр исходная выборка
device:
    value: cuda
epochs:
    value: 6
freq:
    value: null
learning_rate:
    value: 5e-05
max_length:
    value: 60
model:
    value: roberta-base
model_name:
    value: roberta-base
model_type:
    value: bert
num_repeats:
    value: 6
test_size:
    value: 0.2
threshold:
    value: 0.5
tokenizer:
    value: 'RobertaTokenizerFast(name_or_path=''roberta-base'', vocab_size=50265, model_max_length=512, is_fast=True, padding_side=''right'', truncation_side=''right'', special_tokens={''bos_token'': ''<s>'', ''eos_token'': ''</s>'', ''unk_token'': ''<unk>'', ''sep_token'': ''</s>'', ''pad_token'': ''<pad>'', ''cls_token'': ''<s>'', ''mask_token'': AddedToken("<mask>", rstrip=False, lstrip=True, single_word=False, normalized=False)}, clean_up_tokenization_spaces=True)'
type:
    value: pos
