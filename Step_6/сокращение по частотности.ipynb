{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad183046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m../dataset\u001b[00m\r\n",
      "├── \u001b[01;34men_espanol\u001b[00m\r\n",
      "│   ├── docx2txt.py\r\n",
      "│   ├── Второй_жанр_исходная.txt\r\n",
      "│   └── Первый_жанр_исходная.txt\r\n",
      "├── Второй_жанр_исходная.txt\r\n",
      "├── Первый_жанр_исходная.txt\r\n",
      "├── \u001b[01;34mСокращение по частям речи\u001b[00m\r\n",
      "│   ├── 1.Первый жанр исходная выборка.txt\r\n",
      "│   ├── 2.Первый жанр без клауз, включающих наречия.txt\r\n",
      "│   ├── 3.Первый жанр без клауз, включающих глаголы.txt\r\n",
      "│   ├── 4. Первый жанр без клауз, включающих глаголы и наречия.txt\r\n",
      "│   ├── Без прилагательных второй жанр.txt\r\n",
      "│   ├── Без прилагательных первый жанр.txt\r\n",
      "│   └── Случайные выборки.txt\r\n",
      "└── \u001b[01;34mсокращение по частотности\u001b[00m\r\n",
      "    ├── 1а_ без сокращений.txt\r\n",
      "    ├── 1б_Изъяты лексемы с частотой выше 100.txt\r\n",
      "    ├── 1в_Изъяты лексемы с частотой выше 49.txt\r\n",
      "    ├── 1г_Изъяты лексемы с частотой выше 29.txt\r\n",
      "    ├── 1д_Изъяты лексемы с частотой выше 9.txt\r\n",
      "    ├── 1е_Изъяты лексемы с частотой выше 5.txt\r\n",
      "    ├── 1ё_Изъяты лексемы с частотой выше 3.txt\r\n",
      "    ├── 2а_ без сокращений.txt\r\n",
      "    ├── 2б_Изъяты лексемы с частотой выше 100.txt\r\n",
      "    ├── 2в_Изъяты лексемы с частотой выше 49.txt\r\n",
      "    ├── 2г_Изъяты лексемы с частотой выше 29.txt\r\n",
      "    ├── 2д_Изъяты лексемы с частотой выше 9.txt\r\n",
      "    ├── 2е_Изъяты лексемы с частотой выше 5.txt\r\n",
      "    └── 2ё_Изъяты лексемы с частотой выше 3.txt\r\n",
      "\r\n",
      "3 directories, 26 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree ../dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64ca2640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-24 14:19:23.736895: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#moditifcacion para frecuencia\n",
    "import re\n",
    "import razdel\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from isanlp.pipeline_common import PipelineCommon\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from razdel import sentenize\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "def cargar_datos(archivo, etiqueta):\n",
    "    with open(archivo, 'r', encoding='utf-8') as f:\n",
    "        textos = f.readlines()\n",
    "    return [(texto.strip(), etiqueta) for texto in textos]\n",
    "\n",
    "def tokenizar_fuction(texto):\n",
    "    \"\"\"Tokeniza el texto eliminando puntuación y convirtiendo a minúsculas.\"\"\"\n",
    "    texto_preprocesado = re.sub(r'[^\\w\\s]', '', texto.lower())\n",
    "    tokens = nltk.word_tokenize(texto_preprocesado)\n",
    "    return tokens\n",
    "\n",
    "def calcular_similitud_mayoria_optimizado(textos1, textos2, threshold):\n",
    "    \"\"\"\n",
    "        Calcula similitudes entre dos listas de textos usando GPU.\n",
    "        Args:\n",
    "            textos1: Lista de textos (original_datos_procesados).\n",
    "            textos2: Lista de textos (train_data).\n",
    "        Returns:\n",
    "            Matriz booleana [len(textos1), len(textos2)] indicando similitudes.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "    # Preprocesar todos los textos en batch\n",
    "    textos1_preprocesados = [' '.join(tokenizar_fuction(t)) for t in textos1]\n",
    "    textos2_preprocesados = [' '.join(tokenizar_fuction(t)) for t in textos2]\n",
    "\n",
    "    # Crear representación binaria de tokens\n",
    "    vectorizer = CountVectorizer(binary=True, tokenizer=nltk.word_tokenize)\n",
    "    vectorizer.fit(textos1_preprocesados + textos2_preprocesados)\n",
    "\n",
    "    # Convertir a matrices de tokens\n",
    "    matriz_tokens1 = vectorizer.transform(textos1_preprocesados).toarray()\n",
    "    matriz_tokens2 = vectorizer.transform(textos2_preprocesados).toarray()\n",
    "\n",
    "    # Mover a GPU\n",
    "    matriz_tokens1 = torch.from_numpy(matriz_tokens1).to(device)\n",
    "    matriz_tokens2 = torch.from_numpy(matriz_tokens2).to(device)\n",
    "\n",
    "     # Calcular intersecciones\n",
    "    interseccion = torch.matmul(matriz_tokens1.float(), matriz_tokens2.T.float())\n",
    "\n",
    "    # Tamaños de los conjuntos de tokens\n",
    "    tamano_tokens1 = matriz_tokens1.sum(dim=1).unsqueeze(1)\n",
    "    tamano_tokens2 = matriz_tokens2.sum(dim=1).unsqueeze(0)\n",
    "\n",
    "    # Evitar división por cero\n",
    "    tamano_tokens1 = tamano_tokens1.clamp(min=1)\n",
    "    tamano_tokens2 = tamano_tokens2.clamp(min=1)\n",
    "\n",
    "     # Calcular proporciones\n",
    "    prop_1_en_2 = interseccion / tamano_tokens2\n",
    "    prop_2_en_1 = interseccion / tamano_tokens1\n",
    "\n",
    "    similitud = (prop_1_en_2 > threshold) | (prop_2_en_1 > threshold)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return similitud\n",
    "\n",
    "class DataLoader_raw:\n",
    "    def __init__(self, path1, path2):\n",
    "        self.path1 = path1\n",
    "        self.path2 = path2\n",
    "        self.path_original1 = '../dataset/Первый_жанр_исходная.txt'\n",
    "        self.path_original2 = '../dataset/Второй_жанр_исходная.txt'    \n",
    "    \n",
    "    def __call__(self):\n",
    "        datos_genero1 = cargar_datos(self.path1, 0)\n",
    "        datos_genero2 = cargar_datos(self.path2, 1)\n",
    "        datos = datos_genero1 + datos_genero2\n",
    "        \n",
    "        datos_original1 = cargar_datos(self.path_original1, 0)\n",
    "        datos_original2 = cargar_datos(self.path_original2, 1)\n",
    "        datos_originales = datos_original1 + datos_original2\n",
    "        \n",
    "        return {\n",
    "            'datos_raw': datos,\n",
    "            'original_datos_raw': datos_originales\n",
    "        }\n",
    "\n",
    "class SentenceSplitterAndCleaner:\n",
    "    def __init__(self, tokenizer, min_length_threshold=6):\n",
    "        \"\"\"Inicializa el procesador con un tokenizador y un umbral de longitud mínima.\"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.min_length_threshold = min_length_threshold\n",
    "    \n",
    "    def _clean_and_split_text(self, texto):\n",
    "        \"\"\"\n",
    "        Limpia y divide un texto en oraciones procesadas.\n",
    "        \n",
    "        Args:\n",
    "            texto (str): Texto crudo a procesar.\n",
    "        \n",
    "        Returns:\n",
    "            list: Lista de oraciones limpias.\n",
    "        \"\"\"\n",
    "        # Paso 1: Marcar puntos para evitar fusiones no deseadas\n",
    "        texto_limpio = re.sub(r'\\.,', '. Ok999999999 ,', texto)  # Caso 1: Punto seguido de coma\n",
    "        texto_limpio = re.sub(r'\\.;', '. Ok999999999 ', texto_limpio)  # Caso 2: Punto seguido de punto y coma\n",
    "        texto_limpio = re.sub(r'\\. ([a-zа-я])', r'. Ok999999999 \\1', texto_limpio)  # Caso 3: Punto seguido de minúscula\n",
    "        texto_limpio = re.sub(r'(\\w)([А-Я])', r'\\1. \\2', texto_limpio)  # Caso 4: Minúscula seguida de mayúscula\n",
    "        \n",
    "        #creo que hay que agregar una nueva condivion  дл.,0 \n",
    "        #\", 2-3 см шир.\",0 \n",
    "        #\", 1 см шир.\",0 si hay un punto uego numeros quitar el punto\n",
    "        \n",
    "        # Paso 2: Eliminar corchetes y dividir en oraciones\n",
    "        texto_sin_corchetes = re.sub(r'\\[.*?\\]', '', texto_limpio).strip()\n",
    "        oraciones = [oracion.text for oracion in sentenize(texto_sin_corchetes)]\n",
    "        \n",
    "        # Paso 3: Restaurar espacios y limpiar marcadores temporales\n",
    "        oraciones_limpias = [re.sub(r'\\s*Ok999999999', ' ', oracion).strip() for oracion in oraciones]\n",
    "        \n",
    "        return oraciones_limpias\n",
    "    \n",
    "    def _process_sentences(self, datos, target_list):\n",
    "        \"\"\"\n",
    "        Procesa un conjunto de datos y agrega oraciones válidas a la lista objetivo.\n",
    "        \n",
    "        Args:\n",
    "            datos (list): Lista de tuplas (texto, etiqueta).\n",
    "            target_list (list): Lista donde se almacenarán las oraciones procesadas.\n",
    "        \"\"\"\n",
    "        for texto, etiqueta in datos:\n",
    "            oraciones = self._clean_and_split_text(texto)\n",
    "            for oracion in oraciones:\n",
    "                # Filtrar oraciones cortas basadas en el número de tokens\n",
    "                if len(self.tokenizer.encode(oracion, truncation=False)) >= self.min_length_threshold:\n",
    "                    target_list.append((oracion, etiqueta))\n",
    "    \n",
    "    def __call__(self, data_raw, original_data_raw):\n",
    "        \"\"\"\n",
    "        Procesa los datos crudos y originales, devolviendo dos conjuntos limpios.\n",
    "        \n",
    "        Args:\n",
    "            data_raw (list): Datos modificados crudos.\n",
    "            original_data_raw (list): Datos originales crudos.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Diccionario con datos procesados y originales procesados.\n",
    "        \"\"\"\n",
    "        datos_procesados = []\n",
    "        original_datos_procesados = []\n",
    "        \n",
    "        # Procesar datos modificados\n",
    "        self._process_sentences(data_raw, datos_procesados)\n",
    "        \n",
    "        # Procesar datos originales\n",
    "        self._process_sentences(original_data_raw, original_datos_procesados)\n",
    "        \n",
    "        return {\n",
    "            'datos_procesados': datos_procesados,\n",
    "            'original_datos_procesados': original_datos_procesados\n",
    "        }\n",
    "    \n",
    "class DataProcessor:\n",
    "    #def __init__(self, tokenizer, max_length, random_state, test_size=0.2):\n",
    "    def __init__(self,\n",
    "                 tokenizer,\n",
    "                 max_length,\n",
    "                 random_state,\n",
    "                 test_size=0.2,\n",
    "                 name='сокращение по частотности',\n",
    "                 threshold = 0.5\n",
    "                ):\n",
    "    \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.name = name\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def __call__(self, datos_procesados, original_datos_procesados):\n",
    "        #datos_procesados = data['datos_procesados']\n",
    "        df = pd.DataFrame(datos_procesados, columns=[\"text\", \"label\"])\n",
    "\n",
    "\n",
    "        train_data, test_data = train_test_split(df, test_size=self.test_size, random_state=self.random_state)\n",
    "        #generar un nuevo test data\n",
    "        #print(\"test_data:\", len(test_data))\n",
    "        train_texts = train_data[\"text\"].tolist()\n",
    "        original_texts = [oracion for oracion, _ in original_datos_procesados]\n",
    "        original_labels = [etiqueta for _, etiqueta in original_datos_procesados]\n",
    "        \n",
    "        similitudes = calcular_similitud_mayoria_optimizado(original_texts,\n",
    "                                                            train_texts,\n",
    "                                                            self.threshold)  # [len(original), len(train)]\n",
    "        \n",
    "        interseccion = similitudes.any(dim=1)  # True si hay intersección con algún train_text\n",
    "        mask = ~interseccion.cpu().numpy()\n",
    "        nuevo_test = [(texto, etiqueta) for texto, etiqueta, keep in zip(original_texts, original_labels, mask) if keep]\n",
    "        print(\"train_data:\", len(train_data))\n",
    "        print(\"test_data:\", len(test_data))\n",
    "        print(\"new_test:\", len(nuevo_test))\n",
    "        print(\"teorical len:\", len(original_texts) - len(train_data))\n",
    "\n",
    "        \n",
    "         \n",
    "        if self.random_state == 0:\n",
    "            train_data.to_csv(f'сокращение по частотности/train_{self.name}', index=False)\n",
    "            print(f\"train_{self.name}.csv\")\n",
    "            nuevo_test_df = pd.DataFrame(nuevo_test, columns=[\"text\", \"label\"])\n",
    "            \n",
    "            nuevo_test_df.to_csv(f'сокращение по частотности/test_{self.name}', index=False)\n",
    "            print(f\"test_{self.name}.csv\")\n",
    "        \n",
    "        #no se a niandido el nuevo test\n",
    "        # Tokenizar entrenamiento\n",
    "        train_encodings = self.tokenizer(\n",
    "            train_data[\"text\"].tolist(),\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        train_labels = torch.tensor(train_data[\"label\"].values)\n",
    "        # Tokenizar prueba\n",
    "        test_encodings = self.tokenizer(\n",
    "            [texto for texto, _ in nuevo_test],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        test_labels = torch.tensor([etiqueta for _, etiqueta in nuevo_test])\n",
    "        return {\n",
    "            'train_encodings': train_encodings,\n",
    "            'train_labels': train_labels,\n",
    "            'test_encodings': test_encodings,\n",
    "            'test_labels': test_labels\n",
    "        }  \n",
    "    \n",
    "    \n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "class DatasetCreator:\n",
    "    def __init__(self, batch_size=8):\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __call__(self, train_encodings, train_labels, test_encodings, test_labels):\n",
    "        train_dataset = TextDataset(train_encodings, train_labels)\n",
    "        test_dataset = TextDataset(test_encodings, test_labels)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        return {'train_loader': train_loader, 'test_loader': test_loader}\n",
    "    \n",
    "def train_model(model, train_loader, optimizer, loss_fn, device, epochs=3):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        loop = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}')\n",
    "        for batch in loop:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "        torch.cuda.empty_cache()#libera memroria de la gpu\n",
    "        gc.collect()#limpa la cpu\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    accuracy = sum(p == l for p, l in zip(all_preds, all_labels)) / len(all_labels)\n",
    "    report = classification_report(all_labels, all_preds, target_names=[\"Género 1\", \"Género 2\"])\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    return {\"accuracy\": accuracy, \"report\": report, \"conf_matrix\": conf_matrix}\n",
    "\n",
    "def display_results(results):\n",
    "    print(f'Precisión en el conjunto de prueba: {results[\"accuracy\"]:.2%}')\n",
    "    print(\"\\nReporte de clasificación:\\n\", results[\"report\"])\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(results[\"conf_matrix\"], annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=[\"Género 1\", \"Género 2\"], \n",
    "                yticklabels=[\"Género 1\", \"Género 2\"])\n",
    "    plt.title('Matriz de Confusión')\n",
    "    plt.xlabel('Predicción')\n",
    "    plt.ylabel('Real')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbade741",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetición con semilla 0\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 864\n",
      "test_data: 217\n",
      "new_test: 195\n",
      "teorical len: 247\n",
      "train_Изъяты лексемы с частотой выше 100.csv\n",
      "test_Изъяты лексемы с частотой выше 100.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 14/14 [00:03<00:00,  4.06it/s, loss=0.169] \n",
      "Epoch 2/3: 100%|██████████| 14/14 [00:03<00:00,  4.11it/s, loss=0.145] \n",
      "Epoch 3/3: 100%|██████████| 14/14 [00:03<00:00,  4.19it/s, loss=0.00817]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 0 seed: 1.0000\n",
      "Repetición con semilla 1\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 864\n",
      "test_data: 217\n",
      "new_test: 198\n",
      "teorical len: 247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 14/14 [00:03<00:00,  4.12it/s, loss=0.217]\n",
      "Epoch 2/3: 100%|██████████| 14/14 [00:03<00:00,  4.13it/s, loss=0.018] \n",
      "Epoch 3/3: 100%|██████████| 14/14 [00:03<00:00,  4.11it/s, loss=0.00485]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 1 seed: 0.9949\n",
      "Repetición con semilla 2\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 864\n",
      "test_data: 217\n",
      "new_test: 196\n",
      "teorical len: 247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 14/14 [00:03<00:00,  4.10it/s, loss=0.132]\n",
      "Epoch 2/3: 100%|██████████| 14/14 [00:03<00:00,  4.06it/s, loss=0.0617]\n",
      "Epoch 3/3: 100%|██████████| 14/14 [00:03<00:00,  4.02it/s, loss=0.0349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 2 seed: 0.9949\n",
      "Repetición con semilla 3\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 864\n",
      "test_data: 217\n",
      "new_test: 194\n",
      "teorical len: 247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 14/14 [00:03<00:00,  4.03it/s, loss=0.0879]\n",
      "Epoch 2/3: 100%|██████████| 14/14 [00:03<00:00,  4.16it/s, loss=0.0097]\n",
      "Epoch 3/3: 100%|██████████| 14/14 [00:03<00:00,  4.12it/s, loss=0.00445]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 3 seed: 0.9845\n",
      "Repetición con semilla 4\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 864\n",
      "test_data: 217\n",
      "new_test: 201\n",
      "teorical len: 247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 14/14 [00:03<00:00,  4.08it/s, loss=0.152]\n",
      "Epoch 2/3: 100%|██████████| 14/14 [00:03<00:00,  4.14it/s, loss=0.018] \n",
      "Epoch 3/3: 100%|██████████| 14/14 [00:03<00:00,  4.16it/s, loss=0.00751]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 4 seed: 0.9900\n",
      "Repetición con semilla 5\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 864\n",
      "test_data: 217\n",
      "new_test: 184\n",
      "teorical len: 247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 14/14 [00:03<00:00,  4.14it/s, loss=0.141]\n",
      "Epoch 2/3: 100%|██████████| 14/14 [00:03<00:00,  4.16it/s, loss=0.0751]\n",
      "Epoch 3/3: 100%|██████████| 14/14 [00:03<00:00,  4.14it/s, loss=0.00793]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 5 seed: 0.9837\n",
      "\n",
      "Accuracy promedio después de 6 repeticiones: 0.9914 ± 0.0059\n"
     ]
    }
   ],
   "source": [
    "# Inicializar el tokenizador\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n",
    "max_length = 128\n",
    "#Longitud promedio de tokens: 27.77\n",
    "#Longitud máxima de tokens: 192\n",
    "#Longitud mínima de tokens: 2\n",
    "#falta ver la logitud real del nuevo dataset\n",
    "# batch_size = 8\n",
    "batch_size = 64\n",
    "epochs = 3\n",
    "num_repeats = 6\n",
    "seeds = list(range(num_repeats))\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"Repetición con semilla {seed}\")\n",
    "    \n",
    "    # Instanciar DataProcessor con la semilla actual\n",
    "    #data_processor = DataProcessor(tokenizer, max_length, test_size=0.2, random_state=seed)\n",
    "    \n",
    "    # Crear el pipeline (ajusta según tu estructura)\n",
    "    ppl = PipelineCommon([\n",
    "        (DataLoader_raw('../dataset/сокращение по частотности/1б_Изъяты лексемы с частотой выше 100.txt',\n",
    "                        '../dataset/сокращение по частотности/2б_Изъяты лексемы с частотой выше 100.txt'), \n",
    "         [], \n",
    "         {'datos_raw': 'datos_raw',\n",
    "         'original_datos_raw': 'original_datos_raw'}),\n",
    "        (SentenceSplitterAndCleaner(tokenizer), \n",
    "         ['datos_raw','original_datos_raw'], \n",
    "         {'datos_procesados': 'datos_procesados',\n",
    "         'original_datos_procesados': 'original_datos_procesados'}),\n",
    "        (DataProcessor(tokenizer,\n",
    "                       max_length,\n",
    "                       seed,\n",
    "                       name = 'Изъяты лексемы с частотой выше 100',\n",
    "                       threshold = 0.5),  #217 - 195 -> aceptable\n",
    "        \n",
    "         ['datos_procesados','original_datos_procesados'], \n",
    "         {'train_encodings': 'train_encodings', 'train_labels': 'train_labels', \n",
    "          'test_encodings': 'test_encodings', 'test_labels': 'test_labels'}),\n",
    "        (DatasetCreator(batch_size = batch_size), \n",
    "         ['train_encodings', 'train_labels', 'test_encodings', 'test_labels'], \n",
    "         {'train_loader': 'train_loader', 'test_loader': 'test_loader'})\n",
    "    ])\n",
    "    \n",
    "    # Ejecutar el pipeline\n",
    "    result = ppl()\n",
    "    train_loader = result['train_loader']\n",
    "    test_loader = result['test_loader']\n",
    "    \n",
    "    # Configurar el modelo (reiniciarlo en cada repetición)\n",
    "    model = BertForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=2)\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Entrenar el modelo (función train_model definida por ti)\n",
    "    train_model(model, train_loader, optimizer, loss_fn, device, epochs=epochs)\n",
    "    \n",
    "    # Evaluar el modelo (función evaluate_model definida por ti)\n",
    "    results = evaluate_model(model, test_loader, device)\n",
    "    accuracy = results['accuracy']  # Ajusta según la métrica que uses\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    print(f\"Accuracy in {seed} seed: {accuracy:.4f}\")\n",
    "\n",
    "# Calcular promedio y desviación estándar\n",
    "average_accuracy = np.mean(accuracies)\n",
    "std_accuracy = np.std(accuracies)\n",
    "print(f\"\\nAccuracy promedio después de {num_repeats} repeticiones: {average_accuracy:.4f} ± {std_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2bb4f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetición con semilla 0\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 887\n",
      "test_data: 222\n",
      "new_test: 187\n",
      "teorical len: 224\n",
      "train_Изъяты лексемы с частотой выше 49.csv\n",
      "test_Изъяты лексемы с частотой выше 49.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 14/14 [00:03<00:00,  4.01it/s, loss=0.195]\n",
      "Epoch 2/3: 100%|██████████| 14/14 [00:03<00:00,  4.04it/s, loss=0.0412]\n",
      "Epoch 3/3: 100%|██████████| 14/14 [00:03<00:00,  4.05it/s, loss=0.0101] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 0 seed: 0.9947\n",
      "Repetición con semilla 1\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 887\n",
      "test_data: 222\n",
      "new_test: 209\n",
      "teorical len: 224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 14/14 [00:03<00:00,  4.03it/s, loss=0.133]\n",
      "Epoch 2/3: 100%|██████████| 14/14 [00:03<00:00,  4.04it/s, loss=0.0122]\n",
      "Epoch 3/3: 100%|██████████| 14/14 [00:03<00:00,  4.01it/s, loss=0.00555]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 1 seed: 0.9904\n",
      "Repetición con semilla 2\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 887\n",
      "test_data: 222\n",
      "new_test: 205\n",
      "teorical len: 224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 14/14 [00:03<00:00,  3.95it/s, loss=0.174]\n",
      "Epoch 2/3: 100%|██████████| 14/14 [00:03<00:00,  4.03it/s, loss=0.0539]\n",
      "Epoch 3/3: 100%|██████████| 14/14 [00:03<00:00,  4.05it/s, loss=0.0065] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 2 seed: 0.9854\n",
      "Repetición con semilla 3\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 887\n",
      "test_data: 222\n",
      "new_test: 192\n",
      "teorical len: 224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 14/14 [00:03<00:00,  3.96it/s, loss=0.115]\n",
      "Epoch 2/3: 100%|██████████| 14/14 [00:03<00:00,  3.98it/s, loss=0.0133]\n",
      "Epoch 3/3: 100%|██████████| 14/14 [00:03<00:00,  3.98it/s, loss=0.00625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 3 seed: 0.9844\n",
      "Repetición con semilla 4\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 887\n",
      "test_data: 222\n",
      "new_test: 201\n",
      "teorical len: 224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 14/14 [00:03<00:00,  4.00it/s, loss=0.115]\n",
      "Epoch 2/3: 100%|██████████| 14/14 [00:03<00:00,  4.03it/s, loss=0.0963]\n",
      "Epoch 3/3: 100%|██████████| 14/14 [00:03<00:00,  4.06it/s, loss=0.111]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 4 seed: 1.0000\n",
      "Repetición con semilla 5\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 887\n",
      "test_data: 222\n",
      "new_test: 195\n",
      "teorical len: 224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 14/14 [00:03<00:00,  4.04it/s, loss=0.206]\n",
      "Epoch 2/3: 100%|██████████| 14/14 [00:03<00:00,  4.01it/s, loss=0.0323]\n",
      "Epoch 3/3: 100%|██████████| 14/14 [00:03<00:00,  3.98it/s, loss=0.00663]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 5 seed: 0.9949\n",
      "\n",
      "Accuracy promedio después de 6 repeticiones: 0.9916 ± 0.0055\n"
     ]
    }
   ],
   "source": [
    "#    ├── 1а_ без сокращений.txt\n",
    "#    ├── 1б_Изъяты лексемы с частотой выше 100.txt\n",
    "#    ├── 1в_Изъяты лексемы с частотой выше 49.txt ***\n",
    "#    ├── 1г_Изъяты лексемы с частотой выше 29.txt\n",
    "#    ├── 1д_Изъяты лексемы с частотой выше 9.txt\n",
    "#    ├── 2а_ без сокращений.txt\n",
    "#    ├── 2б_Изъяты лексемы с частотой выше 100.txt\n",
    "#    ├── 2в_Изъяты лексемы с частотой выше 49.txt  ***\n",
    "#    ├── 2г_Изъяты лексемы с частотой выше 29.txt\n",
    "#    └── 2д_Изъяты лексемы с частотой выше 9.txt\n",
    "accuracies = []\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"Repetición con semilla {seed}\")\n",
    "    \n",
    "    # Instanciar DataProcessor con la semilla actual\n",
    "    #data_processor = DataProcessor(tokenizer, max_length, test_size=0.2, random_state=seed)\n",
    "    \n",
    "    # Crear el pipeline (ajusta según tu estructura)\n",
    "    ppl = PipelineCommon([\n",
    "        (DataLoader_raw('../dataset/сокращение по частотности/1в_Изъяты лексемы с частотой выше 49.txt',\n",
    "                        '../dataset/сокращение по частотности/2в_Изъяты лексемы с частотой выше 49.txt'), \n",
    "         [], \n",
    "         {'datos_raw': 'datos_raw',\n",
    "         'original_datos_raw': 'original_datos_raw'}),\n",
    "        (SentenceSplitterAndCleaner(tokenizer), \n",
    "         ['datos_raw','original_datos_raw'], \n",
    "         {'datos_procesados': 'datos_procesados',\n",
    "         'original_datos_procesados': 'original_datos_procesados'}),\n",
    "        (DataProcessor(tokenizer,\n",
    "                       max_length,\n",
    "                       seed,\n",
    "                       name = 'Изъяты лексемы с частотой выше 49',\n",
    "                       #threshold = 0.5), #222 - 186\n",
    "                       #threshold = 0.4), #222- 66 \n",
    "                       threshold = 0.6),#222 -209 -> aceptable\n",
    "         ['datos_procesados','original_datos_procesados'], \n",
    "         {'train_encodings': 'train_encodings', 'train_labels': 'train_labels', \n",
    "          'test_encodings': 'test_encodings', 'test_labels': 'test_labels'}),\n",
    "        (DatasetCreator(batch_size = batch_size), \n",
    "         ['train_encodings', 'train_labels', 'test_encodings', 'test_labels'], \n",
    "         {'train_loader': 'train_loader', 'test_loader': 'test_loader'})\n",
    "    ])\n",
    "    \n",
    "    # Ejecutar el pipeline\n",
    "    result = ppl()\n",
    "    train_loader = result['train_loader']\n",
    "    test_loader = result['test_loader']\n",
    "    \n",
    "    # Configurar el modelo (reiniciarlo en cada repetición)\n",
    "    model = BertForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=2)\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Entrenar el modelo (función train_model definida por ti)\n",
    "    train_model(model, train_loader, optimizer, loss_fn, device, epochs=epochs)\n",
    "    \n",
    "    # Evaluar el modelo (función evaluate_model definida por ti)\n",
    "    results = evaluate_model(model, test_loader, device)\n",
    "    accuracy = results['accuracy']  # Ajusta según la métrica que uses\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    print(f\"Accuracy in {seed} seed: {accuracy:.4f}\")\n",
    "\n",
    "# Calcular promedio y desviación estándar\n",
    "average_accuracy = np.mean(accuracies)\n",
    "std_accuracy = np.std(accuracies)\n",
    "print(f\"\\nAccuracy promedio después de {num_repeats} repeticiones: {average_accuracy:.4f} ± {std_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7cdf93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetición con semilla 0\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 869\n",
      "test_data: 218\n",
      "new_test: 197\n",
      "teorical len: 242\n",
      "train_Изъяты лексемы с частотой выше 29.csv\n",
      "test_Изъяты лексемы с частотой выше 29.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 14/14 [00:03<00:00,  4.08it/s, loss=0.117]\n",
      "Epoch 2/3: 100%|██████████| 14/14 [00:03<00:00,  4.10it/s, loss=0.11]  \n",
      "Epoch 3/3: 100%|██████████| 14/14 [00:03<00:00,  4.05it/s, loss=0.00689]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 0 seed: 1.0000\n",
      "Repetición con semilla 1\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 869\n",
      "test_data: 218\n",
      "new_test: 214\n",
      "teorical len: 242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 14/14 [00:03<00:00,  4.02it/s, loss=0.128]\n",
      "Epoch 2/3: 100%|██████████| 14/14 [00:03<00:00,  4.01it/s, loss=0.0545]\n",
      "Epoch 3/3: 100%|██████████| 14/14 [00:03<00:00,  4.02it/s, loss=0.00683]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 1 seed: 0.9953\n",
      "Repetición con semilla 2\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 869\n",
      "test_data: 218\n",
      "new_test: 210\n",
      "teorical len: 242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 14/14 [00:03<00:00,  3.98it/s, loss=0.175]\n",
      "Epoch 2/3: 100%|██████████| 14/14 [00:03<00:00,  4.08it/s, loss=0.0169]\n",
      "Epoch 3/3: 100%|██████████| 14/14 [00:03<00:00,  4.09it/s, loss=0.00732]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 2 seed: 0.9905\n",
      "Repetición con semilla 3\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 869\n",
      "test_data: 218\n",
      "new_test: 211\n",
      "teorical len: 242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 14/14 [00:03<00:00,  4.12it/s, loss=0.197]\n",
      "Epoch 2/3: 100%|██████████| 14/14 [00:03<00:00,  4.09it/s, loss=0.0732]\n",
      "Epoch 3/3: 100%|██████████| 14/14 [00:03<00:00,  4.12it/s, loss=0.0459]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 3 seed: 0.9858\n",
      "Repetición con semilla 4\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 869\n",
      "test_data: 218\n",
      "new_test: 209\n",
      "teorical len: 242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 14/14 [00:03<00:00,  4.05it/s, loss=0.182]\n",
      "Epoch 2/3: 100%|██████████| 14/14 [00:03<00:00,  4.09it/s, loss=0.015] \n",
      "Epoch 3/3: 100%|██████████| 14/14 [00:03<00:00,  4.02it/s, loss=0.0085] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 4 seed: 1.0000\n",
      "Repetición con semilla 5\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 869\n",
      "test_data: 218\n",
      "new_test: 203\n",
      "teorical len: 242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 14/14 [00:03<00:00,  4.01it/s, loss=0.183]\n",
      "Epoch 2/3: 100%|██████████| 14/14 [00:03<00:00,  4.10it/s, loss=0.146] \n",
      "Epoch 3/3: 100%|██████████| 14/14 [00:03<00:00,  4.07it/s, loss=0.00884]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 5 seed: 0.9951\n",
      "\n",
      "Accuracy promedio después de 6 repeticiones: 0.9944 ± 0.0051\n"
     ]
    }
   ],
   "source": [
    "#    ├── 1а_ без сокращений.txt\n",
    "#    ├── 1б_Изъяты лексемы с частотой выше 100.txt\n",
    "#    ├── 1в_Изъяты лексемы с частотой выше 49.txt\n",
    "#    ├── 1г_Изъяты лексемы с частотой выше 29.txt ***\n",
    "#    ├── 1д_Изъяты лексемы с частотой выше 9.txt\n",
    "#    ├── 2а_ без сокращений.txt\n",
    "#    ├── 2б_Изъяты лексемы с частотой выше 100.txt\n",
    "#    ├── 2в_Изъяты лексемы с частотой выше 49.txt\n",
    "#    ├── 2г_Изъяты лексемы с частотой выше 29.txt ***\n",
    "#    └── 2д_Изъяты лексемы с частотой выше 9.txt\n",
    "accuracies = []\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"Repetición con semilla {seed}\")\n",
    "    \n",
    "    # Instanciar DataProcessor con la semilla actual\n",
    "    #data_processor = DataProcessor(tokenizer, max_length, test_size=0.2, random_state=seed)\n",
    "    \n",
    "    # Crear el pipeline (ajusta según tu estructura)\n",
    "    ppl = PipelineCommon([\n",
    "        (DataLoader_raw('../dataset/сокращение по частотности/1г_Изъяты лексемы с частотой выше 29.txt',\n",
    "                        '../dataset/сокращение по частотности/2г_Изъяты лексемы с частотой выше 29.txt'), \n",
    "         [], \n",
    "         {'datos_raw': 'datos_raw',\n",
    "         'original_datos_raw': 'original_datos_raw'}),\n",
    "        (SentenceSplitterAndCleaner(tokenizer), \n",
    "         ['datos_raw','original_datos_raw'], \n",
    "         {'datos_procesados': 'datos_procesados',\n",
    "         'original_datos_procesados': 'original_datos_procesados'}),\n",
    "        (DataProcessor(tokenizer,\n",
    "                       max_length,\n",
    "                       seed,\n",
    "                       name = 'Изъяты лексемы с частотой выше 29',\n",
    "                      threshold = 0.5), #218 -203\n",
    "         ['datos_procesados','original_datos_procesados'], \n",
    "         {'train_encodings': 'train_encodings', 'train_labels': 'train_labels', \n",
    "          'test_encodings': 'test_encodings', 'test_labels': 'test_labels'}),\n",
    "        (DatasetCreator(batch_size = batch_size), \n",
    "         ['train_encodings', 'train_labels', 'test_encodings', 'test_labels'], \n",
    "         {'train_loader': 'train_loader', 'test_loader': 'test_loader'})\n",
    "    ])\n",
    "    \n",
    "    # Ejecutar el pipeline\n",
    "    result = ppl()\n",
    "    train_loader = result['train_loader']\n",
    "    test_loader = result['test_loader']\n",
    "    \n",
    "    # Configurar el modelo (reiniciarlo en cada repetición)\n",
    "    model = BertForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=2)\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Entrenar el modelo (función train_model definida por ti)\n",
    "    train_model(model, train_loader, optimizer, loss_fn, device, epochs=epochs)\n",
    "    \n",
    "    # Evaluar el modelo (función evaluate_model definida por ti)\n",
    "    results = evaluate_model(model, test_loader, device)\n",
    "    accuracy = results['accuracy']  # Ajusta según la métrica que uses\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    print(f\"Accuracy in {seed} seed: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# Calcular promedio y desviación estándar\n",
    "average_accuracy = np.mean(accuracies)\n",
    "std_accuracy = np.std(accuracies)\n",
    "print(f\"\\nAccuracy promedio después de {num_repeats} repeticiones: {average_accuracy:.4f} ± {std_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76ceece2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetición con semilla 0\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 744\n",
      "test_data: 186\n",
      "new_test: 124\n",
      "teorical len: 367\n",
      "train_Изъяты лексемы с частотой выше 9.csv\n",
      "test_Изъяты лексемы с частотой выше 9.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 12/12 [00:02<00:00,  4.05it/s, loss=0.195]\n",
      "Epoch 2/3: 100%|██████████| 12/12 [00:02<00:00,  4.06it/s, loss=0.0663]\n",
      "Epoch 3/3: 100%|██████████| 12/12 [00:02<00:00,  4.07it/s, loss=0.00885]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 0 seed: 1.0000\n",
      "Repetición con semilla 1\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 744\n",
      "test_data: 186\n",
      "new_test: 135\n",
      "teorical len: 367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 12/12 [00:02<00:00,  4.03it/s, loss=0.384]\n",
      "Epoch 2/3: 100%|██████████| 12/12 [00:02<00:00,  4.05it/s, loss=0.159]\n",
      "Epoch 3/3: 100%|██████████| 12/12 [00:02<00:00,  4.09it/s, loss=0.177] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 1 seed: 0.9852\n",
      "Repetición con semilla 2\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 744\n",
      "test_data: 186\n",
      "new_test: 130\n",
      "teorical len: 367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 12/12 [00:02<00:00,  4.06it/s, loss=0.28] \n",
      "Epoch 2/3: 100%|██████████| 12/12 [00:02<00:00,  4.05it/s, loss=0.0322]\n",
      "Epoch 3/3: 100%|██████████| 12/12 [00:02<00:00,  4.08it/s, loss=0.0127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 2 seed: 0.9923\n",
      "Repetición con semilla 3\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 744\n",
      "test_data: 186\n",
      "new_test: 190\n",
      "teorical len: 367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 12/12 [00:02<00:00,  4.11it/s, loss=0.215]\n",
      "Epoch 2/3: 100%|██████████| 12/12 [00:02<00:00,  4.10it/s, loss=0.108]\n",
      "Epoch 3/3: 100%|██████████| 12/12 [00:02<00:00,  4.08it/s, loss=0.107] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 3 seed: 1.0000\n",
      "Repetición con semilla 4\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 744\n",
      "test_data: 186\n",
      "new_test: 154\n",
      "teorical len: 367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 12/12 [00:02<00:00,  4.08it/s, loss=0.265]\n",
      "Epoch 2/3: 100%|██████████| 12/12 [00:02<00:00,  4.12it/s, loss=0.0864]\n",
      "Epoch 3/3: 100%|██████████| 12/12 [00:02<00:00,  4.16it/s, loss=0.0609]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 4 seed: 0.9935\n",
      "Repetición con semilla 5\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 744\n",
      "test_data: 186\n",
      "new_test: 129\n",
      "teorical len: 367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 12/12 [00:02<00:00,  4.12it/s, loss=0.172]\n",
      "Epoch 2/3: 100%|██████████| 12/12 [00:02<00:00,  4.11it/s, loss=0.151] \n",
      "Epoch 3/3: 100%|██████████| 12/12 [00:02<00:00,  4.08it/s, loss=0.0643]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 5 seed: 1.0000\n",
      "\n",
      "Accuracy promedio después de 6 repeticiones: 0.9952 ± 0.0055\n"
     ]
    }
   ],
   "source": [
    "#    ├── 1а_ без сокращений.txt\n",
    "#    ├── 1б_Изъяты лексемы с частотой выше 100.txt\n",
    "#    ├── 1в_Изъяты лексемы с частотой выше 49.txt\n",
    "#    ├── 1г_Изъяты лексемы с частотой выше 29.txt\n",
    "#    ├── 1д_Изъяты лексемы с частотой выше 9.txt ***\n",
    "#    ├── 2а_ без сокращений.txt\n",
    "#    ├── 2б_Изъяты лексемы с частотой выше 100.txt\n",
    "#    ├── 2в_Изъяты лексемы с частотой выше 49.txt\n",
    "#    ├── 2г_Изъяты лексемы с частотой выше 29.txt\n",
    "#    └── 2д_Изъяты лексемы с частотой выше 9.txt ***\n",
    "accuracies = []\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"Repetición con semilla {seed}\")\n",
    "    \n",
    "    # Instanciar DataProcessor con la semilla actual\n",
    "    #data_processor = DataProcessor(tokenizer, max_length, test_size=0.2, random_state=seed)\n",
    "    \n",
    "    # Crear el pipeline (ajusta según tu estructura)\n",
    "    ppl = PipelineCommon([\n",
    "        (DataLoader_raw('../dataset/сокращение по частотности/1д_Изъяты лексемы с частотой выше 9.txt',\n",
    "                        '../dataset/сокращение по частотности/2д_Изъяты лексемы с частотой выше 9.txt'), \n",
    "         [], \n",
    "         {'datos_raw': 'datos_raw',\n",
    "         'original_datos_raw': 'original_datos_raw'}),\n",
    "        (SentenceSplitterAndCleaner(tokenizer), \n",
    "         ['datos_raw','original_datos_raw'], \n",
    "         {'datos_procesados': 'datos_procesados',\n",
    "         'original_datos_procesados': 'original_datos_procesados'}),\n",
    "        (DataProcessor(tokenizer,\n",
    "                       max_length,\n",
    "                       seed,\n",
    "                       name = 'Изъяты лексемы с частотой выше 9',\n",
    "                       #threshold = 0.5), #186 280\n",
    "                        threshold = 0.4),#186 124\n",
    "         ['datos_procesados','original_datos_procesados'], \n",
    "         {'train_encodings': 'train_encodings', 'train_labels': 'train_labels', \n",
    "          'test_encodings': 'test_encodings', 'test_labels': 'test_labels'}),\n",
    "        (DatasetCreator(batch_size = batch_size), \n",
    "         ['train_encodings', 'train_labels', 'test_encodings', 'test_labels'], \n",
    "         {'train_loader': 'train_loader', 'test_loader': 'test_loader'})\n",
    "    ])\n",
    "    \n",
    "    # Ejecutar el pipeline\n",
    "    result = ppl()\n",
    "    train_loader = result['train_loader']\n",
    "    test_loader = result['test_loader']\n",
    "    \n",
    "    # Configurar el modelo (reiniciarlo en cada repetición)\n",
    "    model = BertForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=2)\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Entrenar el modelo (función train_model definida por ti)\n",
    "    train_model(model, train_loader, optimizer, loss_fn, device, epochs=epochs)\n",
    "    \n",
    "    # Evaluar el modelo (función evaluate_model definida por ti)\n",
    "    results = evaluate_model(model, test_loader, device)\n",
    "    accuracy = results['accuracy']  # Ajusta según la métrica que uses\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    print(f\"Accuracy in {seed} seed: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# Calcular promedio y desviación estándar\n",
    "average_accuracy = np.mean(accuracies)\n",
    "std_accuracy = np.std(accuracies)\n",
    "print(f\"\\nAccuracy promedio después de {num_repeats} repeticiones: {average_accuracy:.4f} ± {std_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4eab77e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetición con semilla 0\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 864\n",
      "test_data: 217\n",
      "new_test: 163\n",
      "teorical len: 247\n",
      "train_Изъяты лексемы с частотой выше 5.txt.csv\n",
      "test_Изъяты лексемы с частотой выше 5.txt.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 14/14 [00:03<00:00,  4.12it/s, loss=0.194]\n",
      "Epoch 2/3: 100%|██████████| 14/14 [00:03<00:00,  4.12it/s, loss=0.0711]\n",
      "Epoch 3/3: 100%|██████████| 14/14 [00:03<00:00,  4.12it/s, loss=0.00875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 0 seed: 0.9877\n",
      "Repetición con semilla 1\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 864\n",
      "test_data: 217\n",
      "new_test: 172\n",
      "teorical len: 247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 14/14 [00:03<00:00,  4.13it/s, loss=0.128]\n",
      "Epoch 2/3: 100%|██████████| 14/14 [00:03<00:00,  4.13it/s, loss=0.178] \n",
      "Epoch 3/3: 100%|██████████| 14/14 [00:03<00:00,  4.11it/s, loss=0.00918]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 1 seed: 0.9826\n",
      "Repetición con semilla 2\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 864\n",
      "test_data: 217\n",
      "new_test: 205\n",
      "teorical len: 247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 14/14 [00:03<00:00,  4.04it/s, loss=0.212]\n",
      "Epoch 2/3: 100%|██████████| 14/14 [00:03<00:00,  4.06it/s, loss=0.263] \n",
      "Epoch 3/3: 100%|██████████| 14/14 [00:03<00:00,  3.99it/s, loss=0.00742]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 2 seed: 0.9951\n",
      "Repetición con semilla 3\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 864\n",
      "test_data: 217\n",
      "new_test: 165\n",
      "teorical len: 247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 14/14 [00:03<00:00,  4.07it/s, loss=0.283]\n",
      "Epoch 2/3: 100%|██████████| 14/14 [00:03<00:00,  4.05it/s, loss=0.194]\n",
      "Epoch 3/3: 100%|██████████| 14/14 [00:03<00:00,  4.04it/s, loss=0.139] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 3 seed: 0.9879\n",
      "Repetición con semilla 4\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 864\n",
      "test_data: 217\n",
      "new_test: 169\n",
      "teorical len: 247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 14/14 [00:03<00:00,  4.07it/s, loss=0.186]\n",
      "Epoch 2/3: 100%|██████████| 14/14 [00:03<00:00,  4.08it/s, loss=0.0338]\n",
      "Epoch 3/3: 100%|██████████| 14/14 [00:03<00:00,  4.09it/s, loss=0.343] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 4 seed: 0.9941\n",
      "Repetición con semilla 5\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 864\n",
      "test_data: 217\n",
      "new_test: 166\n",
      "teorical len: 247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 14/14 [00:03<00:00,  4.06it/s, loss=0.163]\n",
      "Epoch 2/3: 100%|██████████| 14/14 [00:03<00:00,  4.05it/s, loss=0.0145]\n",
      "Epoch 3/3: 100%|██████████| 14/14 [00:03<00:00,  4.11it/s, loss=0.00748]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 5 seed: 0.9940\n",
      "\n",
      "Accuracy promedio después de 6 repeticiones: 0.9902 ± 0.0045\n"
     ]
    }
   ],
   "source": [
    "#     ├── 1а_ без сокращений.txt\n",
    "#     ├── 1б_Изъяты лексемы с частотой выше 100.txt\n",
    "#     ├── 1в_Изъяты лексемы с частотой выше 49.txt\n",
    "#     ├── 1г_Изъяты лексемы с частотой выше 29.txt\n",
    "#     ├── 1д_Изъяты лексемы с частотой выше 9.txt\n",
    "#     ├── 1е_Изъяты лексемы с частотой выше 5.txt ***\n",
    "#     ├── 1ё_Изъяты лексемы с частотой выше 3.txt\n",
    "#     ├── 2а_ без сокращений.txt\n",
    "#     ├── 2б_Изъяты лексемы с частотой выше 100.txt\n",
    "#     ├── 2в_Изъяты лексемы с частотой выше 49.txt\n",
    "#     ├── 2г_Изъяты лексемы с частотой выше 29.txt\n",
    "#     ├── 2д_Изъяты лексемы с частотой выше 9.txt\n",
    "#     ├── 2е_Изъяты лексемы с частотой выше 5.txt ***\n",
    "#     └── 2ё_Изъяты лексемы с частотой выше 3.txt\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"Repetición con semilla {seed}\")\n",
    "    \n",
    "    # Instanciar DataProcessor con la semilla actual\n",
    "    #data_processor = DataProcessor(tokenizer, max_length, test_size=0.2, random_state=seed)\n",
    "    \n",
    "    # Crear el pipeline (ajusta según tu estructura)\n",
    "    ppl = PipelineCommon([\n",
    "        (DataLoader_raw('../dataset/сокращение по частотности/1е_Изъяты лексемы с частотой выше 5.txt',\n",
    "                        '../dataset/сокращение по частотности/2е_Изъяты лексемы с частотой выше 5.txt'), \n",
    "         [], \n",
    "         {'datos_raw': 'datos_raw',\n",
    "         'original_datos_raw': 'original_datos_raw'}),\n",
    "        (SentenceSplitterAndCleaner(tokenizer), \n",
    "         ['datos_raw','original_datos_raw'], \n",
    "         {'datos_procesados': 'datos_procesados',\n",
    "         'original_datos_procesados': 'original_datos_procesados'}),\n",
    "        (DataProcessor(tokenizer,\n",
    "                       max_length,\n",
    "                       seed,\n",
    "                       name = 'Изъяты лексемы с частотой выше 5.txt',\n",
    "                       #threshold = 0.5), #186 280\n",
    "                        threshold = 0.4),#186 124\n",
    "         ['datos_procesados','original_datos_procesados'], \n",
    "         {'train_encodings': 'train_encodings', 'train_labels': 'train_labels', \n",
    "          'test_encodings': 'test_encodings', 'test_labels': 'test_labels'}),\n",
    "        (DatasetCreator(batch_size = batch_size), \n",
    "         ['train_encodings', 'train_labels', 'test_encodings', 'test_labels'], \n",
    "         {'train_loader': 'train_loader', 'test_loader': 'test_loader'})\n",
    "    ])\n",
    "    \n",
    "    # Ejecutar el pipeline\n",
    "    result = ppl()\n",
    "    train_loader = result['train_loader']\n",
    "    test_loader = result['test_loader']\n",
    "    \n",
    "    # Configurar el modelo (reiniciarlo en cada repetición)\n",
    "    model = BertForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=2)\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Entrenar el modelo (función train_model definida por ti)\n",
    "    train_model(model, train_loader, optimizer, loss_fn, device, epochs=epochs)\n",
    "    \n",
    "    # Evaluar el modelo (función evaluate_model definida por ti)\n",
    "    results = evaluate_model(model, test_loader, device)\n",
    "    accuracy = results['accuracy']  # Ajusta según la métrica que uses\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    print(f\"Accuracy in {seed} seed: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# Calcular promedio y desviación estándar\n",
    "average_accuracy = np.mean(accuracies)\n",
    "std_accuracy = np.std(accuracies)\n",
    "print(f\"\\nAccuracy promedio después de {num_repeats} repeticiones: {average_accuracy:.4f} ± {std_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad9627f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetición con semilla 0\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 830\n",
      "test_data: 208\n",
      "new_test: 199\n",
      "teorical len: 281\n",
      "train_Изъяты лексемы с частотой выше 3.csv\n",
      "test_Изъяты лексемы с частотой выше 3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 13/13 [00:03<00:00,  3.93it/s, loss=0.234]\n",
      "Epoch 2/3: 100%|██████████| 13/13 [00:03<00:00,  3.95it/s, loss=0.12] \n",
      "Epoch 3/3: 100%|██████████| 13/13 [00:03<00:00,  3.91it/s, loss=0.0983]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 0 seed: 0.9849\n",
      "Repetición con semilla 1\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 830\n",
      "test_data: 208\n",
      "new_test: 257\n",
      "teorical len: 281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 13/13 [00:03<00:00,  3.93it/s, loss=0.306]\n",
      "Epoch 2/3: 100%|██████████| 13/13 [00:03<00:00,  3.99it/s, loss=0.176]\n",
      "Epoch 3/3: 100%|██████████| 13/13 [00:03<00:00,  3.97it/s, loss=0.102] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 1 seed: 1.0000\n",
      "Repetición con semilla 2\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 830\n",
      "test_data: 208\n",
      "new_test: 191\n",
      "teorical len: 281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 13/13 [00:03<00:00,  4.01it/s, loss=0.141]\n",
      "Epoch 2/3: 100%|██████████| 13/13 [00:03<00:00,  4.03it/s, loss=0.021] \n",
      "Epoch 3/3: 100%|██████████| 13/13 [00:03<00:00,  3.98it/s, loss=0.0442] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 2 seed: 0.9843\n",
      "Repetición con semilla 3\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 830\n",
      "test_data: 208\n",
      "new_test: 258\n",
      "teorical len: 281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 13/13 [00:03<00:00,  3.95it/s, loss=0.291]\n",
      "Epoch 2/3: 100%|██████████| 13/13 [00:03<00:00,  3.94it/s, loss=0.0491]\n",
      "Epoch 3/3: 100%|██████████| 13/13 [00:03<00:00,  3.99it/s, loss=0.0123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 3 seed: 0.9806\n",
      "Repetición con semilla 4\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 830\n",
      "test_data: 208\n",
      "new_test: 194\n",
      "teorical len: 281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 13/13 [00:03<00:00,  3.96it/s, loss=0.226]\n",
      "Epoch 2/3: 100%|██████████| 13/13 [00:03<00:00,  3.98it/s, loss=0.0783]\n",
      "Epoch 3/3: 100%|██████████| 13/13 [00:03<00:00,  4.00it/s, loss=0.0226] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 4 seed: 0.9948\n",
      "Repetición con semilla 5\n",
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 830\n",
      "test_data: 208\n",
      "new_test: 257\n",
      "teorical len: 281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|██████████| 13/13 [00:03<00:00,  3.96it/s, loss=0.131]\n",
      "Epoch 2/3: 100%|██████████| 13/13 [00:03<00:00,  3.93it/s, loss=0.0388]\n",
      "Epoch 3/3: 100%|██████████| 13/13 [00:03<00:00,  3.99it/s, loss=0.00878]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in 5 seed: 0.9922\n",
      "\n",
      "Accuracy promedio después de 6 repeticiones: 0.9895 ± 0.0067\n"
     ]
    }
   ],
   "source": [
    "#     ├── 1а_ без сокращений.txt\n",
    "#     ├── 1б_Изъяты лексемы с частотой выше 100.txt\n",
    "#     ├── 1в_Изъяты лексемы с частотой выше 49.txt\n",
    "#     ├── 1г_Изъяты лексемы с частотой выше 29.txt\n",
    "#     ├── 1д_Изъяты лексемы с частотой выше 9.txt\n",
    "#     ├── 1е_Изъяты лексемы с частотой выше 5.txt \n",
    "#     ├── 1ё_Изъяты лексемы с частотой выше 3.txt ***\n",
    "#     ├── 2а_ без сокращений.txt\n",
    "#     ├── 2б_Изъяты лексемы с частотой выше 100.txt\n",
    "#     ├── 2в_Изъяты лексемы с частотой выше 49.txt\n",
    "#     ├── 2г_Изъяты лексемы с частотой выше 29.txt\n",
    "#     ├── 2д_Изъяты лексемы с частотой выше 9.txt\n",
    "#     ├── 2е_Изъяты лексемы с частотой выше 5.txt \n",
    "#     └── 2ё_Изъяты лексемы с частотой выше 3.txt ***\n",
    "accuracies = []\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"Repetición con semilla {seed}\")\n",
    "    \n",
    "    # Instanciar DataProcessor con la semilla actual\n",
    "    #data_processor = DataProcessor(tokenizer, max_length, test_size=0.2, random_state=seed)\n",
    "    \n",
    "    # Crear el pipeline (ajusta según tu estructura)\n",
    "    ppl = PipelineCommon([\n",
    "        (DataLoader_raw('../dataset/сокращение по частотности/1ё_Изъяты лексемы с частотой выше 3.txt',\n",
    "                        '../dataset/сокращение по частотности/2ё_Изъяты лексемы с частотой выше 3.txt'), \n",
    "         [], \n",
    "         {'datos_raw': 'datos_raw',\n",
    "         'original_datos_raw': 'original_datos_raw'}),\n",
    "        (SentenceSplitterAndCleaner(tokenizer), \n",
    "         ['datos_raw','original_datos_raw'], \n",
    "         {'datos_procesados': 'datos_procesados',\n",
    "         'original_datos_procesados': 'original_datos_procesados'}),\n",
    "        (DataProcessor(tokenizer,\n",
    "                       max_length,\n",
    "                       seed,\n",
    "                       name = 'Изъяты лексемы с частотой выше 3',\n",
    "                       #threshold = 0.5), #186 280\n",
    "                        threshold = 0.4),#186 124\n",
    "         ['datos_procesados','original_datos_procesados'], \n",
    "         {'train_encodings': 'train_encodings', 'train_labels': 'train_labels', \n",
    "          'test_encodings': 'test_encodings', 'test_labels': 'test_labels'}),\n",
    "        (DatasetCreator(batch_size = batch_size), \n",
    "         ['train_encodings', 'train_labels', 'test_encodings', 'test_labels'], \n",
    "         {'train_loader': 'train_loader', 'test_loader': 'test_loader'})\n",
    "    ])\n",
    "    \n",
    "    # Ejecutar el pipeline\n",
    "    result = ppl()\n",
    "    train_loader = result['train_loader']\n",
    "    test_loader = result['test_loader']\n",
    "    \n",
    "    # Configurar el modelo (reiniciarlo en cada repetición)\n",
    "    model = BertForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=2)\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Entrenar el modelo (función train_model definida por ti)\n",
    "    train_model(model, train_loader, optimizer, loss_fn, device, epochs=epochs)\n",
    "    \n",
    "    # Evaluar el modelo (función evaluate_model definida por ti)\n",
    "    results = evaluate_model(model, test_loader, device)\n",
    "    accuracy = results['accuracy']  # Ajusta según la métrica que uses\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    print(f\"Accuracy in {seed} seed: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# Calcular promedio y desviación estándar\n",
    "average_accuracy = np.mean(accuracies)\n",
    "std_accuracy = np.std(accuracies)\n",
    "print(f\"\\nAccuracy promedio después de {num_repeats} repeticiones: {average_accuracy:.4f} ± {std_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea34b90b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4d9bee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mi Entorno (Python 3.9)",
   "language": "python",
   "name": "mi_entorno"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
