{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e66a70c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m../dataset\u001b[00m\r\n",
      "├── \u001b[01;34men_espanol\u001b[00m\r\n",
      "│   ├── docx2txt.py\r\n",
      "│   ├── Второй_жанр_исходная.txt\r\n",
      "│   └── Первый_жанр_исходная.txt\r\n",
      "├── Второй_жанр_исходная.txt\r\n",
      "├── Первый_жанр_исходная.txt\r\n",
      "├── \u001b[01;34mСокращение по частям речи\u001b[00m\r\n",
      "│   ├── 1.Первый жанр исходная выборка.txt\r\n",
      "│   ├── 2.Первый жанр без клауз, включающих наречия.txt\r\n",
      "│   ├── 3.Первый жанр без клауз, включающих глаголы.txt\r\n",
      "│   ├── 4. Первый жанр без клауз, включающих глаголы и наречия.txt\r\n",
      "│   ├── Без прилагательных второй жанр.txt\r\n",
      "│   ├── Без прилагательных первый жанр.txt\r\n",
      "│   └── Случайные выборки.txt\r\n",
      "└── \u001b[01;34mсокращение по частотности\u001b[00m\r\n",
      "    ├── 1а_ без сокращений.txt\r\n",
      "    ├── 1б_Изъяты лексемы с частотой выше 100.txt\r\n",
      "    ├── 1в_Изъяты лексемы с частотой выше 49.txt\r\n",
      "    ├── 1г_Изъяты лексемы с частотой выше 29.txt\r\n",
      "    ├── 1д_Изъяты лексемы с частотой выше 9.txt\r\n",
      "    ├── 1е_Изъяты лексемы с частотой выше 5.txt\r\n",
      "    ├── 1ё_Изъяты лексемы с частотой выше 3.txt\r\n",
      "    ├── 2а_ без сокращений.txt\r\n",
      "    ├── 2б_Изъяты лексемы с частотой выше 100.txt\r\n",
      "    ├── 2в_Изъяты лексемы с частотой выше 49.txt\r\n",
      "    ├── 2г_Изъяты лексемы с частотой выше 29.txt\r\n",
      "    ├── 2д_Изъяты лексемы с частотой выше 9.txt\r\n",
      "    ├── 2е_Изъяты лексемы с частотой выше 5.txt\r\n",
      "    └── 2ё_Изъяты лексемы с частотой выше 3.txt\r\n",
      "\r\n",
      "3 directories, 26 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree ../dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accb463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Untitled.ipynb   utils.py\r\n",
      " \u001b[0m\u001b[01;34m__pycache__\u001b[0m/    \u001b[01;34m'Сокращение по частям речи'\u001b[0m/\r\n",
      " funciones.py    \u001b[01;34m'сокращение по частотности'\u001b[0m/\r\n",
      " test_gpu.py     'сокращение по частотности.ipynb'\r\n",
      " train.py        'сокращение по частотности.md'\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11038bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 15:08:41.891397: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Entrenando secuencial:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando conjunto de datos: Изъяты лексемы с частотой выше 100\n",
      "\n",
      "Repetición con semilla 0\n",
      "train_Изъяты лексемы с частотой выше 100.csv\n",
      "test_Изъяты лексемы с частотой выше 100.csv\n",
      "\n",
      "Repetición con semilla 1\n",
      "\n",
      "Repetición con semilla 2\n",
      "\n",
      "Repetición con semilla 3\n",
      "\n",
      "Repetición con semilla 4\n",
      "\n",
      "Repetición con semilla 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Entrenando secuencial:  17%|█▋        | 1/6 [01:21<06:46, 81.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando conjunto de datos: Изъяты лексемы с частотой выше 49\n",
      "\n",
      "Repetición con semilla 0\n",
      "train_Изъяты лексемы с частотой выше 49.csv\n",
      "test_Изъяты лексемы с частотой выше 49.csv\n",
      "\n",
      "Repetición con semilla 1\n",
      "\n",
      "Repetición con semilla 2\n",
      "\n",
      "Repetición con semilla 3\n",
      "\n",
      "Repetición con semilla 4\n",
      "\n",
      "Repetición con semilla 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Entrenando secuencial:  33%|███▎      | 2/6 [02:41<05:22, 80.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando conjunto de datos: Изъяты лексемы с частотой выше 29\n",
      "\n",
      "Repetición con semilla 0\n",
      "train_Изъяты лексемы с частотой выше 29.csv\n",
      "test_Изъяты лексемы с частотой выше 29.csv\n",
      "\n",
      "Repetición con semilla 1\n",
      "\n",
      "Repetición con semilla 2\n",
      "\n",
      "Repetición con semilla 3\n",
      "\n",
      "Repetición con semilla 4\n",
      "\n",
      "Repetición con semilla 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Entrenando secuencial:  50%|█████     | 3/6 [04:00<03:59, 79.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando conjunto de datos: Изъяты лексемы с частотой выше 9\n",
      "\n",
      "Repetición con semilla 0\n",
      "train_Изъяты лексемы с частотой выше 9.csv\n",
      "test_Изъяты лексемы с частотой выше 9.csv\n",
      "\n",
      "Repetición con semilla 1\n",
      "\n",
      "Repetición con semilla 2\n",
      "\n",
      "Repetición con semilla 3\n",
      "\n",
      "Repetición con semilla 4\n",
      "\n",
      "Repetición con semilla 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Entrenando secuencial:  67%|██████▋   | 4/6 [05:10<02:32, 76.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando conjunto de datos: Изъяты лексемы с частотой выше 5\n",
      "\n",
      "Repetición con semilla 0\n",
      "train_Изъяты лексемы с частотой выше 5.csv\n",
      "test_Изъяты лексемы с частотой выше 5.csv\n",
      "\n",
      "Repetición con semilla 1\n",
      "\n",
      "Repetición con semilla 2\n",
      "\n",
      "Repetición con semilla 3\n",
      "\n",
      "Repetición con semilla 4\n",
      "\n",
      "Repetición con semilla 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Entrenando secuencial:  83%|████████▎ | 5/6 [06:29<01:17, 77.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando conjunto de datos: Изъяты лексемы с частотой выше 3\n",
      "\n",
      "Repetición con semilla 0\n",
      "train_Изъяты лексемы с частотой выше 3.csv\n",
      "test_Изъяты лексемы с частотой выше 3.csv\n",
      "\n",
      "Repetición con semilla 1\n"
     ]
    }
   ],
   "source": [
    "#     ├── 1а_ без сокращений.txt\n",
    "#     ├── 1б_Изъяты лексемы с частотой выше 100.txt\n",
    "#     ├── 1в_Изъяты лексемы с частотой выше 49.txt\n",
    "#     ├── 1г_Изъяты лексемы с частотой выше 29.txt\n",
    "#     ├── 1д_Изъяты лексемы с частотой выше 9.txt\n",
    "#     ├── 1е_Изъяты лексемы с частотой выше 5.txt\n",
    "#     ├── 1ё_Изъяты лексемы с частотой выше 3.txt\n",
    "#     ├── 2а_ без сокращений.txt\n",
    "#     ├── 2б_Изъяты лексемы с частотой выше 100.txt\n",
    "#     ├── 2в_Изъяты лексемы с частотой выше 49.txt\n",
    "#     ├── 2г_Изъяты лексемы с частотой выше 29.txt\n",
    "#     ├── 2д_Изъяты лексемы с частотой выше 9.txt\n",
    "#     ├── 2е_Изъяты лексемы с частотой выше 5.txt\n",
    "#     └── 2ё_Изъяты лексемы с частотой выше 3.txt\n",
    "import funciones\n",
    "from utils import train_wrapper\n",
    "import warnings\n",
    "import os\n",
    "# Suprimir warnings específicos\n",
    "warnings.filterwarnings('ignore', category=UserWarning)  # Para sklearn y otros\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)  # Para huggingface y transformers\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "from contextlib import redirect_stderr\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing as mp\n",
    "\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n",
    "def train_wrapper(args):\n",
    "    dataset, config = args  # Desempaquetar los argumentos\n",
    "    return funciones.train_and_evaluate_dataset(\n",
    "        dataset['path1'],\n",
    "        dataset['path2'],\n",
    "        config,\n",
    "        dataset['name']\n",
    "    )\n",
    "\n",
    "# Uso del código\n",
    "def main():\n",
    "    # Crear configuración\n",
    "    config = funciones.TrainingConfig(\n",
    "        model_name='DeepPavlov/rubert-base-cased',\n",
    "        max_length=128,\n",
    "        batch_size=128,\n",
    "        epochs=3,\n",
    "        learning_rate=2e-5,\n",
    "        num_repeats=6,\n",
    "        test_size=0.2,\n",
    "        threshold=0.5\n",
    "    )\n",
    "    \n",
    "    # Lista de conjuntos de datos a procesar\n",
    "    \n",
    "    datasets = [\n",
    "        {\n",
    "            'path1': '../dataset/сокращение по частотности/1б_Изъяты лексемы с частотой выше 100.txt',\n",
    "            'path2': '../dataset/сокращение по частотности/2б_Изъяты лексемы с частотой выше 100.txt',\n",
    "            'name': 'Изъяты лексемы с частотой выше 100'\n",
    "        },\n",
    "        {\n",
    "            'path1': '../dataset/сокращение по частотности/1в_Изъяты лексемы с частотой выше 49.txt',\n",
    "            'path2': '../dataset/сокращение по частотности/2в_Изъяты лексемы с частотой выше 49.txt',\n",
    "            'name': 'Изъяты лексемы с частотой выше 49'\n",
    "        },\n",
    "        {\n",
    "            'path1': '../dataset/сокращение по частотности/1г_Изъяты лексемы с частотой выше 29.txt',\n",
    "            'path2': '../dataset/сокращение по частотности/2г_Изъяты лексемы с частотой выше 29.txt',\n",
    "            'name': 'Изъяты лексемы с частотой выше 29'\n",
    "        },\n",
    "        {\n",
    "            'path1': '../dataset/сокращение по частотности/1д_Изъяты лексемы с частотой выше 9.txt',\n",
    "            'path2': '../dataset/сокращение по частотности/2д_Изъяты лексемы с частотой выше 9.txt',\n",
    "            'name': 'Изъяты лексемы с частотой выше 9'\n",
    "        },\n",
    "        {\n",
    "            'path1': '../dataset/сокращение по частотности/1е_Изъяты лексемы с частотой выше 5.txt',\n",
    "            'path2': '../dataset/сокращение по частотности/2е_Изъяты лексемы с частотой выше 5.txt',\n",
    "            'name': 'Изъяты лексемы с частотой выше 5'\n",
    "        },\n",
    "        {\n",
    "            'path1': '../dataset/сокращение по частотности/1ё_Изъяты лексемы с частотой выше 3.txt',\n",
    "            'path2': '../dataset/сокращение по частотности/2ё_Изъяты лексемы с частотой выше 3.txt',\n",
    "            'name': 'Изъяты лексемы с частотой выше 3'\n",
    "        },\n",
    "        \n",
    "        \n",
    "    ]\n",
    "    \n",
    "    # Procesar todos los conjuntos de datos\n",
    "    results = []\n",
    "    #for dataset in datasets:\n",
    "#     for dataset in funciones.tqdm(datasets, total=len(datasets)):\n",
    "#         result = funciones.train_and_evaluate_dataset(\n",
    "#             dataset['path1'],\n",
    "#             dataset['path2'],\n",
    "#             config,\n",
    "#             dataset['name']\n",
    "#         )\n",
    "#         results.append(result)\n",
    "    \n",
    "\n",
    "    args = [(dataset, config) for dataset in datasets]\n",
    " \n",
    "    results = []\n",
    "    for arg in tqdm(args, desc=\"Entrenando secuencial\"):\n",
    "        result = train_wrapper(arg)\n",
    "        results.append(result)\n",
    "            \n",
    "    \n",
    "    # Mostrar resumen final\n",
    "    print(\"\\nResumen de todos los experimentos:\")\n",
    "    for result in results:\n",
    "        print(f\"{result['dataset_name']}: \"\n",
    "              f\"{result['avg_accuracy']:.4f} ± {result['std_accuracy']:.4f}\")\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d47437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "num_gpus = torch.cuda.device_count() \n",
    "print(num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd31457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d42e478",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Mi Entorno (Python 3.9)",
   "language": "python",
   "name": "mi_entorno"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
