---
jupyter:
  jupytext:
    formats: ipynb,md
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.13.8
  kernelspec:
    display_name: Mi Entorno (Python 3.9)
    language: python
    name: mi_entorno
---

```python

!tree ../dataset
```

```python
import funciones
```

```python
#     ├── 1а_ без сокращений.txt
#     ├── 1б_Изъяты лексемы с частотой выше 100.txt
#     ├── 1в_Изъяты лексемы с частотой выше 49.txt
#     ├── 1г_Изъяты лексемы с частотой выше 29.txt
#     ├── 1д_Изъяты лексемы с частотой выше 9.txt
#     ├── 1е_Изъяты лексемы с частотой выше 5.txt
#     ├── 1ё_Изъяты лексемы с частотой выше 3.txt
#     ├── 2а_ без сокращений.txt
#     ├── 2б_Изъяты лексемы с частотой выше 100.txt
#     ├── 2в_Изъяты лексемы с частотой выше 49.txt
#     ├── 2г_Изъяты лексемы с частотой выше 29.txt
#     ├── 2д_Изъяты лексемы с частотой выше 9.txt
#     ├── 2е_Изъяты лексемы с частотой выше 5.txt
#     └── 2ё_Изъяты лексемы с частотой выше 3.txt


# Uso del código
def main():
    # Crear configuración
    config = funciones.TrainingConfig(
        model_name='DeepPavlov/rubert-base-cased',
        max_length=128,
        batch_size=64,
        epochs=3,
        learning_rate=2e-5,
        num_repeats=6,
        test_size=0.2,
        threshold=0.5
    )
    
    # Lista de conjuntos de datos a procesar
    datasets = [
        {
            'path1': '../dataset/сокращение по частотности/1б_Изъяты лексемы с частотой выше 100.txt',
            'path2': '../dataset/сокращение по частотности/2б_Изъяты лексемы с частотой выше 100.txt',
            'name': 'Изъяты лексемы с частотой выше 100'
        },
        {
            'path1': '../dataset/сокращение по частотности/1в_Изъяты лексемы с частотой выше 49.txt',
            'path2': '../dataset/сокращение по частотности/2в_Изъяты лексемы с частотой выше 49.txt',
            'name': 'Изъяты лексемы с частотой выше 49'
        },
        {
            'path1': '../dataset/сокращение по частотности/1г_Изъяты лексемы с частотой выше 29.txt',
            'path2': '../dataset/сокращение по частотности/2г_Изъяты лексемы с частотой выше 29.txt',
            'name': 'Изъяты лексемы с частотой выше 29'
        },
        {
            'path1': '../dataset/сокращение по частотности/1д_Изъяты лексемы с частотой выше 9.txt',
            'path2': '../dataset/сокращение по частотности/2д_Изъяты лексемы с частотой выше 9.txt',
            'name': 'Изъяты лексемы с частотой выше 9'
        },
        {
            'path1': '../dataset/сокращение по частотности/1е_Изъяты лексемы с частотой выше 5.txt',
            'path2': '../dataset/сокращение по частотности/2е_Изъяты лексемы с частотой выше 5.txt',
            'name': 'Изъяты лексемы с частотой выше 5'
        },
        {
            'path1': '../dataset/сокращение по частотности/1ё_Изъяты лексемы с частотой выше 3.txt',
            'path2': '../dataset/сокращение по частотности/2ё_Изъяты лексемы с частотой выше 3.txt',
            'name': 'Изъяты лексемы с частотой выше 3'
        },
        
        
    ]
    
    # Procesar todos los conjuntos de datos
    results = []
    for dataset in datasets:
        result = funciones.train_and_evaluate_dataset(
            dataset['path1'],
            dataset['path2'],
            config,
            dataset['name']
        )
        results.append(result)
    
    # Mostrar resumen final
    print("\nResumen de todos los experimentos:")
    for result in results:
        print(f"{result['dataset_name']}: "
              f"{result['avg_accuracy']:.4f} ± {result['std_accuracy']:.4f}")

if __name__ == "__main__":
    main()
```

```python

```

```python

```
