{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "482b8f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m../dataset\u001b[00m\r\n",
      "├── \u001b[01;34men_espanol\u001b[00m\r\n",
      "│   ├── docx2txt.py\r\n",
      "│   ├── Второй_жанр_исходная.txt\r\n",
      "│   └── Первый_жанр_исходная.txt\r\n",
      "├── Второй_жанр_исходная.txt\r\n",
      "├── Первый_жанр_исходная.txt\r\n",
      "├── \u001b[01;34mСокращение по частям речи\u001b[00m\r\n",
      "│   ├── 1.Первый жанр исходная выборка.txt\r\n",
      "│   ├── 2.Первый жанр без клауз, включающих наречия.txt\r\n",
      "│   ├── 3.Первый жанр без клауз, включающих глаголы.txt\r\n",
      "│   ├── 4. Первый жанр без клауз, включающих глаголы и наречия.txt\r\n",
      "│   ├── Без прилагательных второй жанр.txt\r\n",
      "│   ├── Без прилагательных первый жанр.txt\r\n",
      "│   └── Случайные выборки.txt\r\n",
      "└── \u001b[01;34mсокращение по частотности\u001b[00m\r\n",
      "    ├── 1а_ без сокращений.txt\r\n",
      "    ├── 1б_Изъяты лексемы с частотой выше 100.txt\r\n",
      "    ├── 1в_Изъяты лексемы с частотой выше 49.txt\r\n",
      "    ├── 1г_Изъяты лексемы с частотой выше 29.txt\r\n",
      "    ├── 1д_Изъяты лексемы с частотой выше 9.txt\r\n",
      "    ├── 2а_ без сокращений.txt\r\n",
      "    ├── 2б_Изъяты лексемы с частотой выше 100.txt\r\n",
      "    ├── 2в_Изъяты лексемы с частотой выше 49.txt\r\n",
      "    ├── 2г_Изъяты лексемы с частотой выше 29.txt\r\n",
      "    └── 2д_Изъяты лексемы с частотой выше 9.txt\r\n",
      "\r\n",
      "3 directories, 22 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree ../dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d051dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-23 17:44:31.259593: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from isanlp import PipelineCommon\n",
    "from isanlp.simple_text_preprocessor import SimpleTextPreprocessor\n",
    "from isanlp.processor_razdel import ProcessorRazdel\n",
    "import re\n",
    "import razdel \n",
    "\n",
    "from isanlp import PipelineCommon\n",
    "from isanlp.simple_text_preprocessor import SimpleTextPreprocessor\n",
    "from isanlp.processor_razdel import ProcessorRazdel\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import ngram\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from isanlp import PipelineCommon\n",
    "from isanlp.simple_text_preprocessor import SimpleTextPreprocessor\n",
    "from isanlp.processor_razdel import ProcessorRazdel\n",
    "import re\n",
    "import razdel\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d40043c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1. Carga de datos modificados (sin adjetivos)\n",
    "def cargar_datos(archivo, etiqueta):\n",
    "    with open(archivo, 'r', encoding='utf-8') as f:\n",
    "        textos = f.readlines()\n",
    "    return [(texto.strip(), etiqueta) for texto in textos]\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n",
    "max_length = 512\n",
    "\n",
    "class GeneroDataset(Dataset):\n",
    "    def __init__(self, datos):\n",
    "        self.datos = datos\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        texto, etiqueta = self.datos[idx]\n",
    "        tokens = tokenizer(texto,\n",
    "                            padding='max_length',\n",
    "                            truncation=True,\n",
    "                            return_tensors='pt',\n",
    "                            max_length=max_length)\n",
    "        return tokens, torch.tensor(etiqueta)\n",
    "\n",
    "class ProcesadorDatos:\n",
    "    \"\"\"Clase para procesar datos con la lógica de procesar_data\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, datos):\n",
    "        \"\"\"\n",
    "        Método que procesa los datos, elimina los textos entre corchetes, divide en oraciones,\n",
    "        y devuelve los textos procesados con su respectiva etiqueta.\n",
    "        \"\"\"\n",
    "        datos_procesados = []\n",
    "        for texto, etiqueta in datos:\n",
    "            texto_limpio = re.sub(r'\\.,', '. Ok999999999 ,', texto)\n",
    "            texto_limpio = re.sub(r'\\.;', '. Ok999999999 ', texto_limpio)\n",
    "            #texto_limpio = re.sub(r'\\. ([a-zа-я])', r'. Ok999999999 \\1', texto_limpio)\n",
    "            for oracion in razdel.sentenize(re.sub(r'\\[.*?\\]', '', texto_limpio).strip()):  # Elimina corchetes y divide en oraciones\n",
    "                (oracion.text)\n",
    "                oracion_texto = re.sub(r'\\s*Ok999999999', ' ', (oracion.text)).strip() # Eliminar 'Carlossss' si aparece\n",
    "\n",
    "                datos_procesados.append((oracion_texto, etiqueta))\n",
    "        \n",
    "        return {'datos': datos_procesados}  # Devolvemos el resultado en formato de diccionario\n",
    "\n",
    "class ProcesadorDatos_modificados:\n",
    "    \"\"\"Clase para procesar datos con la lógica de procesar_data\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, datos):\n",
    "        \"\"\"\n",
    "        Método que procesa los datos, elimina los textos entre corchetes, divide en oraciones,\n",
    "        y devuelve los textos procesados con su respectiva etiqueta.\n",
    "        \"\"\"\n",
    "        datos_procesados = []\n",
    "        for texto, etiqueta in datos:\n",
    "            texto_limpio = re.sub(r'\\.,', '. Ok999999999 ,', texto)\n",
    "            texto_limpio = re.sub(r'\\.;', '. Ok999999999 ', texto_limpio)\n",
    "            texto_limpio = re.sub(r'\\. ([a-zа-я])', r'. Ok999999999 \\1', texto_limpio)\n",
    "            texto_limpio = re.sub(r'(\\w)([А-Я])', r'\\1. \\2', texto_limpio)\n",
    "            for oracion in razdel.sentenize(re.sub(r'\\[.*?\\]', '', texto_limpio).strip()):  # Elimina corchetes y divide en oraciones\n",
    "                (oracion.text)\n",
    "                oracion_texto = re.sub(r'\\s*Ok999999999', ' ', (oracion.text)).strip() # Eliminar 'Carlossss' si aparece\n",
    "                \n",
    "                datos_procesados.append((oracion_texto, etiqueta))\n",
    "        \n",
    "        return {'datos': datos_procesados}  # Devolvemos el resultado en formato de diccionario\n",
    "\n",
    "ppl_mod = PipelineCommon([\n",
    "    (ProcesadorDatos_modificados(), ['datos'], {'datos': 'datos'}) \n",
    "])\n",
    "\n",
    "ppl_orignila = PipelineCommon([\n",
    "    (ProcesadorDatos_modificados(), ['datos'], {'datos': 'datos'}) \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d2336aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_genero1_modificado = cargar_datos('../dataset/Сокращение по частям речи/Без прилагательных первый жанр.txt', 0)\n",
    "datos_genero2_modificado = cargar_datos('../dataset/Сокращение по частям речи/Без прилагательных второй жанр.txt', 1)\n",
    "datos_modificados =  datos_genero1_modificado + datos_genero2_modificado\n",
    "\n",
    "datos_modificados_procesados =  ppl_mod(datos_modificados)\n",
    "\n",
    "\n",
    "datos_genero1 = cargar_datos('../dataset/Первый_жанр_исходная.txt', 0)\n",
    "datos_genero2 = cargar_datos('../dataset/Второй_жанр_исходная.txt', 1)\n",
    "datos_originales =  datos_genero1 + datos_genero2\n",
    "\n",
    "datos_originales_procesados =  ppl_orignila(datos_originales)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74ad9ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Comparando:   0%|          | 0/1148 [00:00<?, ?oración/s]/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Comparando:   0%|          | 0/1148 [00:01<?, ?oración/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 81\u001b[0m\n\u001b[1;32m     79\u001b[0m es_interseccion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m oracion_train, _ \u001b[38;5;129;01min\u001b[39;00m train_modificado:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m calcular_similitud_mayoria(oracion_original, oracion_train):\n\u001b[1;32m     82\u001b[0m         es_interseccion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def tokenizar(texto):\n",
    "        texto_preprocesado = re.sub(r'[^\\w\\s]', '', texto.lower()) # quita puntuacion y lower case\n",
    "        tokens = nltk.word_tokenize(texto_preprocesado)\n",
    "        return tokens\n",
    "\n",
    "def calcular_similitud_mayoria(texto1, texto2):\n",
    "    \"\"\"Calcula si la mayoría de los tokens de una oración están en la otra.\"\"\"\n",
    "    tokens1 = set(tokenizar(texto1.lower()))\n",
    "    tokens2 = set(tokenizar(texto2.lower()))\n",
    "    \n",
    "    # Calcula la proporción de tokens de cada oración que están en la otra\n",
    "    proporción_1_en_2 = len(tokens1 & tokens2) / len(tokens1) if tokens1 else 0\n",
    "    proporción_2_en_1 = len(tokens1 & tokens2) / len(tokens2) if tokens2 else 0\n",
    "    \n",
    "    # Verifica si la mayoría de los tokens de una oración están en la otra (más del 50%)\n",
    "    #return proporción_1_en_2 > 0.5 or proporción_2_en_1 > 0.5 # 220 - 172 \n",
    "    #Classification Report:\n",
    "     #         precision    recall  f1-score   support\n",
    "\n",
    "    #Género 1       0.98      0.98      0.98       107\n",
    "    #Género 2       0.97      0.97      0.97        65\n",
    "    \n",
    "    #return proporción_1_en_2 > 0.7 or proporción_2_en_1 > 0.7 # 220 225\n",
    "#              precision    recall  f1-score   support\n",
    "\n",
    " #   Género 1       0.99      0.99      0.99       145\n",
    " #   Género 2       0.97      0.97      0.97        80\n",
    "    #return proporción_1_en_2 > 0.6 or proporción_2_en_1 > 0.6 # 220 - 189\n",
    "    #          precision    recall  f1-score   support\n",
    "\n",
    "    #Género 1       0.99      0.98      0.99       120\n",
    "    #Género 2       0.97      0.99      0.98        69\n",
    "    return proporción_1_en_2 > 0.7 or proporción_2_en_1 > 0.7# 220 - 225\n",
    "\n",
    "train_modificado, test_modificado = train_test_split(datos_modificados_procesados['datos'],\n",
    "                                                     test_size=0.2, random_state=42)\n",
    "\n",
    "nuevo_test = []\n",
    "\n",
    "# Bucle con barra de progreso\n",
    "for oracion_original, etiqueta_original in tqdm(datos_originales_procesados['datos'], desc=\"Comparando\", unit=\"oración\"):\n",
    "    es_interseccion = False\n",
    "    for oracion_train, _ in train_modificado:\n",
    "        if calcular_similitud_mayoria(oracion_original, oracion_train):\n",
    "            es_interseccion = True\n",
    "            break\n",
    "    if not es_interseccion:\n",
    "        nuevo_test.append((oracion_original, etiqueta_original))\n",
    "\n",
    "print(f\"Tamaño de train: {len(train_modificado)}\")\n",
    "print(f\"Tamaño de test modificado: {len(test_modificado)}\")\n",
    "print(f\"Tamaño de nuevo test sin intersecciones: {len(nuevo_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fc2df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = GeneroDataset(train_modificado)\n",
    "test_dataset = GeneroDataset(nuevo_test)\n",
    "\n",
    "# Crear DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Imprimir tamaños correctamente\n",
    "\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=2)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs = {k: v.to(device).squeeze(1) for k, v in inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "    \n",
    "\n",
    "\n",
    "# 6. Evaluación del modelo\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_predicted = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs = {k: v.to(device).squeeze(1) for k, v in inputs.items()}\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        all_predicted.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = correct / total\n",
    "precision = precision_score(all_labels, all_predicted)\n",
    "recall = recall_score(all_labels, all_predicted)\n",
    "f1 = f1_score(all_labels, all_predicted)\n",
    "conf_matrix = confusion_matrix(all_labels, all_predicted)\n",
    "\n",
    "# Imprimir métricas\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1-score: {f1}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification Report\n",
    "report = classification_report(all_labels, all_predicted, target_names=['Género 1', 'Género 2'])\n",
    "print('Classification Report:')\n",
    "print(report)\n",
    "\n",
    "# 1. Gráfico de la Matriz de Confusión (Heatmap)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Género 1', 'Género 2'],\n",
    "            yticklabels=['Género 1', 'Género 2'])\n",
    "plt.xlabel('Predicciones')\n",
    "plt.ylabel('Valores Reales')\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.show()\n",
    "\n",
    "# 2. Gráfico de Barras de las Métricas\n",
    "metrics = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1-score': f1}\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=list(metrics.keys()), y=list(metrics.values()), palette='viridis')\n",
    "plt.ylim(0, 1.1)  # Ajusta el límite superior para mostrar claramente los valores 1.0\n",
    "plt.ylabel('Puntuación')\n",
    "plt.title('Métricas de Evaluación')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1939211",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mi Entorno (Python 3.9)",
   "language": "python",
   "name": "mi_entorno"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
