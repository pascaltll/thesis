{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e6d1a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc7db3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9474109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>и что мы бы назвали сложными модусными перспек...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Эзофастома бокаловилная, вооружена более крупн...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>, 2008; Durlak et al</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>где c0 - концентрация раствора</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>При сгруктурном исследовании минерала обнаруже...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Эпимеральная область (рис</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Человек рассматривается как «рефлексивное живо...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>, на верхушке чуть закругленные (рис</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Тычинок 2, со свободными, волосисто опушенными...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Длина нерок 2.9-3.2 мм, отношение их длины к д...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  и что мы бы назвали сложными модусными перспек...      1\n",
       "1  Эзофастома бокаловилная, вооружена более крупн...      0\n",
       "2                               , 2008; Durlak et al      1\n",
       "3                     где c0 - концентрация раствора      1\n",
       "4  При сгруктурном исследовании минерала обнаруже...      0\n",
       "5                          Эпимеральная область (рис      0\n",
       "6  Человек рассматривается как «рефлексивное живо...      1\n",
       "7               , на верхушке чуть закругленные (рис      0\n",
       "8  Тычинок 2, со свободными, волосисто опушенными...      0\n",
       "9  Длина нерок 2.9-3.2 мм, отношение их длины к д...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6531b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['text'].tolist()  # Convert to lists\n",
    "labels = df['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4561a11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-22 00:10:58.525261: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./ofline_models/ruBert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"./ofline_models/ruBert-base\"\n",
    "# 1. Load Model and Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2) # num_classes: number of your classes\n",
    "\n",
    "\n",
    "# 3. Tokenize and Create Input Tensors\n",
    "encoded_data = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "input_ids = encoded_data['input_ids']\n",
    "attention_mask = encoded_data['attention_mask']\n",
    "labels_tensor = torch.tensor(labels)\n",
    "\n",
    "# 4. Split Data into Training and Validation Sets\n",
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
    "    input_ids, labels_tensor, test_size=0.2, random_state=42  # Adjust test_size as needed\n",
    ")\n",
    "train_masks, val_masks, _, _ = train_test_split(\n",
    "    attention_mask, input_ids, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# 5. Set up Optimizer\n",
    "learning_rate = 1e-5 #1e-5, 2e-5, 3e-5, 5e-5.\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)  # Adjust learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d083b2a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 352.00 MiB. GPU 0 has a total capacity of 23.47 GiB of which 191.81 MiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 2.70 GiB is allocated by PyTorch, and 401.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     44\u001b[0m     epoch_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 46\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Promedio de la pérdida en el entrenamiento para esta época\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:648\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    646\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta1))\n\u001b[1;32m    647\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m--> 648\u001b[0m denom \u001b[38;5;241m=\u001b[39m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39madd_(group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    650\u001b[0m step_size \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrect_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m]:  \u001b[38;5;66;03m# No bias correction for Bert\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 352.00 MiB. GPU 0 has a total capacity of 23.47 GiB of which 191.81 MiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 2.70 GiB is allocated by PyTorch, and 401.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# 6. Training Loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) # Move the model to the device\n",
    "batch_size = 4  # Adjust batch size\n",
    "num_epochs = 4  # Adjust the number of epochs\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "val_precisions = []\n",
    "val_recalls = []\n",
    "val_f1_scores = []\n",
    "learning_rates = []  # Lista para guardar el learning rate\n",
    "\n",
    "# Define the file path\n",
    "csv_file_path = 'training_metrics_ruBert-base.csv'\n",
    "\n",
    "# Verifica si el archivo existe y si no, agrega la cabecera\n",
    "file_exists = os.path.isfile(csv_file_path)\n",
    "with open(csv_file_path, mode='a', newline='') as file:  # Open ONCE before the loop\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    if not file_exists:  # Write header ONLY if the file doesn't exist\n",
    "        writer.writerow(['Epoch', 'Train Loss', 'Validation Accuracy', 'Precision', 'Recall', 'F1 Score', 'Learning Rate'])\n",
    "\n",
    "\n",
    "    # Ciclo de entrenamiento\n",
    "    for epoch in range(num_epochs):\n",
    "        # Actualiza el learning rate si es necesario (ejemplo de ajuste)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        learning_rates.append(current_lr)\n",
    "\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        for i in range(0, len(train_inputs), batch_size):\n",
    "            batch_inputs = train_inputs[i:i+batch_size].to(device)\n",
    "            batch_labels = train_labels[i:i+batch_size].to(device)\n",
    "            batch_masks = train_masks[i:i+batch_size].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_inputs, attention_mask=batch_masks, labels=batch_labels)\n",
    "            loss = outputs.loss \n",
    "            loss.backward()\n",
    "            epoch_train_loss += loss.item()\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Promedio de la pérdida en el entrenamiento para esta época\n",
    "        avg_train_loss = epoch_train_loss / len(train_inputs) * batch_size\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # 7. Validación\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(val_inputs), batch_size):\n",
    "                batch_inputs = val_inputs[i:i+batch_size].to(device)\n",
    "                batch_labels = val_labels[i:i+batch_size].to(device)\n",
    "                batch_masks = val_masks[i:i+batch_size].to(device)\n",
    "\n",
    "                outputs = model(batch_inputs, attention_mask=batch_masks)\n",
    "                logits = outputs.logits\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_true.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "        val_accuracy = accuracy_score(val_true, val_preds)\n",
    "        val_precision = precision_score(val_true, val_preds, average='weighted')\n",
    "        val_recall = recall_score(val_true, val_preds, average='weighted')\n",
    "        val_f1 = f1_score(val_true, val_preds, average='weighted')\n",
    "\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        val_precisions.append(val_precision)\n",
    "        val_recalls.append(val_recall)\n",
    "        val_f1_scores.append(val_f1)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, \"\n",
    "              f\"Validation Accuracy: {val_accuracy:.4f}, \"\n",
    "              f\"Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}\")\n",
    "        writer.writerow([epoch+1, avg_train_loss, val_accuracy, val_precision, val_recall, val_f1, current_lr])\n",
    "\n",
    "\n",
    "    # Al finalizar, las métricas se habrán guardado en 'training_metrics.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70c07b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Leer el archivo CSV\n",
    "df = pd.read_csv('training_metrics_ruBert-base.csv')\n",
    "\n",
    "# Obtener los valores únicos de learning rate\n",
    "learning_rates = df['Learning Rate'].unique()\n",
    "\n",
    "# Crear la figura y los ejes\n",
    "num_rows = len(learning_rates)\n",
    "num_cols = 5  # Number of columns (plots per LR)\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(30, 5 * num_rows))\n",
    "\n",
    "# Flatten axes if there's only one row (one learning rate)\n",
    "if num_rows == 1:\n",
    "    axes = axes.reshape(1, num_cols) # Reshape to 2D array even if it's one row.\n",
    "\n",
    "# Iterar sobre cada valor de learning rate\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    # Filtrar los datos para este learning rate\n",
    "    lr_data = df[df['Learning Rate'] == lr]\n",
    "\n",
    "    # Entrenamiento vs Pérdida\n",
    "    axes[i, 0].plot(lr_data['Epoch'], lr_data['Train Loss'], label=f'LR = {lr}', color='blue')\n",
    "    axes[i, 0].set_title(f'Train Loss vs Epochs (LR = {lr})')\n",
    "    axes[i, 0].set_xlabel('Epochs')\n",
    "    axes[i, 0].set_ylabel('Train Loss')\n",
    "    axes[i, 0].grid(True)\n",
    "    axes[i, 0].legend()  # Add legend\n",
    "\n",
    "    # Precisión de validación\n",
    "    axes[i, 1].plot(lr_data['Epoch'], lr_data['Validation Accuracy'], label=f'LR = {lr}', color='green')\n",
    "    axes[i, 1].set_title(f'Validation Accuracy vs Epochs (LR = {lr})')\n",
    "    axes[i, 1].set_xlabel('Epochs')\n",
    "    axes[i, 1].set_ylabel('Accuracy')\n",
    "    axes[i, 1].grid(True)\n",
    "    axes[i, 1].legend()\n",
    "\n",
    "    # ... (Repeat for other metrics - Precision, Recall, F1)\n",
    "    axes[i, 2].plot(lr_data['Epoch'], lr_data['Precision'], label=f'LR = {lr}', color='red')\n",
    "    axes[i, 2].set_title(f'Validation Precision vs Epochs (LR = {lr})')\n",
    "    axes[i, 2].set_xlabel('Epochs')\n",
    "    axes[i, 2].set_ylabel('Precision')\n",
    "    axes[i, 2].grid(True)\n",
    "    axes[i, 2].legend()\n",
    "\n",
    "    axes[i, 3].plot(lr_data['Epoch'], lr_data['Recall'], label=f'LR = {lr}', color='orange')\n",
    "    axes[i, 3].set_title(f'Validation Recall vs Epochs (LR = {lr})')\n",
    "    axes[i, 3].set_xlabel('Epochs')\n",
    "    axes[i, 3].set_ylabel('Recall')\n",
    "    axes[i, 3].grid(True)\n",
    "    axes[i, 3].legend()\n",
    "\n",
    "    axes[i, 4].plot(lr_data['Epoch'], lr_data['F1 Score'], label=f'LR = {lr}', color='purple')\n",
    "    axes[i, 4].set_title(f'Validation F1 Score vs Epochs (LR = {lr})')\n",
    "    axes[i, 4].set_xlabel('Epochs')\n",
    "    axes[i, 4].set_ylabel('F1 Score')\n",
    "    axes[i, 4].grid(True)\n",
    "    axes[i, 4].legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cda5d01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mi Entorno (Python 3.9)",
   "language": "python",
   "name": "mi_entorno"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
