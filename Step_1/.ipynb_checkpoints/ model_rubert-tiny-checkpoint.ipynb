{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb5eaf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cda8fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "954df784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>и что мы бы назвали сложными модусными перспек...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Эзофастома бокаловилная, вооружена более крупн...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>, 2008; Durlak et al</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>где c0 - концентрация раствора</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>При сгруктурном исследовании минерала обнаруже...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Эпимеральная область (рис</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Человек рассматривается как «рефлексивное живо...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>, на верхушке чуть закругленные (рис</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Тычинок 2, со свободными, волосисто опушенными...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Длина нерок 2.9-3.2 мм, отношение их длины к д...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  и что мы бы назвали сложными модусными перспек...      1\n",
       "1  Эзофастома бокаловилная, вооружена более крупн...      0\n",
       "2                               , 2008; Durlak et al      1\n",
       "3                     где c0 - концентрация раствора      1\n",
       "4  При сгруктурном исследовании минерала обнаруже...      0\n",
       "5                          Эпимеральная область (рис      0\n",
       "6  Человек рассматривается как «рефлексивное живо...      1\n",
       "7               , на верхушке чуть закругленные (рис      0\n",
       "8  Тычинок 2, со свободными, волосисто опушенными...      0\n",
       "9  Длина нерок 2.9-3.2 мм, отношение их длины к д...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfdaecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e5209ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['text'].tolist()  # Convert to lists\n",
    "labels = df['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d2dc4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 22:28:35.522344: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./ofline_models/rubert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/workspace/notebooks/Carlos/fine_tunig_project/mi_entorno/lib/python3.8/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Accuracy: 0.9319727891156463\n",
      "Epoch 2: Validation Accuracy: 0.9387755102040817\n",
      "Epoch 3: Validation Accuracy: 0.9387755102040817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('fine_tuned_rubert_tiny/tokenizer_config.json',\n",
       " 'fine_tuned_rubert_tiny/special_tokens_map.json',\n",
       " 'fine_tuned_rubert_tiny/vocab.txt',\n",
       " 'fine_tuned_rubert_tiny/added_tokens.json',\n",
       " 'fine_tuned_rubert_tiny/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_name = \"./ofline_models/rubert-tiny\"\n",
    "# 1. Load Model and Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2) # num_classes: number of your classes\n",
    "\n",
    "\n",
    "# 3. Tokenize and Create Input Tensors\n",
    "encoded_data = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "input_ids = encoded_data['input_ids']\n",
    "attention_mask = encoded_data['attention_mask']\n",
    "labels_tensor = torch.tensor(labels)\n",
    "\n",
    "# 4. Split Data into Training and Validation Sets\n",
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
    "    input_ids, labels_tensor, test_size=0.2, random_state=42  # Adjust test_size as needed\n",
    ")\n",
    "train_masks, val_masks, _, _ = train_test_split(\n",
    "    attention_mask, input_ids, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# 5. Set up Optimizer\n",
    "learning_rate = 5e-5 \n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)  # Adjust learning rate\n",
    "\n",
    "# 6. Training Loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device) # Move the model to the device\n",
    "batch_size = 8  # Adjust batch size\n",
    "num_epochs = 3  # Adjust the number of epochs\n",
    "gradient_accumulation_steps = 2  # Simula un batch size de 32 si batch_size es 16\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i in range(0, len(train_inputs), batch_size):\n",
    "        batch_inputs = train_inputs[i:i+batch_size].to(device)\n",
    "        batch_labels = train_labels[i:i+batch_size].to(device)\n",
    "        batch_masks = train_masks[i:i+batch_size].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_inputs, attention_mask=batch_masks, labels=batch_labels)\n",
    "        loss = outputs.loss / gradient_accumulation_steps  # Escala la pérdida\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i + 1) % gradient_accumulation_steps == 0:  # Acumula gradientes\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "    # 7. Validation\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(val_inputs), batch_size):\n",
    "            batch_inputs = val_inputs[i:i+batch_size].to(device)\n",
    "            batch_labels = val_labels[i:i+batch_size].to(device)\n",
    "            batch_masks = val_masks[i:i+batch_size].to(device)\n",
    "\n",
    "            outputs = model(batch_inputs, attention_mask=batch_masks)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    val_accuracy = accuracy_score(val_labels.cpu().numpy(), np.array(val_preds))\n",
    "    print(f\"Epoch {epoch+1}: Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# 8. Save the Fine-Tuned Model\n",
    "model.save_pretrained(\"fine_tuned_rubert_tiny\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_rubert_tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36f87a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6371cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mi Entorno (Python 3.9)",
   "language": "python",
   "name": "mi_entorno"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
